{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85038691-cbcc-4a11-8f66-0e283034a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q pyhere p_tqdm glum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa45eeff-1bf9-40bf-a5ff-9b412d0a787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import warnings\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from pyhere import here\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import pickle\n",
    "\n",
    "import pyarrow\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import p_tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneGroupOut, cross_val_score, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr,  pearsonr\n",
    "\n",
    "from task_modeling_utils import *\n",
    "from prediction_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c23dcdde-2310-49fe-9f1c-b4b4ee121561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_pattern = re.compile(\"20k-points\")\n",
    "wa_pattern = re.compile(\"cm-False\")\n",
    "\n",
    "data_dir = here(\"data\")\n",
    "directory = here(\"data\", \"random_features\", \"summary\")\n",
    "files = os.listdir(directory)\n",
    "files = [f for f in files if f not in ('.gitkeep', '.ipynb_checkpoints')]\n",
    "files = [f for f in files if not (bool(point_pattern.search(f)) & bool(wa_pattern.search(f)))]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6877398-a7af-4d30-8be5-f2f5b92c8a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramlist = list(itertools.product(files, [True, False]))\n",
    "len(paramlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ae5aca-1f61-4214-a348-11ca6f4b5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask import delayed\n",
    "# @delayed\n",
    "def model(params):\n",
    "#########################################     SET PARAMS    #########################################\n",
    "    file         = params[0][0]\n",
    "    hot_encode   = params[1][0]\n",
    "    f            = file.split(sep=\"_\")\n",
    "    satellite    = f[0]\n",
    "    bands        = f[1].replace(\"bands-\", \"\")\n",
    "    country_code = f[2]\n",
    "    points       = f[3].replace(\"k-points\", \"\")\n",
    "    num_features = f[4].replace(\"-features\", \"\")\n",
    "    yrs          = f[5].replace(\"yr-\", \"\").split(sep=\"-\")\n",
    "    mns          = f[6].replace(\"mn-\", \"\").split(sep=\"-\")\n",
    "    limit_months = str2bool(f[7].replace(\"lm-\", \"\"))\n",
    "    crop_mask    = str2bool(f[8].replace(\"cm-\", \"\"))\n",
    "    weighted_avg = str2bool(f[9].replace(\"wa-\", \"\"))\n",
    "    years        = range(int(yrs[0]), int(yrs[1])+1)\n",
    "    month_range  = list(range(int(mns[0]), int(mns[1])+1))\n",
    "\n",
    "#########################################     READ DATA    #########################################\n",
    "    fn = f\"{directory}/{file}\"\n",
    "    features = pd.read_feather(fn)\n",
    "    features.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "    drop_cols = ['district', 'year', 'yield_mt']\n",
    "\n",
    "    crop_yield = features.copy().loc[:, tuple(drop_cols)]\n",
    "    crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "########################################     HOT ENCODE    ###########################################\n",
    "    if hot_encode:\n",
    "        drop_cols.remove(\"district\")\n",
    "        features = pd.get_dummies(features, columns=[\"district\"], drop_first=False)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "    x_all = features.drop(drop_cols, axis = 1) \n",
    "    y_all = np.log10(features.yield_mt.to_numpy() + 1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV    ###########################################\n",
    "    ### SETUP\n",
    "    ridge  = Ridge()  \n",
    "    kfold  = KFold(n_splits=5)\n",
    "    alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "    ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER(S)\n",
    "    if hot_encode:\n",
    "        best_lambdas, best_scores, best_model = kfold_rr_multi_lambda_tuning(\n",
    "            X=x_train, y=y_train, \n",
    "            grid=alphas.get('alpha'), \n",
    "            start=[0, x_train.shape[1]-72],\n",
    "            end=[x_train.shape[1]-72, x_train.shape[1]], \n",
    "            static_lam=1e-16,\n",
    "            verbose=False,\n",
    "            show_linalg_warning=False,\n",
    "            fit_model_after_tuning=True\n",
    "        )\n",
    "    else:\n",
    "        search = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold).fit(x_train, y_train)\n",
    "        best_model   = search.best_estimator_\n",
    "        best_scores  = search.best_score_\n",
    "        best_lambdas = best_model.alpha\n",
    "    ### PREDICT WITH BEST HYPERPARAMETER(S)\n",
    "    val_predictions   = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)   \n",
    "    train_predictions = best_model.predict(x_train)\n",
    "    test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################    \n",
    "    crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "    train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "    train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "    train_split['cv_prediction'] = np.maximum(val_predictions, 0)\n",
    "    train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"]-train_split.groupby('district')['log_yield'].transform('mean')\n",
    "    train_split[\"demean_cv_prediction\"] = train_split[\"cv_prediction\"]-train_split.groupby('district')['cv_prediction'].transform('mean')\n",
    "\n",
    "    test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "    test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "    test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "    test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "    test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "    predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "#########################################     SAVE RESULTS    #########################################\n",
    "    d = {\n",
    "        'country'     : country_code,\n",
    "        'satellite'   : satellite,\n",
    "        'bands'       : bands,\n",
    "        'num_features': num_features,\n",
    "        'points'      : points, \n",
    "        'month_range' : f'{min(month_range)}-{max(month_range)}',\n",
    "\n",
    "        'limit_months': limit_months,\n",
    "        'crop_mask'   : crop_mask,\n",
    "        'weighted_avg': weighted_avg,\n",
    "        'hot_encode': hot_encode,\n",
    "\n",
    "        'total_n': len(x_all),\n",
    "        'train_n': len(x_train),\n",
    "        'test_n' : len(x_test),\n",
    "\n",
    "        'best_reg_param': [best_lambdas],\n",
    "        'mean_of_val_R2': [best_scores],\n",
    "        'val_R2': r2_score(y_train, val_predictions),\n",
    "        'val_r' : pearsonr(val_predictions, y_train)[0],\n",
    "        'val_r2': pearsonr(val_predictions, y_train)[0] ** 2,\n",
    "\n",
    "        'train_R2': r2_score(y_train, train_predictions),\n",
    "        'train_r' : pearsonr(train_predictions, y_train)[0],\n",
    "        'train_r2': pearsonr(train_predictions, y_train)[0] ** 2,\n",
    "\n",
    "        'test_R2': r2_score(y_test, test_predictions),\n",
    "        'test_r' : pearsonr(test_predictions, y_test)[0],\n",
    "        'test_r2': pearsonr(test_predictions, y_test)[0] ** 2,\n",
    "\n",
    "        'demean_cv_R2': r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction),\n",
    "        'demean_cv_r':  pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0],\n",
    "        'demean_cv_r2': pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0] ** 2,\n",
    "    }\n",
    "    # return d\n",
    "    return pd.DataFrame(data=d, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30720303-2152-4ac9-9ba6-c1b14f64275a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time    \n",
    "# ##### With progress bar\n",
    "# workers = os.cpu_count()\n",
    "# if __name__ == \"__main__\":\n",
    "#     output = []\n",
    "#     for result in p_tqdm.p_umap(model, paramlist, num_cpus=workers):\n",
    "#         output.append(result)\n",
    "#     results = pd.concat(output).reset_index(drop=True)\n",
    "#     today = date.today().strftime(\"%Y-%m-%d\")\n",
    "#     file_name = f'results_{today}.csv'\n",
    "#     print(f\"Saving results as: {file_name}\\n\\n\")           \n",
    "#     results.to_csv(here(\"data\",\"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9a4912e-d950-41c4-ae2f-bc8e23f2d15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask_gateway\n",
    "# from distributed.diagnostics.plugin import UploadFile, UploadDirectory, PipInstall\n",
    "\n",
    "# cluster = dask_gateway.GatewayCluster()\n",
    "# client = cluster.get_client()\n",
    "# client.register_worker_plugin(PipInstall(['glum', 'pyhere']))\n",
    "# client.register_worker_plugin(UploadFile(here(\"code\", \"3_task_modeling\", \"task_modeling_utils.py\")))\n",
    "# # client.register_worker_plugin(UploadDirectory(here(\"data\", \"random_features\", \"summary\")))\n",
    "# cluster.scale(44)\n",
    "# print(cluster.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "05130c01-d05f-4b18-b6a9-78c39143808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fn in files:\n",
    "#     client.register_worker_plugin(UploadFile(here('data', 'random_features', 'summary', fn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0143ad17-22ba-42a5-9c2e-c6ed9742d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# futures = []\n",
    "# for fn in files:\n",
    "#     df = pd.read_feather(here('data', 'random_features', 'summary', fn))\n",
    "#     future = client.scatter(df)\n",
    "#     futures.append(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8e7a8be2-1619-47b2-a340-b16f9af8a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd\n",
    "# ddf = dd.from_delayed(futures, meta=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a43f259d-3784-4067-a5f7-5eb76bc53429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf.map_partitions(model, True, meta=(None, 'f8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "12392b92-b4fd-4ac0-b2b6-7f555935847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf.map_partitions(model, paramlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "16918013-b5ed-460c-807f-57fab99f909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_map = client.map(model, paramlist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5b92e5cc-33af-444e-9d09-34151a5a13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent = client.submit(model, paramlist) \n",
    "# result = client.gather(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5848c966-48a0-4c70-aeba-d0e0147bd2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9622bcb7-76ba-4cd7-aee5-e162c83f53fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "569c2d3f-29fd-400c-9065-6860bb22b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "# with Client(n_workers=88) as client:\n",
    "#     sent = client.submit(model, paramlist)\n",
    "#     result = client.gather(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f539711e-2bc0-43cc-8cac-b595c5fb1932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dask.distributed import Client\n",
    "# workers = os.cpu_count()\n",
    "# if __name__ == \"__main__\":\n",
    "#     output = []\n",
    "#     with Client(n_workers=88) as client:\n",
    "#         for result in p_tqdm.p_umap(model, paramlist):\n",
    "#             output.append(result).compute()\n",
    "#     results = pd.concat(output).reset_index(drop=True)\n",
    "#     today = date.today().strftime(\"%Y-%m-%d\")\n",
    "#     file_name = f'results_{today}.csv'\n",
    "#     print(f\"Saving results as: {file_name}\\n\\n\")           \n",
    "#     results.to_csv(here(\"data\",\"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d016b23-b6ad-42f4-b2c9-d70f5f9c6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time    \n",
    "# #### No progress bar\n",
    "# multiprocessing.set_start_method('spawn')\n",
    "# workers = os.cpu_count()\n",
    "# if __name__ == \"__main__\":\n",
    "#     with multiprocessing.Pool(processes=workers) as pool:\n",
    "#         output = []\n",
    "#         for result in pool.imap_unordered(model, paramlist, chunksize=2):\n",
    "#             output.append(result)\n",
    "#     results = pd.concat(output).reset_index(drop=True)\n",
    "#     today = date.today().strftime(\"%Y-%m-%d\")\n",
    "#     file_name = f'results_{today}.csv'\n",
    "#     print(f\"Saving results as: {file_name}\\n\\n\")           \n",
    "#     results.to_csv(here(\"data\",\"results\", file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c078d5-adb2-42c1-b5ab-91bf563df81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING\n",
    "file         = 'landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_15k-points_1000-features_yr-2013-2021_mn-4-9_lm-True_cm-False_wa-False_summary.feather'\n",
    "hot_encode   = True\n",
    "\n",
    "#########################################     SET PARAMS    #########################################\n",
    "# file         = params[0]\n",
    "# hot_encode   = params[1]\n",
    "f            = file.split(sep=\"_\")\n",
    "satellite    = f[0]\n",
    "bands        = f[1].replace(\"bands-\", \"\")\n",
    "country_code = f[2]\n",
    "points       = f[3].replace(\"k-points\", \"\")\n",
    "num_features = f[4].replace(\"-features\", \"\")\n",
    "yrs          = f[5].replace(\"yr-\", \"\").split(sep=\"-\")\n",
    "mns          = f[6].replace(\"mn-\", \"\").split(sep=\"-\")\n",
    "limit_months = str2bool(f[7].replace(\"lm-\", \"\"))\n",
    "crop_mask    = str2bool(f[8].replace(\"cm-\", \"\"))\n",
    "weighted_avg = str2bool(f[9].replace(\"wa-\", \"\"))\n",
    "years        = range(int(yrs[0]), int(yrs[1])+1)\n",
    "month_range  = list(range(int(mns[0]), int(mns[1])+1))\n",
    "\n",
    "#########################################     READ DATA    #########################################\n",
    "fn = f\"{directory}/{file}\"\n",
    "features = pd.read_feather(fn)\n",
    "features.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "\n",
    "drop_cols = ['district', 'year', 'yield_mt']\n",
    "\n",
    "#########################################    JOIN CLIMATE VARS    #########################################  \n",
    "ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains('ndvi')]\n",
    "keep_cols = [*ndvi_cols, *drop_cols]\n",
    "climate_df = climate_df.loc[:, keep_cols]\n",
    "\n",
    "features = features.set_index(drop_cols).join(climate_df.set_index(drop_cols)).reset_index()\n",
    "features = features[features.year <= max(climate_df.year)]\n",
    "\n",
    "crop_yield = features.copy().loc[:, tuple(drop_cols)]\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################     HOT ENCODE    ###########################################\n",
    "if hot_encode:\n",
    "    # features['district'] = features.district.astype('category')\n",
    "    drop_cols.remove(\"district\")\n",
    "    features = pd.get_dummies(features, columns=[\"district\"], drop_first=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "features = features.set_index(drop_cols) \n",
    "features_scaled = StandardScaler().fit_transform(features.values)\n",
    "features = pd.DataFrame(features_scaled, index=features.index).reset_index()\n",
    "features.columns = features.columns.astype(str)          \n",
    "\n",
    "features.yield_mt = np.log10(features.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = features.drop(drop_cols, axis = 1) \n",
    "y_all = features.yield_mt\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fc760eb-7f92-457f-866a-5a18de7deb46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>6074</th>\n",
       "      <th>6075</th>\n",
       "      <th>6076</th>\n",
       "      <th>6077</th>\n",
       "      <th>6078</th>\n",
       "      <th>6079</th>\n",
       "      <th>6080</th>\n",
       "      <th>6081</th>\n",
       "      <th>6082</th>\n",
       "      <th>6083</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.122405</td>\n",
       "      <td>-0.602627</td>\n",
       "      <td>-0.962090</td>\n",
       "      <td>-0.373393</td>\n",
       "      <td>0.087622</td>\n",
       "      <td>-0.395953</td>\n",
       "      <td>0.665469</td>\n",
       "      <td>0.815541</td>\n",
       "      <td>0.836532</td>\n",
       "      <td>0.860886</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>-0.745102</td>\n",
       "      <td>-1.319972</td>\n",
       "      <td>-1.137262</td>\n",
       "      <td>-0.828373</td>\n",
       "      <td>-0.100150</td>\n",
       "      <td>-0.273584</td>\n",
       "      <td>0.508537</td>\n",
       "      <td>-0.248048</td>\n",
       "      <td>-0.594027</td>\n",
       "      <td>-0.255724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>8.426150</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>-1.108298</td>\n",
       "      <td>-1.835012</td>\n",
       "      <td>-1.133221</td>\n",
       "      <td>-0.726423</td>\n",
       "      <td>-0.530001</td>\n",
       "      <td>-1.330731</td>\n",
       "      <td>-1.639542</td>\n",
       "      <td>-2.489729</td>\n",
       "      <td>-2.327456</td>\n",
       "      <td>-2.254350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>8.426150</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>0.832510</td>\n",
       "      <td>0.480655</td>\n",
       "      <td>0.677306</td>\n",
       "      <td>0.382840</td>\n",
       "      <td>0.190721</td>\n",
       "      <td>-0.118106</td>\n",
       "      <td>0.180867</td>\n",
       "      <td>0.882195</td>\n",
       "      <td>0.817981</td>\n",
       "      <td>0.662049</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>0.200511</td>\n",
       "      <td>1.014885</td>\n",
       "      <td>1.385173</td>\n",
       "      <td>0.913613</td>\n",
       "      <td>0.745622</td>\n",
       "      <td>0.396749</td>\n",
       "      <td>-0.745719</td>\n",
       "      <td>0.204117</td>\n",
       "      <td>0.234500</td>\n",
       "      <td>0.226061</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.507697</td>\n",
       "      <td>1.086205</td>\n",
       "      <td>1.644558</td>\n",
       "      <td>1.474270</td>\n",
       "      <td>-2.304456</td>\n",
       "      <td>-0.379790</td>\n",
       "      <td>0.718619</td>\n",
       "      <td>0.913073</td>\n",
       "      <td>0.959412</td>\n",
       "      <td>0.971583</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>-0.855198</td>\n",
       "      <td>1.012120</td>\n",
       "      <td>0.686110</td>\n",
       "      <td>0.021326</td>\n",
       "      <td>0.500773</td>\n",
       "      <td>0.143984</td>\n",
       "      <td>-1.111080</td>\n",
       "      <td>0.031450</td>\n",
       "      <td>0.429969</td>\n",
       "      <td>-0.844280</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.520318</td>\n",
       "      <td>0.075699</td>\n",
       "      <td>0.494057</td>\n",
       "      <td>0.007725</td>\n",
       "      <td>0.658465</td>\n",
       "      <td>0.811826</td>\n",
       "      <td>0.411325</td>\n",
       "      <td>-0.106081</td>\n",
       "      <td>0.321837</td>\n",
       "      <td>0.033495</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>-2.786142</td>\n",
       "      <td>-0.523502</td>\n",
       "      <td>-0.856285</td>\n",
       "      <td>-0.245399</td>\n",
       "      <td>-0.068380</td>\n",
       "      <td>-0.054709</td>\n",
       "      <td>-2.941933</td>\n",
       "      <td>-0.661904</td>\n",
       "      <td>-0.925709</td>\n",
       "      <td>-0.882008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>8.426150</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>1.342915</td>\n",
       "      <td>-4.057653</td>\n",
       "      <td>-1.973948</td>\n",
       "      <td>-0.802720</td>\n",
       "      <td>-0.429874</td>\n",
       "      <td>-1.245794</td>\n",
       "      <td>0.509807</td>\n",
       "      <td>-4.759445</td>\n",
       "      <td>-2.419918</td>\n",
       "      <td>-0.355657</td>\n",
       "      <td>...</td>\n",
       "      <td>8.426150</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "      <td>-0.118678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518 rows × 6084 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "34   0.122405 -0.602627 -0.962090 -0.373393  0.087622 -0.395953  0.665469   \n",
       "627 -0.745102 -1.319972 -1.137262 -0.828373 -0.100150 -0.273584  0.508537   \n",
       "592 -1.108298 -1.835012 -1.133221 -0.726423 -0.530001 -1.330731 -1.639542   \n",
       "530  0.832510  0.480655  0.677306  0.382840  0.190721 -0.118106  0.180867   \n",
       "443  0.200511  1.014885  1.385173  0.913613  0.745622  0.396749 -0.745719   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "9    1.507697  1.086205  1.644558  1.474270 -2.304456 -0.379790  0.718619   \n",
       "359 -0.855198  1.012120  0.686110  0.021326  0.500773  0.143984 -1.111080   \n",
       "192  0.520318  0.075699  0.494057  0.007725  0.658465  0.811826  0.411325   \n",
       "629 -2.786142 -0.523502 -0.856285 -0.245399 -0.068380 -0.054709 -2.941933   \n",
       "559  1.342915 -4.057653 -1.973948 -0.802720 -0.429874 -1.245794  0.509807   \n",
       "\n",
       "            7         8         9  ...      6074      6075      6076  \\\n",
       "34   0.815541  0.836532  0.860886  ... -0.118678 -0.118678 -0.118678   \n",
       "627 -0.248048 -0.594027 -0.255724  ... -0.118678 -0.118678 -0.118678   \n",
       "592 -2.489729 -2.327456 -2.254350  ... -0.118678 -0.118678 -0.118678   \n",
       "530  0.882195  0.817981  0.662049  ... -0.118678 -0.118678 -0.118678   \n",
       "443  0.204117  0.234500  0.226061  ... -0.118678 -0.118678 -0.118678   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "9    0.913073  0.959412  0.971583  ... -0.118678 -0.118678 -0.118678   \n",
       "359  0.031450  0.429969 -0.844280  ... -0.118678 -0.118678 -0.118678   \n",
       "192 -0.106081  0.321837  0.033495  ... -0.118678 -0.118678 -0.118678   \n",
       "629 -0.661904 -0.925709 -0.882008  ... -0.118678 -0.118678 -0.118678   \n",
       "559 -4.759445 -2.419918 -0.355657  ...  8.426150 -0.118678 -0.118678   \n",
       "\n",
       "         6077      6078      6079      6080      6081      6082      6083  \n",
       "34  -0.118678 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678  \n",
       "627 -0.118678 -0.118678 -0.118678 -0.118678  8.426150 -0.118678 -0.118678  \n",
       "592  8.426150 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678  \n",
       "530 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678  \n",
       "443 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "9   -0.118678 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678  \n",
       "359 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678  \n",
       "192 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678  \n",
       "629 -0.118678 -0.118678 -0.118678 -0.118678  8.426150 -0.118678 -0.118678  \n",
       "559 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678 -0.118678  \n",
       "\n",
       "[518 rows x 6084 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2162df9f-c9be-4fb4-bd14-a09ab15b702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1.0 10.0 100.0 1000.0 10000.0 100000.0 1000000.0 10000000.0 100000000.0 \n",
      "\tBest λ 1: 10.0\n",
      "\tVal R2 1: 0.7478\n",
      "1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1.0 10.0 100.0 1000.0 10000.0 100000.0 1000000.0 10000000.0 100000000.0 \n",
      "\tBest λ 2: 0.01\n",
      "\tVal R2 2: 0.7572\n",
      "1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1.0 10.0 100.0 1000.0 10000.0 100000.0 1000000.0 10000000.0 100000000.0 \n",
      "\tBest λ 3: 0.01\n",
      "\tVal R2 3: 0.8292\n",
      "Total time: 26.35 minutes\n",
      "Final Val R2: 0.8299\n",
      "Final Test R2: 0.8006\n"
     ]
    }
   ],
   "source": [
    "best_lam, res, model = kfold_rr_multi_lambda_tuning(\n",
    "    x_train, y_train, \n",
    "    grid=np.logspace(-8, 8, base = 10, num = 17), \n",
    "    start=[0, x_train.shape[1]-(72+12), x_train.shape[1]-72],\n",
    "    end=[x_train.shape[1]-(72+12), x_train.shape[1]-72, x_train.shape[1]], \n",
    "    static_lam=1,\n",
    "    verbose=True,\n",
    "    show_linalg_warning=False,\n",
    "    fit_model_after_tuning=True\n",
    ")\n",
    "kfold = KFold(n_splits=5)\n",
    "val_predictions = cross_val_predict(model, X = x_train, y = y_train, cv = kfold) \n",
    "print(f\"\"\"Final Val R2: {r2_score(y_train, val_predictions):0.4f}\n",
    "Final Test R2: {r2_score(y_test, model.predict(x_test)):0.4f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0defbe9a-1594-498b-9a81-ea862a2b799e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5479d4e1-7d56-4c27-805c-a2a422919e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad37df9-95ea-41fc-bc76-ce41622df3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2e8b229-7b4e-43e8-85c2-651aa1a04756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linalg warning on lambda=10000000.0: \n",
      "we will allow this model upon model selection\n",
      "linalg warning on lambda=100000000.0: \n",
      "we will allow this model upon model selection\n",
      "linalg warning on lambda=10000000.0: \n",
      "we will allow this model upon model selection\n",
      "linalg warning on lambda=100000000.0: \n",
      "we will allow this model upon model selection\n",
      "linalg warning on lambda=10000000.0: \n",
      "we will allow this model upon model selection\n",
      "linalg warning on lambda=100000000.0: \n",
      "we will allow this model upon model selection\n",
      "linalg warning on lambda=10000000.0: \n",
      "we will allow this model upon model selection\n",
      "linalg warning on lambda=100000000.0: \n",
      "we will allow this model upon model selection\n",
      "linalg warning on lambda=10000000.0: \n",
      "we will allow this model upon model selection\n",
      "linalg warning on lambda=100000000.0: \n",
      "we will allow this model upon model selection\n",
      "The optimal hyperparameter is the lowest of the acceptable (i.e. no precision warnings) hyperparameters supplied. It is index 0 of the orignal hyperparamters passed in. For reference, 17 of 17 hyperparamters are considered acceptable; their indices  are [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16].\n",
      "The optimal hyperparameter is the lowest of the acceptable (i.e. no precision warnings) hyperparameters supplied. It is index 0 of the orignal hyperparamters passed in. For reference, 17 of 17 hyperparamters are considered acceptable; their indices  are [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16].\n",
      "The optimal hyperparameter is the lowest of the acceptable (i.e. no precision warnings) hyperparameters supplied. It is index 0 of the orignal hyperparamters passed in. For reference, 17 of 17 hyperparamters are considered acceptable; their indices  are [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linalg warning on lambda=100000000.0: \n",
      "we will allow this model upon model selection\n",
      "linalg warning on lambda=100000000.0: \n",
      "we will allow this model upon model selection\n",
      "linalg warning on lambda=100000000.0: \n",
      "we will allow this model upon model selection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.10/site-packages/numpy/core/shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 44s, sys: 8min 48s, total: 19min 32s\n",
      "Wall time: 6min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lambdas=np.logspace(-8, 8, base = 10, num = 17)\n",
    "\n",
    "kfold_results = kfold_solve_custom_split_col(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    locations=x_train.index,\n",
    "    split_col=x_train.reset_index().index,\n",
    "    lambdas=lambdas,\n",
    "    static_lam_val=1e-8,\n",
    "    static_lam_idxs=list(range(0,x_train.shape[1]-72)),\n",
    "    intercept=True,\n",
    "    num_folds=5,\n",
    "    random_state=0,\n",
    "    return_preds=True,\n",
    "    return_model=False,\n",
    "    svd_solve=False,\n",
    "    allow_linalg_warning_instances=True,\n",
    "    fit_model_after_tuning=False,\n",
    ")\n",
    "best_alpha_1_idx = interpret_kfold_results(kfold_results, \"r2_score\")[0][0][0]\n",
    "best_alpha_1 = lambdas[best_alpha_1_idx]\n",
    "preds = np.maximum(get_pred_truth_locs(kfold_results)[0].flatten(), 0)\n",
    "truth = get_pred_truth_locs(kfold_results)[1].flatten()\n",
    "# print(\n",
    "# f\"\"\"Best alpha 1: {best_alpha_1}\n",
    "# Val R2: {r2_score(truth, preds):0.4f}\\n\"\"\"\n",
    "# )\n",
    "\n",
    "kfold_results = kfold_solve_custom_split_col(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    locations=x_train.index,\n",
    "    split_col=x_train.reset_index().index,\n",
    "    lambdas=lambdas,\n",
    "    static_lam_val=best_alpha_1,\n",
    "    static_lam_idxs=list(range(x_train.shape[1]-72, x_train.shape[1])),\n",
    "    intercept=True,\n",
    "    num_folds=5,\n",
    "    random_state=0,\n",
    "    return_preds=True,\n",
    "    return_model=False,\n",
    "    svd_solve=False,\n",
    "    allow_linalg_warning_instances=True,\n",
    "    fit_model_after_tuning=False,\n",
    ")\n",
    "best_alpha_2_idx = interpret_kfold_results(kfold_results, \"r2_score\")[0][0][0]\n",
    "best_alpha_2 = lambdas[best_alpha_2_idx]\n",
    "preds = np.maximum(get_pred_truth_locs(kfold_results)[0].flatten(), 0)\n",
    "truth = get_pred_truth_locs(kfold_results)[1].flatten()\n",
    "\n",
    "\n",
    "model, intercept_term = custom_ridge(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    lam=best_alpha_1, \n",
    "    intercept=True,\n",
    "    static_lam_val=best_alpha_2,\n",
    "    static_lam_idxs=list(range(x_train.shape[1]-72, x_train.shape[1])))\n",
    "pred_test = np.asarray(x_test).dot(model) + intercept_term \n",
    "pred_test = np.maximum(pred_test, 0)\n",
    "\n",
    "# print(\n",
    "# f\"\"\"Best alpha 2: {best_alpha_2}\n",
    "# Fianl Val R2: {r2_score(truth, preds):0.4f}\n",
    "# Final test R2: {r2_score(y_test, pred_test):0.4f}\\n\"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50916045-31e0-479f-b845-705599b6fe82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha 2: 1000.0\n",
      "Fianl Val R2: 0.7900\n",
      "Final test R2: 0.0057\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "f\"\"\"Best alpha 2: {best_alpha_2}\n",
    "Fianl Val R2: {r2_score(truth, preds):0.4f}\n",
    "Final test R2: {r2_score(y_test, pred_test):0.4f}\\n\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bc098c4-9b76-49fb-9ce2-dad9303caa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_train_pred_scatterplot(task = \"Validation\", y_test = y_test, preds_test = pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f052b17-16e5-43e0-b380-05e5ae15ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_train_pred_scatterplot(task = \"Validation\", y_test = truth, preds_test = preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28524161-4f05-4bd1-a55e-981ad7842c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1.0 10.0 100.0 1000.0 10000.0 100000.0 1000000.0 10000000.0 100000000.0 \n",
      "\tBest λ 1: 0.1\n",
      "\tVal R2 1: 0.7755\n",
      "1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1.0 10.0 100.0 1000.0 10000.0 100000.0 1000000.0 10000000.0 100000000.0 \n",
      "\tBest λ 2: 0.001\n",
      "\tVal R2 2: 0.7837\n",
      "Total time: 24.49 minutes\n",
      "Val  R2: 0.78\n",
      "Test R2: 0.72 \n",
      "\n",
      "Demean Val  R2: -0.02 \n",
      "Demean Test R2: -0.02\n",
      "CPU times: user 42min 44s, sys: 21min 2s, total: 1h 3min 47s\n",
      "Wall time: 25min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### TESTING\n",
    "file         = 'landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_15k-points_1000-features_yr-2013-2021_mn-4-9_lm-True_cm-False_wa-False_summary.feather'\n",
    "hot_encode   = True\n",
    "\n",
    "#########################################     SET PARAMS    #########################################\n",
    "# file         = params[0]\n",
    "# hot_encode   = params[1]\n",
    "f            = file.split(sep=\"_\")\n",
    "satellite    = f[0]\n",
    "bands        = f[1].replace(\"bands-\", \"\")\n",
    "country_code = f[2]\n",
    "points       = f[3].replace(\"k-points\", \"\")\n",
    "num_features = f[4].replace(\"-features\", \"\")\n",
    "yrs          = f[5].replace(\"yr-\", \"\").split(sep=\"-\")\n",
    "mns          = f[6].replace(\"mn-\", \"\").split(sep=\"-\")\n",
    "limit_months = str2bool(f[7].replace(\"lm-\", \"\"))\n",
    "crop_mask    = str2bool(f[8].replace(\"cm-\", \"\"))\n",
    "weighted_avg = str2bool(f[9].replace(\"wa-\", \"\"))\n",
    "years        = range(int(yrs[0]), int(yrs[1])+1)\n",
    "month_range  = list(range(int(mns[0]), int(mns[1])+1))\n",
    "\n",
    "#########################################     READ DATA    #########################################\n",
    "fn = f\"{directory}/{file}\"\n",
    "features = pd.read_feather(fn)\n",
    "features.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "\n",
    "drop_cols = ['district', 'year', 'yield_mt']\n",
    "\n",
    "crop_yield = features.copy().loc[:, tuple(drop_cols)]\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "    \n",
    "########################################     HOT ENCODE    ###########################################\n",
    "if hot_encode:\n",
    "    drop_cols.remove(\"district\")\n",
    "    features = pd.get_dummies(features, columns=[\"district\"], drop_first=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = features.drop(drop_cols, axis = 1) \n",
    "y_all = np.log10(features.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV    ###########################################\n",
    "### SETUP\n",
    "ridge  = Ridge()  \n",
    "kfold  = KFold(n_splits=5)\n",
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER(S)\n",
    "if hot_encode:\n",
    "    best_lambdas, best_scores, best_model = kfold_rr_multi_lambda_tuning(\n",
    "        X=x_train, y=y_train, \n",
    "        grid=alphas.get('alpha'), \n",
    "        start=[0, x_train.shape[1]-72],\n",
    "        end=[x_train.shape[1]-72, x_train.shape[1]], \n",
    "        static_lam=1e-16,\n",
    "        verbose=True,\n",
    "        show_linalg_warning=False,\n",
    "        fit_model_after_tuning=True\n",
    "    )\n",
    "else:\n",
    "    search = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold).fit(x_train, y_train)\n",
    "    best_model   = search.best_estimator_\n",
    "    best_scores  = search.best_score_\n",
    "    best_lambdas = best_model.alpha\n",
    "### PREDICT WITH BEST HYPERPARAMETER(S)\n",
    "val_predictions   = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################    \n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split['cv_prediction'] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"]-train_split.groupby('district')['log_yield'].transform('mean')\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\"cv_prediction\"]-train_split.groupby('district')['cv_prediction'].transform('mean')\n",
    "\n",
    "test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "#########################################     SAVE RESULTS    #########################################\n",
    "d = {\n",
    "    'country'     : country_code,\n",
    "    'satellite'   : satellite,\n",
    "    'bands'       : bands,\n",
    "    'num_features': num_features,\n",
    "    'points'      : points, \n",
    "    'month_range' : f'{min(month_range)}-{max(month_range)}',\n",
    "\n",
    "    'limit_months': limit_months,\n",
    "    'crop_mask'   : crop_mask,\n",
    "    'weighted_avg': weighted_avg,\n",
    "    'hot_encode': hot_encode,\n",
    "\n",
    "    'total_n': len(x_all),\n",
    "    'train_n': len(x_train),\n",
    "    'test_n' : len(x_test),\n",
    "\n",
    "    'best_reg_param': [best_lambdas],\n",
    "    'mean_of_val_R2': [best_scores],\n",
    "    'val_R2': r2_score(y_train, val_predictions),\n",
    "    'val_r' : pearsonr(val_predictions, y_train)[0],\n",
    "    'val_r2': pearsonr(val_predictions, y_train)[0] ** 2,\n",
    "\n",
    "    'train_R2': r2_score(y_train, train_predictions),\n",
    "    'train_r' : pearsonr(train_predictions, y_train)[0],\n",
    "    'train_r2': pearsonr(train_predictions, y_train)[0] ** 2,\n",
    "\n",
    "    'test_R2': r2_score(y_test, test_predictions),\n",
    "    'test_r' : pearsonr(test_predictions, y_test)[0],\n",
    "    'test_r2': pearsonr(test_predictions, y_test)[0] ** 2,\n",
    "\n",
    "    'demean_cv_R2': r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction),\n",
    "    'demean_cv_r':  pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0],\n",
    "    'demean_cv_r2': pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0] ** 2,\n",
    "}\n",
    "\n",
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"]-test_split.groupby('district')['log_yield'].transform('mean')\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"]-test_split.groupby('district')['prediction'].transform('mean')\n",
    "\n",
    "print(f'Val  R2: {r2_score(y_train, val_predictions):0.2f}\\nTest R2: {r2_score(y_test, test_predictions):0.2f}',\n",
    "     f'\\n\\nDemean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.2f}',\n",
    "     f'\\nDemean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8672f872-5885-48de-82cd-25a5bca983d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>satellite</th>\n",
       "      <th>bands</th>\n",
       "      <th>num_features</th>\n",
       "      <th>points</th>\n",
       "      <th>month_range</th>\n",
       "      <th>limit_months</th>\n",
       "      <th>crop_mask</th>\n",
       "      <th>weighted_avg</th>\n",
       "      <th>hot_encode</th>\n",
       "      <th>total_n</th>\n",
       "      <th>train_n</th>\n",
       "      <th>test_n</th>\n",
       "      <th>best_reg_param</th>\n",
       "      <th>mean_of_val_R2</th>\n",
       "      <th>val_R2</th>\n",
       "      <th>val_r</th>\n",
       "      <th>val_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZMB</td>\n",
       "      <td>landsat-8-c2-l2</td>\n",
       "      <td>1-2-3-4-5-6-7</td>\n",
       "      <td>1000</td>\n",
       "      <td>15</td>\n",
       "      <td>4-9</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>648</td>\n",
       "      <td>518</td>\n",
       "      <td>130</td>\n",
       "      <td>[0.1, 0.001]</td>\n",
       "      <td>[0.7754502460328604, 0.7837355325757185]</td>\n",
       "      <td>0.784073</td>\n",
       "      <td>0.885803</td>\n",
       "      <td>0.784646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country        satellite          bands num_features points month_range  \\\n",
       "0     ZMB  landsat-8-c2-l2  1-2-3-4-5-6-7         1000     15         4-9   \n",
       "\n",
       "   limit_months  crop_mask  weighted_avg  hot_encode  total_n  train_n  \\\n",
       "0          True      False         False        True      648      518   \n",
       "\n",
       "   test_n best_reg_param                            mean_of_val_R2    val_R2  \\\n",
       "0     130   [0.1, 0.001]  [0.7754502460328604, 0.7837355325757185]  0.784073   \n",
       "\n",
       "      val_r    val_r2  \n",
       "0  0.885803  0.784646  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(d).iloc[:, 0:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9822c9d-42c4-4f92-aa93-c5def08bae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TESTING\n",
    "# file         = 'landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_15k-points_1000-features_yr-2013-2021_mn-4-9_lm-True_cm-False_wa-False_summary.feather'\n",
    "# hot_encode   = True\n",
    "\n",
    "# #########################################     SET PARAMS    #########################################\n",
    "# # file         = params[0]\n",
    "# # hot_encode   = params[1]\n",
    "# f            = file.split(sep=\"_\")\n",
    "# satellite    = f[0]\n",
    "# bands        = f[1].replace(\"bands-\", \"\")\n",
    "# country_code = f[2]\n",
    "# points       = f[3].replace(\"k-points\", \"\")\n",
    "# num_features = f[4].replace(\"-features\", \"\")\n",
    "# yrs          = f[5].replace(\"yr-\", \"\").split(sep=\"-\")\n",
    "# mns          = f[6].replace(\"mn-\", \"\").split(sep=\"-\")\n",
    "# limit_months = str2bool(f[7].replace(\"lm-\", \"\"))\n",
    "# crop_mask    = str2bool(f[8].replace(\"cm-\", \"\"))\n",
    "# weighted_avg = str2bool(f[9].replace(\"wa-\", \"\"))\n",
    "# years        = range(int(yrs[0]), int(yrs[1])+1)\n",
    "# month_range  = list(range(int(mns[0]), int(mns[1])+1))\n",
    "\n",
    "# #########################################     READ DATA    #########################################\n",
    "# fn = f\"{directory}/{file}\"\n",
    "# features = pd.read_feather(fn)\n",
    "# features.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "# climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "\n",
    "# drop_cols = ['district', 'year', 'yield_mt']\n",
    "\n",
    "# #########################################    JOIN CLIMATE VARS    #########################################  \n",
    "# ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains('ndvi')]\n",
    "# keep_cols = [*ndvi_cols, *drop_cols]\n",
    "# climate_df = climate_df.loc[:, keep_cols]\n",
    "\n",
    "# features = features.set_index(drop_cols).join(climate_df.set_index(drop_cols)).reset_index()\n",
    "# features = features[features.year <= max(climate_df.year)]\n",
    "\n",
    "# crop_yield = features.copy().loc[:, tuple(drop_cols)]\n",
    "# crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "# #########################################    STANDARDIZE FEATURES    #########################################    \n",
    "# features = features.set_index(drop_cols) \n",
    "# features_scaled = StandardScaler().fit_transform(features.values)\n",
    "# features = pd.DataFrame(features_scaled, index=features.index).reset_index()\n",
    "# features.columns = features.columns.astype(str)          \n",
    "\n",
    "# features.yield_mt = np.log10(features.yield_mt.to_numpy() + 1)\n",
    "\n",
    "# #########################################     HOT ENCODE    ###########################################\n",
    "# if hot_encode:\n",
    "#     drop_cols.remove(\"district\")\n",
    "#     features = pd.get_dummies(features, columns=[\"district\"], drop_first=False)\n",
    "#     # features = mfe(\n",
    "#     #     df = features,\n",
    "#     #     var_cols=features.set_index(['district', 'year']).columns,\n",
    "#     #     group_cols='district'\n",
    "#     # )\n",
    "# else:\n",
    "#     pass\n",
    "\n",
    "# #########################################     K-FOLD SPLIT    #########################################\n",
    "# x_all = features.drop(drop_cols, axis = 1) \n",
    "# y_all = features.yield_mt\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "# \n",
    "# #########################################     K-FOLD CV    ###########################################\n",
    "# ### SETUP\n",
    "# alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "# kfold  = KFold()\n",
    "# ridge  = Ridge()   \n",
    "# ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "# ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "# ridge_reg.fit(x_train, y_train)\n",
    "# best_model = ridge_reg.best_estimator_\n",
    "# ### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "# val_predictions = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "# train_predictions = best_model.predict(x_train)\n",
    "# test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "# #########################################     DE-MEAN R2    #########################################    \n",
    "# crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "# train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "# train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "# train_split['cv_prediction'] = np.maximum(val_predictions, 0)\n",
    "# train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"]-train_split.groupby('district')['log_yield'].transform('mean')\n",
    "# train_split[\"demean_cv_prediction\"] = train_split[\"cv_prediction\"]-train_split.groupby('district')['cv_prediction'].transform('mean')\n",
    "\n",
    "# test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "# test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "# test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "# test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "# test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "# predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "# #########################################     SAVE MODELS   #########################################  \n",
    "# #     model_fn_suffix = file.replace('_summary.feather', '')\n",
    "# #     k_model_fn  = f'kfold-cv_rr-model_{model_fn_suffix}_he-{hot_encode}.pkl'\n",
    "\n",
    "# #     with open(here('models', k_model_fn),'wb') as f:\n",
    "# #         pickle.dump(best_model, f)\n",
    "\n",
    "# #########################################     SAVE RESULTS    #########################################\n",
    "# d = {\n",
    "#     'country'     : country_code,\n",
    "#     'satellite'   : satellite,\n",
    "#     'bands'       : bands,\n",
    "#     'num_features': num_features,\n",
    "#     'points'      : points, \n",
    "#     'month_range' : f'{min(month_range)}-{max(month_range)}',\n",
    "\n",
    "#     'limit_months': limit_months,\n",
    "#     'crop_mask'   : crop_mask,\n",
    "#     'weighted_avg': weighted_avg,\n",
    "#     'hot_encode': hot_encode,\n",
    "\n",
    "#     'total_n': len(x_all),\n",
    "#     'train_n': len(x_train),\n",
    "#     'test_n' : len(x_test),\n",
    "\n",
    "#     'best_reg_param': list(ridge_reg.best_params_.values())[0],\n",
    "#     'mean_of_val_R2': ridge_reg.best_score_,\n",
    "#     'val_R2': r2_score(y_train, val_predictions),\n",
    "#     'val_r' : pearsonr(val_predictions, y_train)[0],\n",
    "#     'val_r2': pearsonr(val_predictions, y_train)[0] ** 2,\n",
    "\n",
    "#     'train_R2': r2_score(y_train, train_predictions),\n",
    "#     'train_r' : pearsonr(train_predictions, y_train)[0],\n",
    "#     'train_r2': pearsonr(train_predictions, y_train)[0] ** 2,\n",
    "\n",
    "#     'test_R2': r2_score(y_test, test_predictions),\n",
    "#     'test_r' : pearsonr(test_predictions, y_test)[0],\n",
    "#     'test_r2': pearsonr(test_predictions, y_test)[0] ** 2,\n",
    "\n",
    "#     'demean_cv_R2': r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction),\n",
    "#     'demean_cv_r':  pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0],\n",
    "#     'demean_cv_r2': pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0] ** 2,\n",
    "# }\n",
    "\n",
    "# test_split[\"demean_test_yield\"] = test_split[\"log_yield\"]-test_split.groupby('district')['log_yield'].transform('mean')\n",
    "# test_split[\"demean_test_prediction\"] = test_split[\"prediction\"]-test_split.groupby('district')['prediction'].transform('mean')\n",
    "\n",
    "# print(f'Val  R2: {r2_score(y_train, val_predictions):0.2f}',\n",
    "#       f'\\nTest R2: {r2_score(y_test, test_predictions):0.2f}',\n",
    "#      f'\\n\\nDemean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.2f}',\n",
    "#      f'\\nDemean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1e44e3-5182-4b69-be53-073a1c6778ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743ae49a-2112-4a22-866c-786ec63f945c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
