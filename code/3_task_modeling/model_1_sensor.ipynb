{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa45eeff-1bf9-40bf-a5ff-9b412d0a787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import warnings\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from pyhere import here\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import pickle\n",
    "\n",
    "import pyarrow\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import p_tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneGroupOut, cross_val_score, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr,  pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f32198-986c-413d-9e1a-47faf0e55b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(string):\n",
    "    return string.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "point_pattern = re.compile(\"20k-points\")\n",
    "wa_pattern = re.compile(\"cm-False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "353ccafc-8838-44ad-9e39-8ed4595cd09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = here(\"data\")\n",
    "directory = here(\"data\", \"random_features\", \"summary\")\n",
    "files = os.listdir(directory)\n",
    "files = [f for f in files if f not in ('.gitkeep', '.ipynb_checkpoints')]\n",
    "files = [f for f in files if not (bool(point_pattern.search(f)) & bool(wa_pattern.search(f)))]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6877398-a7af-4d30-8be5-f2f5b92c8a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramlist = list(itertools.product(files, [True, False]))\n",
    "len(paramlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7ae5aca-1f61-4214-a348-11ca6f4b5f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(params):\n",
    "#########################################     SET PARAMS    #########################################\n",
    "    file         = params[0]\n",
    "    hot_encode   = params[1]\n",
    "    f            = file.split(sep=\"_\")\n",
    "    satellite    = f[0]\n",
    "    bands        = f[1].replace(\"bands-\", \"\")\n",
    "    country_code = f[2]\n",
    "    points       = f[3].replace(\"k-points\", \"\")\n",
    "    num_features = f[4].replace(\"-features\", \"\")\n",
    "    yrs          = f[5].replace(\"yr-\", \"\").split(sep=\"-\")\n",
    "    mns          = f[6].replace(\"mn-\", \"\").split(sep=\"-\")\n",
    "    limit_months = str2bool(f[7].replace(\"lm-\", \"\"))\n",
    "    crop_mask    = str2bool(f[8].replace(\"cm-\", \"\"))\n",
    "    weighted_avg = str2bool(f[9].replace(\"wa-\", \"\"))\n",
    "    years        = range(int(yrs[0]), int(yrs[1])+1)\n",
    "    month_range  = list(range(int(mns[0]), int(mns[1])+1))\n",
    "    \n",
    "#########################################     READ DATA    #########################################\n",
    "    fn = f\"{directory}/{file}\"\n",
    "    features = pd.read_feather(fn)\n",
    "    features.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "    climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "    \n",
    "    drop_cols = ['district', 'year', 'yield_mt']\n",
    "    \n",
    "#########################################    JOIN CLIMATE VARS    #########################################  \n",
    "    ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains('ndvi')]\n",
    "    keep_cols = [*ndvi_cols, *drop_cols]\n",
    "    climate_df = climate_df.loc[:, keep_cols]\n",
    "    \n",
    "    features = features.set_index(drop_cols).join(climate_df.set_index(drop_cols)).reset_index()\n",
    "    features = features[features.year <= max(climate_df.year)]\n",
    "    \n",
    "    crop_yield = features.copy().loc[:, tuple(drop_cols)]\n",
    "    crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "    \n",
    "#########################################     HOT ENCODE    ###########################################\n",
    "    if hot_encode:\n",
    "        drop_cols.remove(\"district\")\n",
    "        features = pd.get_dummies(features, columns=[\"district\"], drop_first=False)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "    features = features.set_index(drop_cols) \n",
    "    features_scaled = StandardScaler().fit_transform(features.values)\n",
    "    features = pd.DataFrame(features_scaled, index=features.index).reset_index()\n",
    "    features.columns = features.columns.astype(str)          \n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "    x_all = features.drop(drop_cols, axis = 1) \n",
    "    y_all = np.log10(features.yield_mt.to_numpy() + 1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV    ###########################################\n",
    "    ### SETUP\n",
    "    alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "    kfold  = KFold()\n",
    "    ridge  = Ridge()   \n",
    "    ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "    ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "    ridge_reg.fit(x_train, y_train)\n",
    "    best_model = ridge_reg.best_estimator_\n",
    "    ### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "    val_predictions = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "    train_predictions = best_model.predict(x_train)\n",
    "    test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################    \n",
    "    crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "    train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "    train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "    train_split['cv_prediction'] = np.maximum(val_predictions, 0)\n",
    "    train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"]-train_split.groupby('district')['log_yield'].transform('mean')\n",
    "    train_split[\"demean_cv_prediction\"] = train_split[\"cv_prediction\"]-train_split.groupby('district')['cv_prediction'].transform('mean')\n",
    "\n",
    "    test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "    test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "    test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "    test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "    test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "    predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "#########################################     SAVE MODELS   #########################################  \n",
    "#     model_fn_suffix = file.replace('_summary.feather', '')\n",
    "#     k_model_fn  = f'kfold-cv_rr-model_{model_fn_suffix}_he-{hot_encode}.pkl'\n",
    "\n",
    "#     with open(here('models', k_model_fn),'wb') as f:\n",
    "#         pickle.dump(best_model, f)\n",
    "\n",
    "#########################################     SAVE RESULTS    #########################################\n",
    "    d = {\n",
    "        'country'     : country_code,\n",
    "        'satellite'   : satellite,\n",
    "        'bands'       : bands,\n",
    "        'num_features': num_features,\n",
    "        'points'      : points, \n",
    "        'month_range' : f'{min(month_range)}-{max(month_range)}',\n",
    "        \n",
    "        'limit_months': limit_months,\n",
    "        'crop_mask'   : crop_mask,\n",
    "        'weighted_avg': weighted_avg,\n",
    "        'hot_encode': hot_encode,\n",
    "\n",
    "        'total_n': len(x_all),\n",
    "        'train_n': len(x_train),\n",
    "        'test_n' : len(x_test),\n",
    "\n",
    "        'best_reg_param': list(ridge_reg.best_params_.values())[0],\n",
    "        'mean_of_val_R2': ridge_reg.best_score_,\n",
    "        'val_R2': r2_score(y_train, val_predictions),\n",
    "        'val_r' : pearsonr(val_predictions, y_train)[0],\n",
    "        'val_r2': pearsonr(val_predictions, y_train)[0] ** 2,\n",
    "\n",
    "        'train_R2': r2_score(y_train, train_predictions),\n",
    "        'train_r' : pearsonr(train_predictions, y_train)[0],\n",
    "        'train_r2': pearsonr(train_predictions, y_train)[0] ** 2,\n",
    "\n",
    "        'test_R2': r2_score(y_test, test_predictions),\n",
    "        'test_r' : pearsonr(test_predictions, y_test)[0],\n",
    "        'test_r2': pearsonr(test_predictions, y_test)[0] ** 2,\n",
    "\n",
    "        'demean_cv_R2': r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction),\n",
    "        'demean_cv_r':  pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0],\n",
    "        'demean_cv_r2': pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0] ** 2,\n",
    "    }\n",
    "    return pd.DataFrame(data=d, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30720303-2152-4ac9-9ba6-c1b14f64275a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time    \n",
    "##### With progress bar\n",
    "workers = os.cpu_count()\n",
    "if __name__ == \"__main__\":\n",
    "    output = []\n",
    "    for result in p_tqdm.p_umap(model, paramlist, num_cpus=workers):\n",
    "        output.append(result)\n",
    "    results = pd.concat(output).reset_index(drop=True)\n",
    "    today = date.today().strftime(\"%Y-%m-%d\")\n",
    "    file_name = f'results_{today}.csv'\n",
    "    print(f\"Saving results as: {file_name}\\n\\n\")           \n",
    "    results.to_csv(here(\"data\",\"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d016b23-b6ad-42f4-b2c9-d70f5f9c6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time    \n",
    "# #### No progress bar\n",
    "# multiprocessing.set_start_method('spawn')\n",
    "# workers = os.cpu_count()\n",
    "# if __name__ == \"__main__\":\n",
    "#     with multiprocessing.Pool(processes=workers) as pool:\n",
    "#         output = []\n",
    "#         for result in pool.imap_unordered(model, paramlist, chunksize=2):\n",
    "#             output.append(result)\n",
    "#     results = pd.concat(output).reset_index(drop=True)\n",
    "#     today = date.today().strftime(\"%Y-%m-%d\")\n",
    "#     file_name = f'results_{today}.csv'\n",
    "#     print(f\"Saving results as: {file_name}\\n\\n\")           \n",
    "#     results.to_csv(here(\"data\",\"results\", file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "716d5d8d-b979-4a28-baef-aa2238994258",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING\n",
    "file         = 'landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_15k-points_1000-features_yr-2013-2021_mn-4-9_lm-True_cm-False_wa-False_summary.feather'\n",
    "hot_encode   = True\n",
    "\n",
    "#########################################     SET PARAMS    #########################################\n",
    "# file         = params[0]\n",
    "# hot_encode   = params[1]\n",
    "f            = file.split(sep=\"_\")\n",
    "satellite    = f[0]\n",
    "bands        = f[1].replace(\"bands-\", \"\")\n",
    "country_code = f[2]\n",
    "points       = f[3].replace(\"k-points\", \"\")\n",
    "num_features = f[4].replace(\"-features\", \"\")\n",
    "yrs          = f[5].replace(\"yr-\", \"\").split(sep=\"-\")\n",
    "mns          = f[6].replace(\"mn-\", \"\").split(sep=\"-\")\n",
    "limit_months = str2bool(f[7].replace(\"lm-\", \"\"))\n",
    "crop_mask    = str2bool(f[8].replace(\"cm-\", \"\"))\n",
    "weighted_avg = str2bool(f[9].replace(\"wa-\", \"\"))\n",
    "years        = range(int(yrs[0]), int(yrs[1])+1)\n",
    "month_range  = list(range(int(mns[0]), int(mns[1])+1))\n",
    "\n",
    "#########################################     READ DATA    #########################################\n",
    "fn = f\"{directory}/{file}\"\n",
    "features = pd.read_feather(fn)\n",
    "features.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "\n",
    "drop_cols = ['district', 'year', 'yield_mt']\n",
    "\n",
    "#########################################    JOIN CLIMATE VARS    #########################################  \n",
    "ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains('ndvi')]\n",
    "keep_cols = [*ndvi_cols, *drop_cols]\n",
    "climate_df = climate_df.loc[:, keep_cols]\n",
    "\n",
    "features = features.set_index(drop_cols).join(climate_df.set_index(drop_cols)).reset_index()\n",
    "features = features[features.year <= max(climate_df.year)]\n",
    "\n",
    "crop_yield = features.copy().loc[:, tuple(drop_cols)]\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "#########################################     HOT ENCODE    ###########################################\n",
    "if hot_encode:\n",
    "    drop_cols.remove(\"district\")\n",
    "    features = pd.get_dummies(features, columns=[\"district\"], drop_first=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "features = features.set_index(drop_cols) \n",
    "features_scaled = StandardScaler().fit_transform(features.values)\n",
    "features = pd.DataFrame(features_scaled, index=features.index).reset_index()\n",
    "features.columns = features.columns.astype(str)          \n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = features.drop(drop_cols, axis = 1) \n",
    "y_all = np.log10(features.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV    ###########################################\n",
    "### SETUP\n",
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "kfold  = KFold()\n",
    "ridge  = Ridge()   \n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################    \n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split['cv_prediction'] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"]-train_split.groupby('district')['log_yield'].transform('mean')\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\"cv_prediction\"]-train_split.groupby('district')['cv_prediction'].transform('mean')\n",
    "\n",
    "test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "#########################################     SAVE MODELS   #########################################  \n",
    "#     model_fn_suffix = file.replace('_summary.feather', '')\n",
    "#     k_model_fn  = f'kfold-cv_rr-model_{model_fn_suffix}_he-{hot_encode}.pkl'\n",
    "\n",
    "#     with open(here('models', k_model_fn),'wb') as f:\n",
    "#         pickle.dump(best_model, f)\n",
    "\n",
    "#########################################     SAVE RESULTS    #########################################\n",
    "d = {\n",
    "    'country'     : country_code,\n",
    "    'satellite'   : satellite,\n",
    "    'bands'       : bands,\n",
    "    'num_features': num_features,\n",
    "    'points'      : points, \n",
    "    'month_range' : f'{min(month_range)}-{max(month_range)}',\n",
    "\n",
    "    'limit_months': limit_months,\n",
    "    'crop_mask'   : crop_mask,\n",
    "    'weighted_avg': weighted_avg,\n",
    "    'hot_encode': hot_encode,\n",
    "\n",
    "    'total_n': len(x_all),\n",
    "    'train_n': len(x_train),\n",
    "    'test_n' : len(x_test),\n",
    "\n",
    "    'best_reg_param': list(ridge_reg.best_params_.values())[0],\n",
    "    'mean_of_val_R2': ridge_reg.best_score_,\n",
    "    'val_R2': r2_score(y_train, val_predictions),\n",
    "    'val_r' : pearsonr(val_predictions, y_train)[0],\n",
    "    'val_r2': pearsonr(val_predictions, y_train)[0] ** 2,\n",
    "\n",
    "    'train_R2': r2_score(y_train, train_predictions),\n",
    "    'train_r' : pearsonr(train_predictions, y_train)[0],\n",
    "    'train_r2': pearsonr(train_predictions, y_train)[0] ** 2,\n",
    "\n",
    "    'test_R2': r2_score(y_test, test_predictions),\n",
    "    'test_r' : pearsonr(test_predictions, y_test)[0],\n",
    "    'test_r2': pearsonr(test_predictions, y_test)[0] ** 2,\n",
    "\n",
    "    'demean_cv_R2': r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction),\n",
    "    'demean_cv_r':  pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0],\n",
    "    'demean_cv_r2': pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0] ** 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e43c710-ed1e-4da6-b253-32746d8ad25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"]-test_split.groupby('district')['log_yield'].transform('mean')\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"]-test_split.groupby('district')['prediction'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51239d6c-e4e5-41bf-90e1-cd00a86d18aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.72 \n",
      "Test R2: 0.68 \n",
      "\n",
      "Demean Val  R2: 0.14 \n",
      "Demean Test R2: -0.08\n"
     ]
    }
   ],
   "source": [
    "print(f'Val  R2: {r2_score(y_train, val_predictions):0.2f}',\n",
    "      f'\\nTest R2: {r2_score(y_test, test_predictions):0.2f}',\n",
    "     f'\\n\\nDemean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.2f}',\n",
    "     f'\\nDemean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85038691-cbcc-4a11-8f66-0e283034a8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
