{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d64e464-31b8-4c88-bbf5-877a8fa59fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import warnings\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from pyhere import here\n",
    "from datetime import date\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import pickle\n",
    "\n",
    "import pyarrow\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import p_tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneGroupOut, cross_val_score, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr,  pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffc9fc-32ea-47c7-93fc-0d0aea13d688",
   "metadata": {},
   "source": [
    "# NDVI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50874f01-370e-40eb-84c1-4f98234d2632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.64 \n",
      "Test R2: 0.60 \n",
      "\n",
      "Demean Val  R2: 0.06 \n",
      "Demean Test R2: 0.15\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = ['year', 'district', 'yield_mt']\n",
    "ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains('ndvi')]\n",
    "keep_cols = [*ndvi_cols, *drop_cols]\n",
    "climate_df = climate_df.loc[:, keep_cols]\n",
    "# climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "hot_encode = True\n",
    "# hot_encode = False\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop = True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    HOT ENCODE    ######################################### \n",
    "if hot_encode:\n",
    "    drop_cols.remove('district')\n",
    "    climate_df = pd.get_dummies(climate_df, columns = [\"district\"], drop_first = False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "climate_df = climate_df.set_index(drop_cols) \n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis = 1) \n",
    "y_all = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge()    \n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################    \n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split['cv_prediction'] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"]-train_split.groupby('district')['log_yield'].transform('mean')\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\"cv_prediction\"]-train_split.groupby('district')['cv_prediction'].transform('mean')\n",
    "\n",
    "test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"]-test_split.groupby('district')['log_yield'].transform('mean')\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"]-test_split.groupby('district')['prediction'].transform('mean')\n",
    "\n",
    "print(f'Val  R2: {r2_score(y_train, val_predictions):0.2f}',\n",
    "      f'\\nTest R2: {r2_score(y_test, test_predictions):0.2f}',\n",
    "     f'\\n\\nDemean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.2f}',\n",
    "     f'\\nDemean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3eef3-18bb-4b2b-8da8-02bc0230bda2",
   "metadata": {},
   "source": [
    "# Precipitation, Temperature, and NDVI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1840a92e-3eca-4039-a5d4-6a8367316314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.75 \n",
      "Test R2: 0.75 \n",
      "\n",
      "Demean Val  R2: 0.35 \n",
      "Demean Test R2: 0.49\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = ['year', 'district', 'yield_mt']\n",
    "# climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "hot_encode = True\n",
    "# hot_encode = False\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop = True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    HOT ENCODE    ######################################### \n",
    "if hot_encode:\n",
    "    drop_cols.remove('district')\n",
    "    climate_df = pd.get_dummies(climate_df, columns = [\"district\"], drop_first = False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "climate_df = climate_df.set_index(drop_cols) \n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis = 1) \n",
    "y_all = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge()    \n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################    \n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split['cv_prediction'] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"]-train_split.groupby('district')['log_yield'].transform('mean')\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\"cv_prediction\"]-train_split.groupby('district')['cv_prediction'].transform('mean')\n",
    "\n",
    "test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"]-test_split.groupby('district')['log_yield'].transform('mean')\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"]-test_split.groupby('district')['prediction'].transform('mean')\n",
    "\n",
    "print(f'Val  R2: {r2_score(y_train, val_predictions):0.2f}',\n",
    "      f'\\nTest R2: {r2_score(y_test, test_predictions):0.2f}',\n",
    "     f'\\n\\nDemean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.2f}',\n",
    "     f'\\nDemean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3dfe35-025c-4b06-8b2b-23b4f223f11b",
   "metadata": {},
   "source": [
    "# NDVI Anomaly Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f20bb178-7708-45e3-aa68-124c88541b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.23\n",
      "Test R2: 0.13\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = ['year', 'district', 'yield_mt']\n",
    "ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains('ndvi')]\n",
    "keep_cols = [*ndvi_cols, *drop_cols]\n",
    "climate_df = climate_df.loc[:, keep_cols]\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop = True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "climate_df = climate_df.set_index(drop_cols) \n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################\n",
    "climate_df['yield_mt'] = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "climate_df.set_index(['year', 'district'], inplace=True)\n",
    "var_cols = climate_df.columns\n",
    "climate_df = climate_df[var_cols] - climate_df.groupby(['district'], as_index=True)[var_cols].transform('mean')\n",
    "climate_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis = 1) \n",
    "y_all = climate_df.yield_mt\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge()    \n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions   = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################    \n",
    "crop_yield[\"prediction\"] = best_model.predict(x_all)\n",
    "\n",
    "train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split['cv_prediction'] = val_predictions\n",
    "\n",
    "test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "print(f'Val  R2: {r2_score(y_train, val_predictions):0.2f}\\nTest R2: {r2_score(y_test, test_predictions):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c574bb5-882f-4588-ae97-44af906398a6",
   "metadata": {},
   "source": [
    "# Precipitation, Temperature, and NDVI  Anomaly model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "102e934f-c6e8-4222-9956-a621dda75298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.47\n",
      "Test R2: 0.47\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = ['year', 'district', 'yield_mt']\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop = True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "climate_df = climate_df.set_index(drop_cols) \n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################\n",
    "climate_df['yield_mt'] = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "climate_df.set_index(['year', 'district'], inplace=True)\n",
    "var_cols = climate_df.columns\n",
    "climate_df = climate_df[var_cols] - climate_df.groupby(['district'], as_index=True)[var_cols].transform('mean')\n",
    "climate_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis = 1) \n",
    "y_all = climate_df.yield_mt\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge()    \n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions   = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################    \n",
    "crop_yield[\"prediction\"] = best_model.predict(x_all)\n",
    "\n",
    "train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split['cv_prediction'] = val_predictions\n",
    "\n",
    "test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "print(f'Val  R2: {r2_score(y_train, val_predictions):0.2f}\\nTest R2: {r2_score(y_test, test_predictions):0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb25ab-e9b1-4bfc-b2e9-a3f4c106a446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mosaiks-env] *",
   "language": "python",
   "name": "conda-env-mosaiks-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
