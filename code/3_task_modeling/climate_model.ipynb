{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fec0a85-fded-480e-bc42-c258cc39c736",
   "metadata": {},
   "source": [
    "# Benchmark Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0585a80-f431-4f2b-bcd1-396386652fab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502744d3-ce9a-42bc-ac76-a5feae8bee55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pyarrow\n",
    "import concurrent.futures\n",
    "\n",
    "from pyhere import here\n",
    "from datetime import date\n",
    "from itertools import product, combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    GridSearchCV,\n",
    "    cross_val_predict,\n",
    ")\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr, pearsonr, t\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "\n",
    "from task_modeling_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2216f1bb-8f61-417f-bbfc-6bba313014c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pre'],\n",
       " ['tmp'],\n",
       " ['ndvi'],\n",
       " ['pre', 'tmp'],\n",
       " ['pre', 'ndvi'],\n",
       " ['tmp', 'ndvi'],\n",
       " ['pre', 'tmp', 'ndvi']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables = [\"pre\", \"tmp\", \"ndvi\"]\n",
    "climate_vars = list(combinations(variables, 2))\n",
    "climate_vars = (\n",
    "    climate_vars + [[variables[i]] for i in range(len(variables))] + [variables]\n",
    ")\n",
    "climate_vars = [list(elem) for elem in climate_vars]\n",
    "climate_vars.sort(key=len)\n",
    "climate_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acc679e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "n_splits = 10\n",
    "random.seed(42)\n",
    "random_seeds = [random.randint(0, 1_000_000) for _ in range(n_splits)]\n",
    "\n",
    "he_anom_combinations = [(True, False), (False, False), (False, True)]\n",
    "\n",
    "kwarg_list = [\n",
    "    {\n",
    "        \"variable_groups\": clim,\n",
    "        \"hot_encode\": he,\n",
    "        \"anomaly\": anom,\n",
    "        \"index_cols\": [\"year\", \"district\", \"yield_mt\"],\n",
    "        \"year_start\": 2016,\n",
    "        \"n_splits\": 5,\n",
    "        \"split\": split,\n",
    "        \"random_state\": random_state,\n",
    "        \"return_oos_predictions\": True,\n",
    "    }\n",
    "    for clim in climate_vars\n",
    "    for he, anom in he_anom_combinations\n",
    "    for split, random_state in enumerate(random_seeds)\n",
    "]\n",
    "\n",
    "len(kwarg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfea5468-aefd-4b02-be4a-f405eee83c22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 210/210 [15:08<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results as: climate_model_10-splits_2023-07-01.csv\n",
      "\n",
      "\n",
      "Saving results as: climate_model_oos_predictions_10-splits_2023-07-01.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    output, oos_preds = [], []\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        # with ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(climate_model, **kwargs): kwargs for kwargs in kwarg_list\n",
    "        }\n",
    "        for future in tqdm(\n",
    "            as_completed(futures), total=len(futures), desc=\"Processing models\"\n",
    "        ):\n",
    "            out, oos = future.result()\n",
    "            output.append(out)\n",
    "            oos_preds.append(oos)\n",
    "\n",
    "    today = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    results = pd.DataFrame(output)\n",
    "    results_fn = f\"climate_model_{n_splits}-splits_{today}.csv\"\n",
    "    print(f\"Saving results as: {results_fn}\\n\\n\")\n",
    "    results.to_csv(here(\"data\", \"results\", results_fn), index=False)\n",
    "\n",
    "    oos_predictions = pd.concat(oos_preds)\n",
    "    oos_fn = f\"climate_model_oos_predictions_{n_splits}-splits_{today}.csv\"\n",
    "    print(f\"Saving results as: {oos_fn}\\n\\n\")\n",
    "    oos_predictions.to_csv(here(\"data\", \"results\", oos_fn), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58286a9-9383-4e70-a4e8-5a84628636f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be0b60b-49c6-4a4d-bac4-ab595fe0f469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oos_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5b40a-4f5e-41ad-a969-1c1debc38afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(output)\n",
    "results\n",
    "# mask = results.anomaly == True\n",
    "# cols = [\n",
    "#     \"demean_cv_R2\",\n",
    "#     \"demean_cv_r\",\n",
    "#     \"demean_cv_r2\",\n",
    "#     \"demean_test_R2\",\n",
    "#     \"demean_test_r\",\n",
    "#     \"demean_test_r2\",\n",
    "# ]\n",
    "# results.loc[mask, cols] = np.nan\n",
    "\n",
    "# today = date.today().strftime(\"%Y-%m-%d\")\n",
    "# file_name = f\"climate_model_{num_seeds}-splits_{today}.csv\"\n",
    "# print(f\"Saving results as: {file_name}\\n\\n\")\n",
    "# results.to_csv(here(\"data\", \"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733736a5-2b9e-4d91-821c-ab7f5962ca14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = results.copy()\n",
    "a = a[a.year_start == 2016]\n",
    "a = a[a.hot_encode]\n",
    "# a = a[a.variables.isin([\"tmp_ndvi\"])]\n",
    "\n",
    "b = get_mean_std_ste(\n",
    "    df=a,\n",
    "    groupby_columns=[\"variables\", \"year_start\", \"hot_encode\", \"anomaly\"],\n",
    "    target_columns=[\"val_R2\", \"test_R2\", \"demean_cv_R2\", \"demean_cv_r2\"],\n",
    ")\n",
    "b.sort_values([\"summary_var\", \"mean\"], ascending=False)\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0087e232-9f79-49ab-8f8e-b6949a9225cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def climate_model(\n",
    "variable_groups = [\"ndvi\"]\n",
    "hot_encode = True\n",
    "anomaly = False\n",
    "index_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "year_start = 2016\n",
    "n_splits = 5\n",
    "split = 0\n",
    "random_state = 42\n",
    "return_oos_predictions = True\n",
    "# ):\n",
    "if variable_groups is None:\n",
    "    variable_groups_str = \"rcf\"\n",
    "else:\n",
    "    variable_groups_str = \"_\".join(variable_groups)\n",
    "#########################################     READ DATA    #########################################\n",
    "data = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "data = data.dropna()\n",
    "\n",
    "#########################################     FILTER DATA    #########################################\n",
    "keep_cols = []\n",
    "\n",
    "for var in variable_groups:\n",
    "    tmp = data.columns[data.columns.to_series().str.contains(var)].tolist()\n",
    "    keep_cols.append(tmp)\n",
    "\n",
    "keep_cols = [*index_cols, *[col for cols in keep_cols for col in cols]]\n",
    "data = data.loc[:, keep_cols]\n",
    "data = data[data.year >= year_start]\n",
    "\n",
    "#########################################    MAKE A COPY    #########################################\n",
    "crop_yield = data.copy().loc[:, tuple(index_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################\n",
    "if anomaly:\n",
    "    data[\"yield_mt\"] = np.log10(data.yield_mt.to_numpy() + 1)\n",
    "    data.set_index([\"year\", \"district\"], inplace=True)\n",
    "    var_cols = data.columns\n",
    "    data = data[var_cols] - data.groupby([\"district\"], as_index=True)[\n",
    "        var_cols\n",
    "    ].transform(\"mean\")\n",
    "\n",
    "else:\n",
    "    pass\n",
    "data.reset_index(drop=False, inplace=True)\n",
    "\n",
    "#########################################    HOT ENCODE    #########################################\n",
    "if hot_encode and not anomaly:\n",
    "    index_cols.remove(\"district\")\n",
    "    data = pd.get_dummies(data, columns=[\"district\"], drop_first=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = data.drop(index_cols, axis=1)\n",
    "y_all = np.log10(data.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=random_state\n",
    ")\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = pd.DataFrame(\n",
    "    scaler.transform(x_train), columns=x_train.columns, index=x_train.index\n",
    ")\n",
    "x_test = pd.DataFrame(\n",
    "    scaler.transform(x_test), columns=x_test.columns, index=x_test.index\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV    #########################################\n",
    "### SETUP\n",
    "tic = time.time()\n",
    "alphas = {\"alpha\": np.logspace(-1, 1, base=10, num=3)}\n",
    "\n",
    "### LAMBDA INDICIES\n",
    "i = 0\n",
    "start = [i]\n",
    "end = [x_train.shape[1]]\n",
    "\n",
    "for var in variable_groups:\n",
    "    i += 12\n",
    "    start.append(i)\n",
    "    end.append(i)\n",
    "start.sort()\n",
    "end.sort()\n",
    "\n",
    "if not hot_encode:\n",
    "    start = start[0:-1]\n",
    "    end = end[0:-1]\n",
    "\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER(S)\n",
    "best_lambdas, best_scores, best_model = kfold_rr_multi_lambda_tuning(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    grid=alphas.get(\"alpha\"),\n",
    "    n_splits=n_splits,\n",
    "    start=start,\n",
    "    end=end,\n",
    "    static_lam=1,\n",
    "    verbose=0,\n",
    "    show_linalg_warning=False,\n",
    "    fit_model_after_tuning=True,\n",
    ")\n",
    "### PREDICT WITH BEST HYPERPARAMETER(S)\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "if anomaly:\n",
    "    pass\n",
    "else:\n",
    "    val_predictions = np.maximum(val_predictions, 0)\n",
    "    train_predictions = np.maximum(train_predictions, 0)\n",
    "    test_predictions = np.maximum(test_predictions, 0)\n",
    "\n",
    "#########################################     DE-MEAN TRAIN R2    #########################################\n",
    "fold_list = []\n",
    "for i in range(n_splits):\n",
    "    idx = len(list(kfold.split(y_train))[i][1])\n",
    "    fold = np.repeat(i + 1, idx).tolist()\n",
    "    fold_list.append(fold)\n",
    "fold_list = [item for sublist in fold_list for item in sublist]\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"data_fold\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"oos_prediction\"] = val_predictions\n",
    "train_split[\"val_fold\"] = fold_list\n",
    "train_split = demean_by_group(\n",
    "    train_split, predicted=\"oos_prediction\", group=[\"district\"]\n",
    ")\n",
    "\n",
    "#########################################     DE-MEAN TEST R2    #########################################\n",
    "test_split = pd.DataFrame(\n",
    "    {\"data_fold\": np.repeat(\"test\", len(x_test))}, index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"oos_prediction\"] = test_predictions\n",
    "test_split[\"val_fold\"] = n_splits + 1\n",
    "test_split = demean_by_group(test_split, predicted=\"oos_prediction\", group=[\"district\"])\n",
    "\n",
    "#########################################     OUT OF SAMPLE PREDICTIONS    #########################################\n",
    "oos_preds = pd.concat([train_split, test_split])\n",
    "oos_preds[[\"split\", \"random_state\"]] = split, random_state\n",
    "oos_preds[\"variables\"] = variable_groups_str\n",
    "\n",
    "#########################################     SCORES    #########################################\n",
    "val_R2 = r2_score(y_train, val_predictions)\n",
    "val_r = pearsonr(val_predictions, y_train)[0]\n",
    "train_R2 = r2_score(y_train, train_predictions)\n",
    "train_r = pearsonr(train_predictions, y_train)[0]\n",
    "test_R2 = r2_score(y_test, test_predictions)\n",
    "test_r = pearsonr(test_predictions, y_test)[0]\n",
    "\n",
    "if anomaly:\n",
    "    demean_cv_R2 = np.nan\n",
    "    demean_cv_r = np.nan\n",
    "    demean_test_R2 = np.nan\n",
    "    demean_test_r = np.nan\n",
    "else:\n",
    "    demean_cv_R2 = r2_score(\n",
    "        train_split.demean_log_yield, train_split.demean_oos_prediction\n",
    "    )\n",
    "    demean_cv_r = pearsonr(\n",
    "        train_split.demean_log_yield, train_split.demean_oos_prediction\n",
    "    )[0]\n",
    "    demean_test_R2 = r2_score(\n",
    "        test_split.demean_log_yield, test_split.demean_oos_prediction\n",
    "    )\n",
    "    demean_test_r = pearsonr(\n",
    "        test_split.demean_log_yield, test_split.demean_oos_prediction\n",
    "    )[0]\n",
    "\n",
    "d = {\n",
    "    \"split\": split,\n",
    "    \"random_state\": random_state,\n",
    "    \"variables\": \"_\".join(variable_groups),\n",
    "    \"year_start\": year_start,\n",
    "    \"hot_encode\": hot_encode,\n",
    "    \"anomaly\": anomaly,\n",
    "    \"total_n\": len(x_all),\n",
    "    \"train_n\": len(x_train),\n",
    "    \"test_n\": len(x_test),\n",
    "    \"best_reg_param\": best_lambdas,\n",
    "    \"mean_of_val_R2\": best_scores,\n",
    "    \"val_R2\": val_R2,\n",
    "    \"val_r\": val_r,\n",
    "    \"val_r2\": val_r**2,\n",
    "    \"train_R2\": train_R2,\n",
    "    \"train_r\": train_r,\n",
    "    \"train_r2\": train_r**2,\n",
    "    \"test_R2\": test_R2,\n",
    "    \"test_r\": test_r,\n",
    "    \"test_r2\": test_r**2,\n",
    "    \"demean_cv_R2\": demean_cv_R2,\n",
    "    \"demean_cv_r\": demean_cv_r,\n",
    "    \"demean_cv_r2\": demean_cv_r**2,\n",
    "    \"demean_test_R2\": demean_test_R2,\n",
    "    \"demean_test_r\": demean_test_r,\n",
    "    \"demean_test_r2\": demean_test_r**2,\n",
    "}\n",
    "d, oos_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
