{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aaf7810-5612-46c5-8369-1e95387fe44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pyhere p_tqdm glum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d64e464-31b8-4c88-bbf5-877a8fa59fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from pyhere import here\n",
    "from datetime import date\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import pickle\n",
    "\n",
    "import pyarrow\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import p_tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneGroupOut, cross_val_score, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr,  pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012168f4-4cf6-4ab5-83f1-73731c8f2473",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = ['year', 'district', 'yield_mt']\n",
    "ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains('ndvi')]\n",
    "keep_cols = [*ndvi_cols, *drop_cols]\n",
    "# climate_df = climate_df.loc[:, keep_cols]\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "# climate_df = climate_df[climate_df.year != 2011]\n",
    "\n",
    "hot_encode = True\n",
    "# hot_encode = False\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop = True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "########################################    STANDARDIZE FEATURES    #########################################    \n",
    "climate_df = climate_df.set_index(drop_cols) \n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################    HOT ENCODE    ######################################### \n",
    "if hot_encode:\n",
    "    drop_cols.remove('district')\n",
    "    climate_df = pd.get_dummies(climate_df, columns = [\"district\"], drop_first = False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis = 1) \n",
    "y_all = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b683dfb7-3e4e-4d39-be22-25bf65ef3bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train#.iloc[:,0:37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e5cc991-7fed-4aae-b6f8-d4d8be1cdf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.contrib.itertools import product\n",
    "# from glum import GeneralizedLinearRegressor as glm\n",
    "# import warnings\n",
    "# from scipy.linalg import LinAlgWarning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316fb959-df40-480b-b9dc-fd748d99cb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=x_train\n",
    "# y=y_train\n",
    "# grid=np.logspace(-8, 8, base = 10, num = 17) \n",
    "# n_splits=5\n",
    "# start=[0, 12, 24, 36]\n",
    "# end=[12, 24, 36, 108]\n",
    "# static_lam=1e-16\n",
    "# verbose=True\n",
    "# fit_model_after_tuning=True\n",
    "\n",
    "# kfold = KFold(n_splits=n_splits)\n",
    "# alpha = {'alpha': [1]}\n",
    "\n",
    "# if hasattr(start, '__iter__'):\n",
    "#     pass\n",
    "# else:\n",
    "#     start = [start]; end = [end]\n",
    "    \n",
    "# penalties = [static_lam for j in range(X.shape[1])] \n",
    "# lambdas = []\n",
    "# for i in range(len(start)):\n",
    "#     scores = []\n",
    "#     for pen in grid:\n",
    "#         if verbose:\n",
    "#             print(pen, end=\" \")\n",
    "#         penalties[start[i]:end[i]] = [pen for j in range(end[i]-start[i])]\n",
    "#         ridge = glm(family=\"normal\", P2=penalties, l1_ratio=0, random_state=42)   \n",
    "#         search = GridSearchCV(ridge, alpha, scoring = 'r2', cv = kfold).fit(X, y)\n",
    "#         scores.append(search.best_score_)\n",
    "#     best_lambda = grid[np.argmax(scores)]\n",
    "#     penalties[start[i]:end[i]] = [best_lambda for j in range(end[i]-start[i])]\n",
    "#     if verbose:\n",
    "#         print(f'''\\n\\tBest \\u03BB {i+1}: {best_lambda}\\n\\tVal R2 {i+1}: {scores[np.argmax(scores)]:0.4f}''') \n",
    "#     lambdas.append(best_lambda)\n",
    "# if fit_model_after_tuning:\n",
    "#     for k in range(len(start)):\n",
    "#         penalties[start[k]:end[k]] = [lambdas[k] for j in range(end[k]-start[k])]\n",
    "#     ridge = glm(family=\"normal\", P2=penalties, l1_ratio=0, random_state=42) \n",
    "#     model = GridSearchCV(ridge, alpha, scoring = 'r2', cv = kfold).fit(X, y).best_estimator_\n",
    "# else:\n",
    "#     model = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "069f3d8e-a6d3-4296-8961-79fc459022c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5)\n",
    "import task_modeling_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e9b03d-0841-45f2-a229-6c306b9f6d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01,\n",
       "       1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06, 1.e+07,\n",
       "       1.e+08])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "alphas.get('alpha')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f84358d-827a-44f1-bf46-62269f37a77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1.0 10.0 100.0 1000.0 10000.0 100000.0 1000000.0 10000000.0 100000000.0 \n",
      "\tBest λ 1: 0.01\n",
      "\tVal R2 1: 0.7937\n",
      "1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1.0 10.0 100.0 1000.0 10000.0 100000.0 1000000.0 10000000.0 100000000.0 \n",
      "\tBest λ 2: 0.0001\n",
      "\tVal R2 2: 0.7944\n",
      "Total time: 2.33 minutes\n",
      "Final Val R2: 0.8058\n",
      "Final Test R2: 0.8304\n"
     ]
    }
   ],
   "source": [
    "best_lam, res, model = kfold_rr_multi_lambda_tuning(\n",
    "    x_train, y_train, \n",
    "    grid=np.logspace(-8, 8, base = 10, num = 17), \n",
    "    start=[0, 36],\n",
    "    end=[36, 108], \n",
    "    static_lam=1e-16,\n",
    "    verbose=True,\n",
    "    show_linalg_warning=False,\n",
    "    fit_model_after_tuning=True\n",
    ")\n",
    "val_predictions = cross_val_predict(model, X = x_train, y = y_train, cv = kfold) \n",
    "print(f\"\"\"Final Val R2: {r2_score(y_train, val_predictions):0.4f}\n",
    "Final Test R2: {r2_score(y_test, model.predict(x_test)):0.4f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6557c59-e6bc-4ed6-8817-48fbfc9e5497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1.0 10.0 100.0 1000.0 10000.0 100000.0 1000000.0 10000000.0 100000000.0 \n",
      "\tBest λ 1: 0.1\n",
      "\tVal R2 1: 0.7899\n",
      "1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1.0 10.0 100.0 1000.0 10000.0 100000.0 1000000.0 10000000.0 100000000.0 \n",
      "\tBest λ 2: 0.01\n",
      "\tVal R2 2: 0.7987\n",
      "1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1.0 10.0 100.0 1000.0 10000.0 100000.0 1000000.0 10000000.0 100000000.0 \n",
      "\tBest λ 3: 0.01\n",
      "\tVal R2 3: 0.7994\n",
      "1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1.0 10.0 100.0 1000.0 10000.0 100000.0 1000000.0 10000000.0 100000000.0 \n",
      "\tBest λ 4: 0.0001\n",
      "\tVal R2 4: 0.8016\n",
      "Final Val R2: 0.8127\n",
      "Final Test R2: 0.8381\n"
     ]
    }
   ],
   "source": [
    "best_lam, res, model = kfold_rr_multi_lambda_tuning(\n",
    "    x_train, y_train, grid=np.logspace(-8, 8, base = 10, num = 17), \n",
    "    start=[0, 12, 24, 36], end=[12, 24, 36, 108], static_lam=1e-16\n",
    ")\n",
    "val_predictions = cross_val_predict(model, X = x_train, y = y_train, cv = kfold) \n",
    "print(f\"\"\"Final Val R2: {r2_score(y_train, val_predictions):0.4f}\n",
    "Final Test R2: {r2_score(y_test, model.predict(x_test)):0.4f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd2f4164-7beb-4337-90cb-f423644fd143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Val R2: 0.7971\n",
      "Final Test R2: 0.8190\n",
      "CPU times: user 2.76 s, sys: 10.6 s, total: 13.4 s\n",
      "Wall time: 3.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge()    \n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)\n",
    "print(\n",
    "f\"\"\"Final Val R2: {r2_score(y_train, val_predictions):0.4f}\n",
    "Final Test R2: {r2_score(y_test, test_predictions):0.4f}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5e95643-369d-462c-86dd-9cc8ec301176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prediction_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c4964c2-64e9-40c9-b3d8-54a3bfdfb26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 594 ms, sys: 2.6 s, total: 3.2 s\n",
      "Wall time: 794 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lambdas=np.logspace(-8, 8, base = 10, num = 17)\n",
    "\n",
    "kfold_results = kfold_solve_custom_split_col(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    locations=x_train.index,\n",
    "    split_col=x_train.reset_index().index,\n",
    "    lambdas=lambdas,\n",
    "    static_lam_val=1e-20,\n",
    "    static_lam_idxs=list(range(0,x_train.shape[1]-72)),\n",
    "    intercept=True,\n",
    "    num_folds=5,\n",
    "    random_state=0,\n",
    "    return_preds=True,\n",
    "    return_model=False,\n",
    "    svd_solve=False,\n",
    "    allow_linalg_warning_instances=True,\n",
    "    fit_model_after_tuning=False,\n",
    ")\n",
    "best_alpha_1_idx = interpret_kfold_results(kfold_results, \"r2_score\")[0][0][0]\n",
    "best_alpha_1 = lambdas[best_alpha_1_idx]\n",
    "preds = np.maximum(get_pred_truth_locs(kfold_results)[0].flatten(), 0)\n",
    "truth = get_pred_truth_locs(kfold_results)[1].flatten()\n",
    "# print(\n",
    "# f\"\"\"Best alpha 1: {best_alpha_1}\n",
    "# Val R2: {r2_score(truth, preds):0.4f}\\n\"\"\"\n",
    "# )\n",
    "\n",
    "kfold_results = kfold_solve_custom_split_col(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    locations=x_train.index,\n",
    "    split_col=x_train.reset_index().index,\n",
    "    lambdas=lambdas,\n",
    "    static_lam_val=best_alpha_1,\n",
    "    static_lam_idxs=list(range(x_train.shape[1]-72, x_train.shape[1])),\n",
    "    intercept=True,\n",
    "    num_folds=5,\n",
    "    random_state=0,\n",
    "    return_preds=True,\n",
    "    return_model=False,\n",
    "    svd_solve=False,\n",
    "    allow_linalg_warning_instances=True,\n",
    "    fit_model_after_tuning=False,\n",
    ")\n",
    "best_alpha_2_idx = interpret_kfold_results(kfold_results, \"r2_score\")[0][0][0]\n",
    "best_alpha_2 = lambdas[best_alpha_2_idx]\n",
    "preds = np.maximum(get_pred_truth_locs(kfold_results)[0].flatten(), 0)\n",
    "truth = get_pred_truth_locs(kfold_results)[1].flatten()\n",
    "\n",
    "\n",
    "model, intercept_term = custom_ridge(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    lam=best_alpha_1, \n",
    "    intercept=True,\n",
    "    static_lam_val=best_alpha_2,\n",
    "    static_lam_idxs=list(range(x_train.shape[1]-72, x_train.shape[1])))\n",
    "pred_test = np.asarray(x_test).dot(model) + intercept_term \n",
    "pred_test = np.maximum(pred_test, 0)\n",
    "\n",
    "# print(\n",
    "# f\"\"\"Best alpha 2: {best_alpha_2}\n",
    "# Fianl Val R2: {r2_score(truth, preds):0.4f}\n",
    "# Final test R2: {r2_score(y_test, pred_test):0.4f}\\n\"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffc9fc-32ea-47c7-93fc-0d0aea13d688",
   "metadata": {},
   "source": [
    "# NDVI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "50874f01-370e-40eb-84c1-4f98234d2632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.7802 \n",
      "Test R2: 0.8085 \n",
      "\n",
      "Demean Val  R2: 0.0921 \n",
      "Demean Test R2: 0.4394\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = ['year', 'district', 'yield_mt']\n",
    "ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains('ndvi')]\n",
    "keep_cols = [*ndvi_cols, *drop_cols]\n",
    "climate_df = climate_df.loc[:, keep_cols]\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "hot_encode = True\n",
    "# hot_encode = False\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop = True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    HOT ENCODE    ######################################### \n",
    "if hot_encode:\n",
    "    drop_cols.remove('district')\n",
    "    climate_df = pd.get_dummies(climate_df, columns = [\"district\"], drop_first = False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "climate_df = climate_df.set_index(drop_cols) \n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis = 1) \n",
    "y_all = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)    \n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################    \n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split['cv_prediction'] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"]-train_split.groupby('district')['log_yield'].transform('mean')\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\"cv_prediction\"]-train_split.groupby('district')['cv_prediction'].transform('mean')\n",
    "\n",
    "test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"]-test_split.groupby('district')['log_yield'].transform('mean')\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"]-test_split.groupby('district')['prediction'].transform('mean')\n",
    "\n",
    "print(f'Val  R2: {r2_score(y_train, val_predictions):0.4f}',\n",
    "      f'\\nTest R2: {r2_score(y_test, test_predictions):0.4f}',\n",
    "     f'\\n\\nDemean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.4f}',\n",
    "     f'\\nDemean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3eef3-18bb-4b2b-8da8-02bc0230bda2",
   "metadata": {},
   "source": [
    "# Precipitation, Temperature, and NDVI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1840a92e-3eca-4039-a5d4-6a8367316314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.8044 \n",
      "Test R2: 0.8297 \n",
      "\n",
      "Demean Val  R2: 0.1723 \n",
      "Demean Test R2: 0.5297\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = ['year', 'district', 'yield_mt']\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "hot_encode = True\n",
    "# hot_encode = False\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop = True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    HOT ENCODE    ######################################### \n",
    "if hot_encode:\n",
    "    drop_cols.remove('district')\n",
    "    climate_df = pd.get_dummies(climate_df, columns = [\"district\"], drop_first = False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "climate_df = climate_df.set_index(drop_cols) \n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis = 1) \n",
    "y_all = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)      \n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################    \n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split['cv_prediction'] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"]-train_split.groupby('district')['log_yield'].transform('mean')\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\"cv_prediction\"]-train_split.groupby('district')['cv_prediction'].transform('mean')\n",
    "\n",
    "test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"]-test_split.groupby('district')['log_yield'].transform('mean')\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"]-test_split.groupby('district')['prediction'].transform('mean')\n",
    "\n",
    "print(f'Val  R2: {r2_score(y_train, val_predictions):0.4f}',\n",
    "      f'\\nTest R2: {r2_score(y_test, test_predictions):0.4f}',\n",
    "     f'\\n\\nDemean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.4f}',\n",
    "     f'\\nDemean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b577bc71-d766-42fe-adad-93286a5a19ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.792567922097444"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_reg.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3dfe35-025c-4b06-8b2b-23b4f223f11b",
   "metadata": {},
   "source": [
    "# NDVI Anomaly Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f20bb178-7708-45e3-aa68-124c88541b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.4449\n",
      "Test R2: 0.4293\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = ['year', 'district', 'yield_mt']\n",
    "ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains('ndvi')]\n",
    "keep_cols = [*ndvi_cols, *drop_cols]\n",
    "climate_df = climate_df.loc[:, keep_cols]\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop = True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "climate_df = climate_df.set_index(drop_cols) \n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################\n",
    "climate_df['yield_mt'] = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "climate_df.set_index(['year', 'district'], inplace=True)\n",
    "var_cols = climate_df.columns\n",
    "climate_df = climate_df[var_cols] - climate_df.groupby(['district'], as_index=True)[var_cols].transform('mean')\n",
    "climate_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis = 1) \n",
    "y_all = climate_df.yield_mt\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)      \n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions   = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################    \n",
    "crop_yield[\"prediction\"] = best_model.predict(x_all)\n",
    "\n",
    "train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split['cv_prediction'] = val_predictions\n",
    "\n",
    "test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "print(f'Val  R2: {r2_score(y_train, val_predictions):0.4f}\\nTest R2: {r2_score(y_test, test_predictions):0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c574bb5-882f-4588-ae97-44af906398a6",
   "metadata": {},
   "source": [
    "# Precipitation, Temperature, and NDVI  Anomaly model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "102e934f-c6e8-4222-9956-a621dda75298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.5284\n",
      "Test R2: 0.5145\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = ['year', 'district', 'yield_mt']\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop = True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "climate_df = climate_df.set_index(drop_cols) \n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################\n",
    "climate_df['yield_mt'] = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "climate_df.set_index(['year', 'district'], inplace=True)\n",
    "var_cols = climate_df.columns\n",
    "climate_df = climate_df[var_cols] - climate_df.groupby(['district'], as_index=True)[var_cols].transform('mean')\n",
    "climate_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis = 1) \n",
    "y_all = climate_df.yield_mt\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)      \n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions   = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################    \n",
    "crop_yield[\"prediction\"] = best_model.predict(x_all)\n",
    "\n",
    "train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split['cv_prediction'] = val_predictions\n",
    "\n",
    "test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "print(f'Val  R2: {r2_score(y_train, val_predictions):0.4f}\\nTest R2: {r2_score(y_test, test_predictions):0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb25ab-e9b1-4bfc-b2e9-a3f4c106a446",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
