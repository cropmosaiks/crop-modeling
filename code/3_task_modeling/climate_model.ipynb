{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aaf7810-5612-46c5-8369-1e95387fe44f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q pyhere p_tqdm glum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0fcec9b-2f29-4ef1-b070-dd823431cbf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d64e464-31b8-4c88-bbf5-877a8fa59fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "from pyhere import here\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyarrow\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import p_tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    GridSearchCV,\n",
    "    cross_val_predict,\n",
    ")\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "from task_modeling_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae1e989c-2996-4ee4-abab-0076e5146c42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'alpha': 0.1}\n",
      "Best AUC score:  0.9921428571428572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Define the stratified shuffle split with 100 splits\n",
    "sss = StratifiedShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the ridge regression model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Define the GridSearchCV object with AUC as the scoring metric\n",
    "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='roc_auc', cv=sss)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding AUC score\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best AUC score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25566c3c-c0c5-4302-a7b3-adb7707b5068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cda9b93-ec89-443d-b3b9-b33b440a2a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(next(sss.split(X, y))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f12ea47-2536-4c35-8d1f-0dc31435d869",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split93_test_score</th>\n",
       "      <th>split94_test_score</th>\n",
       "      <th>split95_test_score</th>\n",
       "      <th>split96_test_score</th>\n",
       "      <th>split97_test_score</th>\n",
       "      <th>split98_test_score</th>\n",
       "      <th>split99_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.004305</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'alpha': 0.01}</td>\n",
       "      <td>0.995370</td>\n",
       "      <td>0.988426</td>\n",
       "      <td>0.999008</td>\n",
       "      <td>0.999008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996032</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.990410</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.991402</td>\n",
       "      <td>0.991733</td>\n",
       "      <td>0.999339</td>\n",
       "      <td>0.992083</td>\n",
       "      <td>0.007115</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.003337</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.003371</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>0.997685</td>\n",
       "      <td>0.990410</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.996362</td>\n",
       "      <td>0.976521</td>\n",
       "      <td>0.991402</td>\n",
       "      <td>0.986442</td>\n",
       "      <td>0.991071</td>\n",
       "      <td>0.992063</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.992143</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>1</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>0.997024</td>\n",
       "      <td>0.986442</td>\n",
       "      <td>0.999008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.995040</td>\n",
       "      <td>0.975860</td>\n",
       "      <td>0.991733</td>\n",
       "      <td>0.983135</td>\n",
       "      <td>0.990079</td>\n",
       "      <td>0.991733</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.990241</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>10</td>\n",
       "      <td>{'alpha': 10}</td>\n",
       "      <td>0.995701</td>\n",
       "      <td>0.988757</td>\n",
       "      <td>0.998016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.984788</td>\n",
       "      <td>0.990079</td>\n",
       "      <td>0.991733</td>\n",
       "      <td>0.999669</td>\n",
       "      <td>0.989997</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>100</td>\n",
       "      <td>{'alpha': 100}</td>\n",
       "      <td>0.993717</td>\n",
       "      <td>0.987765</td>\n",
       "      <td>0.997024</td>\n",
       "      <td>0.999008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.983135</td>\n",
       "      <td>0.985780</td>\n",
       "      <td>0.990741</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.990079</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.998677</td>\n",
       "      <td>0.988072</td>\n",
       "      <td>0.007690</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "0       0.001159      0.003678         0.001722        0.004305        0.01   \n",
       "1       0.000906      0.003337         0.000977        0.003371         0.1   \n",
       "2       0.000426      0.001591         0.001482        0.003640           1   \n",
       "3       0.000500      0.000500         0.001549        0.002760          10   \n",
       "4       0.000979      0.001316         0.001415        0.000602         100   \n",
       "\n",
       "            params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'alpha': 0.01}           0.995370           0.988426           0.999008   \n",
       "1   {'alpha': 0.1}           0.997685           0.990410           0.999669   \n",
       "2     {'alpha': 1}           0.997024           0.986442           0.999008   \n",
       "3    {'alpha': 10}           0.995701           0.988757           0.998016   \n",
       "4   {'alpha': 100}           0.993717           0.987765           0.997024   \n",
       "\n",
       "   split3_test_score  ...  split93_test_score  split94_test_score  \\\n",
       "0           0.999008  ...            0.996032            0.979167   \n",
       "1           0.999669  ...            0.996362            0.976521   \n",
       "2           1.000000  ...            0.995040            0.975860   \n",
       "3           1.000000  ...            0.989418            0.982143   \n",
       "4           0.999008  ...            0.983135            0.985780   \n",
       "\n",
       "   split95_test_score  split96_test_score  split97_test_score  \\\n",
       "0            0.990410            0.986111            0.991402   \n",
       "1            0.991402            0.986442            0.991071   \n",
       "2            0.991733            0.983135            0.990079   \n",
       "3            0.989418            0.984788            0.990079   \n",
       "4            0.990741            0.982143            0.990079   \n",
       "\n",
       "   split98_test_score  split99_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.991733            0.999339         0.992083        0.007115   \n",
       "1            0.992063            0.999669         0.992143        0.006830   \n",
       "2            0.991733            0.999669         0.990241        0.007900   \n",
       "3            0.991733            0.999669         0.989997        0.007299   \n",
       "4            0.988095            0.998677         0.988072        0.007690   \n",
       "\n",
       "   rank_test_score  \n",
       "0                2  \n",
       "1                1  \n",
       "2                3  \n",
       "3                4  \n",
       "4                5  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ee3e3-d0f8-4f06-9bb7-74e1f8a3f71c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "935e6a83-35f9-4d22-8b35-f5c61b42ea86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC score: 0.991415343915344\n",
      "Standard deviation of AUC scores: 0.007350636333770812\n",
      "95% confidence interval: [0.9900, 0.9929]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Define the stratified shuffle split with 100 splits\n",
    "sss = StratifiedShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the ridge regression model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Define lists to store the AUC scores for each split\n",
    "auc_scores = []\n",
    "\n",
    "# Perform the hyperparameter tuning with cross-validation\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Define the GridSearchCV object with AUC as the scoring metric\n",
    "    grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='roc_auc', cv=5)\n",
    "\n",
    "    # Fit the GridSearchCV object to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Compute the AUC score on the test set\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    # Store the AUC score\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "# Compute the mean and standard deviation of the AUC scores\n",
    "mean_auc = np.mean(auc_scores)\n",
    "std_auc = np.std(auc_scores)\n",
    "\n",
    "# Compute the 95% confidence interval using the normal approximation method\n",
    "lower_bound = mean_auc - 1.96 * std_auc / np.sqrt(len(auc_scores))\n",
    "upper_bound = mean_auc + 1.96 * std_auc / np.sqrt(len(auc_scores))\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean AUC score:\", mean_auc)\n",
    "print(\"Standard deviation of AUC scores:\", std_auc)\n",
    "print(\"95% confidence interval: [{:.4f}, {:.4f}]\".format(lower_bound, upper_bound))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001cbad-1cd8-4c6f-9f83-a817ddaf1906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a171d6a2-f713-41df-919a-275b3fa9746d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.1\n",
      "Mean AUC scores: [0.99340278 0.99382606 0.9921627  0.99170966]\n",
      "Standard deviations of AUC scores: [0.0060642  0.00591905 0.00711855 0.00673244]\n",
      "95% Confidence intervals: ([0.99221419 0.99266593 0.99076746 0.9903901 ], [0.99459136 0.99498619 0.99355793 0.99302921])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters to be tuned\n",
    "alphas = [0.01, 0.1, 1, 10]\n",
    "\n",
    "# Define number of random samples\n",
    "n_samples = 100\n",
    "\n",
    "# Define confidence level\n",
    "conf_level = 0.95\n",
    "\n",
    "# Create arrays to store AUC scores and coefficients for each split and alpha value\n",
    "auc_scores = np.zeros((n_samples, len(alphas)))\n",
    "coefs = np.zeros((n_samples, len(alphas), X.shape[1]))\n",
    "\n",
    "# Perform 100 stratified random splits, each with 80% for training and 20% for testing\n",
    "for i in range(n_samples):\n",
    "    # Generate a stratified random sample of the data\n",
    "    X_sample, y_sample = resample(X, y, stratify=y, replace=False)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, stratify=y_sample)\n",
    "    \n",
    "    # Fit a Ridge model to the training data using different values of alpha\n",
    "    for j, alpha in enumerate(alphas):\n",
    "        ridge = Ridge(alpha=alpha)\n",
    "        ridge.fit(X_train, y_train)\n",
    "        coefs[i, j] = ridge.coef_\n",
    "        \n",
    "        # Calculate AUC score for the testing set\n",
    "        y_pred = ridge.predict(X_test)\n",
    "        auc_scores[i, j] = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "# Calculate mean and standard deviation of AUC scores for each alpha value\n",
    "mean_auc = np.mean(auc_scores, axis=0)\n",
    "std_auc = np.std(auc_scores, axis=0)\n",
    "\n",
    "# Calculate confidence intervals for each alpha value\n",
    "z = 1.96 # 95% confidence level\n",
    "ci_lower = mean_auc - z * std_auc / np.sqrt(n_samples)\n",
    "ci_upper = mean_auc + z * std_auc / np.sqrt(n_samples)\n",
    "\n",
    "# Find the best performing alpha value based on the mean AUC score\n",
    "best_alpha = alphas[np.argmax(mean_auc)]\n",
    "\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Mean AUC scores: {mean_auc}\")\n",
    "print(f\"Standard deviations of AUC scores: {std_auc}\")\n",
    "print(f\"95% Confidence intervals: ({ci_lower}, {ci_upper})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "015ee13d-cd8a-4ad0-ac86-7389ca0a2192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99340278, 0.99382606, 0.9921627 , 0.99170966])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8893616f-ca0d-4369-9202-cd5ed2cfdf70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a856dad1-61a2-429e-925e-261552949e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7eee6a1d-dcfe-450e-8611-3cfefc0bddd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.498\n",
      "Standard error: 0.004\n",
      "95% Confidence interval: (0.4887089394236692, 0.5064036843558082)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from scipy.stats import sem, t\n",
    "\n",
    "# Define your features and targets here\n",
    "np.random.seed(42)\n",
    "num_samples = 1000\n",
    "num_features = 10\n",
    "features = np.random.randn(num_samples, num_features)\n",
    "targets = np.random.randint(0, 2, size=num_samples)\n",
    "\n",
    "# Define the hyperparameters to be tested\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Define the number of stratified random splits to perform\n",
    "num_splits = 100\n",
    "\n",
    "# Define the array to be used for stratification (not in features or targets)\n",
    "strat_array = np.random.randint(0, 5, size=len(targets))\n",
    "\n",
    "# Initialize lists to store scores and best hyperparameters for each split\n",
    "split_scores = []\n",
    "split_best_params = []\n",
    "\n",
    "# Loop over the specified number of splits\n",
    "for i in range(num_splits):\n",
    "    # Split the data into training and testing sets using stratified random sampling\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=i)\n",
    "    train_index, test_index = next(split.split(features, strat_array))\n",
    "    X_train, X_test = features[train_index], features[test_index]\n",
    "    y_train, y_test = targets[train_index], targets[test_index]\n",
    "\n",
    "    # Fit the ridge regression model with hyperparameter tuning\n",
    "    ridge = Ridge()\n",
    "    grid_search = GridSearchCV(ridge, param_grid, scoring='roc_auc', cv=split, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Record the best hyperparameters and score for this split\n",
    "    split_scores.append(grid_search.best_score_)\n",
    "    split_best_params.append(grid_search.best_params_)\n",
    "\n",
    "# Calculate the mean and standard error of the scores\n",
    "mean_score = np.mean(split_scores)\n",
    "std_err_score = sem(split_scores)\n",
    "\n",
    "# Calculate the 95% confidence interval for the mean score\n",
    "conf_interval = t.interval(0.95, num_splits - 1, loc=mean_score, scale=std_err_score)\n",
    "\n",
    "print(f\"Mean score: {mean_score:.3f}\")\n",
    "print(f\"Standard error: {std_err_score:.3f}\")\n",
    "print(f\"95% Confidence interval: {conf_interval}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a37451b5-23fb-4d21-ad69-eb61e7789ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5807157368338802,\n",
       " 0.5167994999218628,\n",
       " 0.5632812500000001,\n",
       " 0.5210189092045633,\n",
       " 0.474202626641651,\n",
       " 0.5446163463041099,\n",
       " 0.43483023001095295,\n",
       " 0.5656660412757974,\n",
       " 0.4613220815752461,\n",
       " 0.4427254258477887,\n",
       " 0.44928895139865604,\n",
       " 0.49070167213627136,\n",
       " 0.48983739837398377,\n",
       " 0.4738240350054696,\n",
       " 0.5202375371151742,\n",
       " 0.5404687499999999,\n",
       " 0.49624765478424016,\n",
       " 0.46109375,\n",
       " 0.47585560243788083,\n",
       " 0.5509693558474047,\n",
       " 0.5153508771929824,\n",
       " 0.4907754846779237,\n",
       " 0.4911704953899047,\n",
       " 0.5364900765744647,\n",
       " 0.491875,\n",
       " 0.5921237693389593,\n",
       " 0.45859375,\n",
       " 0.5103922487888732,\n",
       " 0.3809375,\n",
       " 0.46038443506797944,\n",
       " 0.527673545966229,\n",
       " 0.4238162212845757,\n",
       " 0.4243277048155097,\n",
       " 0.46210345366463507,\n",
       " 0.43980612883051906,\n",
       " 0.49773402094077196,\n",
       " 0.5,\n",
       " 0.5563369276449445,\n",
       " 0.54875,\n",
       " 0.39453124999999994,\n",
       " 0.5015625,\n",
       " 0.49445225816533833,\n",
       " 0.4578125,\n",
       " 0.4677293327082357,\n",
       " 0.44928895139865593,\n",
       " 0.5519612439443663,\n",
       " 0.521875,\n",
       " 0.5866166353971232,\n",
       " 0.49054539771839345,\n",
       " 0.55953125,\n",
       " 0.4921875,\n",
       " 0.44155844155844154,\n",
       " 0.49257696515080485,\n",
       " 0.4824191279887482,\n",
       " 0.5157055789967182,\n",
       " 0.4881175734834272,\n",
       " 0.4855446163463041,\n",
       " 0.48427476138319514,\n",
       " 0.49015624999999996,\n",
       " 0.51015625,\n",
       " 0.551594746716698,\n",
       " 0.5539928113767776,\n",
       " 0.51921875,\n",
       " 0.5275,\n",
       " 0.458880550343965,\n",
       " 0.46841776110068795,\n",
       " 0.5265791119449656,\n",
       " 0.4963275511798718,\n",
       " 0.54921875,\n",
       " 0.47812499999999997,\n",
       " 0.4689406978563606,\n",
       " 0.5424285044538208,\n",
       " 0.5495622263914948,\n",
       " 0.5146116580715737,\n",
       " 0.5144553836536958,\n",
       " 0.5161744022503516,\n",
       " 0.45984374999999994,\n",
       " 0.483125,\n",
       " 0.5361163227016885,\n",
       " 0.49061913696060033,\n",
       " 0.5261759649945303,\n",
       " 0.4882739212007505,\n",
       " 0.4789810907954368,\n",
       " 0.4222534771057977,\n",
       " 0.48717948717948717,\n",
       " 0.47234375000000006,\n",
       " 0.4525,\n",
       " 0.5119699577530903,\n",
       " 0.44246404002501566,\n",
       " 0.6476011876855758,\n",
       " 0.48273167682450385,\n",
       " 0.4844506954211596,\n",
       " 0.5311767463666198,\n",
       " 0.4596030629785904,\n",
       " 0.584375,\n",
       " 0.49101422097202685,\n",
       " 0.47076297686053775,\n",
       " 0.4860588972431078,\n",
       " 0.45606629143214517,\n",
       " 0.45296875000000003]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad71a5-6d1e-4924-889f-c0fa3b9c2889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1e8710e-23cd-4e15-ba2e-b274185fd27f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#########################################     K-FOLD SPLIT    #########################################\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m x_all \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(drop_cols, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \n\u001b[0;32m      3\u001b[0m y_all \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39myield_mt\n\u001b[0;32m      4\u001b[0m x_train, x_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(x_all, y_all, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = features.drop(drop_cols, axis = 1) \n",
    "y_all = features.yield_mt\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=2, train_size=.8, random_state=seed)\n",
    "\n",
    "\n",
    "#########################################     K-FOLD CV    ###########################################\n",
    "### SETUP\n",
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "kfold  = KFold()\n",
    "ridge  = Ridge()   \n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "214c501a-3ca6-4f6b-b833-42407ed635c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  4  5  8  9 11 15 16 17] [ 0  2  3  6  7 10 12 13 14]\n",
      "[ 0  2  3  6  7 10 12 13 14] [ 1  4  5  8  9 11 15 16 17]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "X = list(range(18))\n",
    "y = [1] * 6 + [0] * 12\n",
    "groups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6]\n",
    "sgkf = StratifiedGroupKFold(n_splits=2)\n",
    "for train, test in sgkf.split(X, y, groups=groups):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ad1094-300b-463b-b14f-222fc96f230a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8,)\n",
      "GroupShuffleSplit(n_splits=1, random_state=42, test_size=None, train_size=0.8)\n",
      "Fold 0:\n",
      "  Train: index=[2 3 4 5 6 7], group=[2 2 2 3 3 3]\n",
      "  Test:  index=[0 1], group=[1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "X = np.ones(shape=(8, 2))\n",
    "y = np.ones(shape=(8, 1))\n",
    "groups = np.array([1, 1, 2, 2, 2, 3, 3, 3])\n",
    "print(groups.shape)\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=42)\n",
    "gss.get_n_splits()\n",
    "\n",
    "print(gss)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(gss.split(X, y, groups)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}, group={groups[train_index]}\")\n",
    "    print(f\"  Test:  index={test_index}, group={groups[test_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdbfdc2e-1216-43b2-9425-ff5da1eb858d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, (array([2, 3, 4, 5, 6, 7], dtype=int64), array([0, 1], dtype=int64)))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(gss.split(X, y, groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae240738-37cd-476c-b7b2-e0bc386d55c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([2, 3, 4, 5, 6, 7], dtype=int64), array([0, 1], dtype=int64))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gss.split(X, y, groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0790b5a-fb5b-475a-a8b8-286a94c4b6da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 3, 4, 5, 6, 7], dtype=int64), array([0, 1], dtype=int64))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_index, x_test_index = next(gss.split(X, y, groups))\n",
    "x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b3750c-d520-4102-8fe8-dd4b4a4358f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def climate_model(\n",
    "    variable_groups=[\"pre\", \"tmp\", \"ndvi\"],\n",
    "    # he_anom=[True, False],\n",
    "    hot_encode=True,\n",
    "    anomaly=False,\n",
    "    index_cols=[\"year\", \"district\", \"yield_mt\"],\n",
    "    year_start=2016,\n",
    "    n_splits=5,\n",
    "):\n",
    "    #########################################     READ DATA    #########################################\n",
    "    data = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "    data = data.dropna()\n",
    "\n",
    "    # hot_encode = he_anom[0]\n",
    "    # anom = he_anom[1]\n",
    "\n",
    "    keep_cols = []\n",
    "\n",
    "    for var in variable_groups:\n",
    "        tmp = data.columns[data.columns.to_series().str.contains(var)].tolist()\n",
    "        keep_cols.append(tmp)\n",
    "\n",
    "    keep_cols = [*index_cols, *[col for cols in keep_cols for col in cols]]\n",
    "\n",
    "    data = data.loc[:, keep_cols]\n",
    "\n",
    "    data = data[data.year >= year_start]\n",
    "\n",
    "    crop_yield = data.copy().loc[:, tuple(index_cols)].reset_index(drop=True)\n",
    "    crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "    ########################################    STANDARDIZE FEATURES    #########################################\n",
    "    data = data.set_index(index_cols)\n",
    "    data_scaled = StandardScaler().fit_transform(data.values)\n",
    "    data = pd.DataFrame(data_scaled, index=data.index).reset_index()\n",
    "    data.columns = data.columns.astype(str)\n",
    "\n",
    "    #########################################     CALCULATE ANOMALY   #########################################\n",
    "    if anomaly:\n",
    "        data[\"yield_mt\"] = np.log10(data.yield_mt.to_numpy() + 1)\n",
    "        data.set_index([\"year\", \"district\"], inplace=True)\n",
    "        var_cols = data.columns\n",
    "        data = data[var_cols] - data.groupby([\"district\"], as_index=True)[\n",
    "            var_cols\n",
    "        ].transform(\"mean\")\n",
    "        data.reset_index(drop=False, inplace=True)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    #########################################    HOT ENCODE    #########################################\n",
    "    if hot_encode:\n",
    "        index_cols.remove(\"district\")\n",
    "        data = pd.get_dummies(data, columns=[\"district\"], drop_first=False)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    #########################################     K-FOLD SPLIT    #########################################\n",
    "    x_all = data.drop(index_cols, axis=1)\n",
    "    y_all = np.log10(data.yield_mt.to_numpy() + 1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_all, y_all, test_size=0.2, random_state=0\n",
    "    )\n",
    "\n",
    "    #########################################     K-FOLD CV    #########################################\n",
    "    ### SETUP\n",
    "    tic = time.time()\n",
    "    kfold = KFold(n_splits=n_splits)\n",
    "    alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "\n",
    "    i = 0\n",
    "    start = [i]\n",
    "    end = [x_train.shape[1]]\n",
    "\n",
    "    for var in variable_groups:\n",
    "        i += 12\n",
    "        start.append(i)\n",
    "        end.append(i)\n",
    "    start.sort()\n",
    "    end.sort()\n",
    "\n",
    "    if not hot_encode:\n",
    "        start = start[0:-1]\n",
    "        end = end[0:-1]\n",
    "\n",
    "    ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER(S)\n",
    "    best_lambdas, best_scores, best_model = kfold_rr_multi_lambda_tuning(\n",
    "        X=x_train,\n",
    "        y=y_train,\n",
    "        grid=alphas.get(\"alpha\"),\n",
    "        n_splits=n_splits,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        static_lam=1,\n",
    "        verbose=0,\n",
    "        show_linalg_warning=False,\n",
    "        fit_model_after_tuning=True,\n",
    "    )\n",
    "    ### PREDICT WITH BEST HYPERPARAMETER(S)\n",
    "    val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "    train_predictions = best_model.predict(x_train)\n",
    "    test_predictions = best_model.predict(x_test)\n",
    "\n",
    "    #########################################     DE-MEAN R2    #########################################\n",
    "    crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "    train_split = pd.DataFrame(\n",
    "        np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    "    )\n",
    "    train_split = train_split.join(\n",
    "        crop_yield.copy()[crop_yield.index.isin(x_train.index)]\n",
    "    )\n",
    "    train_split[\"cv_prediction\"] = np.maximum(val_predictions, 0)\n",
    "    train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"] - train_split.groupby(\n",
    "        \"district\"\n",
    "    )[\"log_yield\"].transform(\"mean\")\n",
    "    train_split[\"demean_cv_prediction\"] = train_split[\n",
    "        \"cv_prediction\"\n",
    "    ] - train_split.groupby(\"district\")[\"cv_prediction\"].transform(\"mean\")\n",
    "\n",
    "    test_split = pd.DataFrame(\n",
    "        np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    "    )\n",
    "    test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "    test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "    test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "    test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "    predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "    test_split[\"demean_test_yield\"] = test_split[\"log_yield\"] - test_split.groupby(\n",
    "        \"district\"\n",
    "    )[\"log_yield\"].transform(\"mean\")\n",
    "    test_split[\"demean_test_prediction\"] = test_split[\n",
    "        \"prediction\"\n",
    "    ] - test_split.groupby(\"district\")[\"prediction\"].transform(\"mean\")\n",
    "\n",
    "    print(\n",
    "        f\"\"\"\n",
    "\\t\\tFinish:\n",
    "\\t\\t\\tVariables: {variable_groups}\n",
    "\\t\\t\\tLambdas:   {best_lambdas}\n",
    "\\t\\t\\tOne-hot encoding: {hot_encode}\n",
    "\\t\\t\\tAnomaly: {anomaly}\n",
    "\n",
    "\\t\\t\\tFinal Val  R2: {r2_score(y_train, val_predictions):0.4f} \n",
    "\\t\\t\\tFinal Test R2: {r2_score(y_test, test_predictions):0.4f}\n",
    "\n",
    "\\t\\t\\tDemean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.4f}\n",
    "\\t\\t\\tDemean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.4f}\n",
    "\n",
    "\\t\\t\\tTotal time: {(time.time()-tic)/60:0.2f} minutes\n",
    "    \"\"\"\n",
    "    )\n",
    "    d = {\n",
    "        \"variables\": variable_groups,\n",
    "        \"year_start\": year_start,\n",
    "        \"hot_encode\": hot_encode,\n",
    "        \"anomaly\": anomaly,\n",
    "        \"total_n\": len(x_all),\n",
    "        \"train_n\": len(x_train),\n",
    "        \"test_n\": len(x_test),\n",
    "        \"best_reg_param\": [best_lambdas],\n",
    "        \"mean_of_val_R2\": [best_scores],\n",
    "        \"val_R2\": r2_score(y_train, val_predictions),\n",
    "        \"val_r\": pearsonr(val_predictions, y_train)[0],\n",
    "        \"val_r2\": pearsonr(val_predictions, y_train)[0] ** 2,\n",
    "        \"train_R2\": r2_score(y_train, train_predictions),\n",
    "        \"train_r\": pearsonr(train_predictions, y_train)[0],\n",
    "        \"train_r2\": pearsonr(train_predictions, y_train)[0] ** 2,\n",
    "        \"test_R2\": r2_score(y_test, test_predictions),\n",
    "        \"test_r\": pearsonr(test_predictions, y_test)[0],\n",
    "        \"test_r2\": pearsonr(test_predictions, y_test)[0] ** 2,\n",
    "        \"demean_cv_R2\": r2_score(\n",
    "            train_split.demean_cv_yield, train_split.demean_cv_prediction\n",
    "        ),\n",
    "        \"demean_cv_r\": pearsonr(\n",
    "            train_split.demean_cv_yield, train_split.demean_cv_prediction\n",
    "        )[0],\n",
    "        \"demean_cv_r2\": pearsonr(\n",
    "            train_split.demean_cv_yield, train_split.demean_cv_prediction\n",
    "        )[0]\n",
    "        ** 2,\n",
    "    }\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2216f1bb-8f61-417f-bbfc-6bba313014c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pre'],\n",
       " ['tmp'],\n",
       " ['ndvi'],\n",
       " ['pre', 'tmp'],\n",
       " ['pre', 'ndvi'],\n",
       " ['tmp', 'ndvi'],\n",
       " ['pre', 'tmp', 'ndvi']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables = [\"pre\", \"tmp\", \"ndvi\"]\n",
    "HE = [True, False]\n",
    "# anom = [True, False]\n",
    "\n",
    "clim = list(itertools.combinations(variables, 2))\n",
    "clim.append([variables[0]])\n",
    "clim.append([variables[1]])\n",
    "clim.append([variables[2]])\n",
    "clim.append(variables)\n",
    "clim = [list(elem) for elem in clim]\n",
    "clim.sort(key=len)\n",
    "clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0045e815-29dc-48d2-98fe-0c3fdc2be1df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = pd.read_csv(here(\"data\", \"results\", \"climate_model_2023-03-22.csv\"))\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dc8f5bc-769e-4973-9784-3411b9fae522",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: ['pre']\n",
      "\tYear start: 2009\n",
      "\tHE: True\n",
      "\tAnomaly: True\n",
      "\t\tPass, one-hot encoding does not make sense with an anomaly model\n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre']\n",
      "\t\t\tLambdas:   [0.01, 0.0001]\n",
      "\t\t\tOne-hot encoding: True\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.5849 \n",
      "\t\t\tFinal Test R2: 0.6076\n",
      "\n",
      "\t\t\tDemean Val  R2: -0.0879\n",
      "\t\t\tDemean Test R2: 0.1732\n",
      "\n",
      "\t\t\tTotal time: 0.14 minutes\n",
      "    \n",
      "\tHE: False\n",
      "\tAnomaly: True\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre']\n",
      "\t\t\tLambdas:   [0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: True\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.0922 \n",
      "\t\t\tFinal Test R2: 0.1317\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.0536\n",
      "\t\t\tDemean Test R2: 0.0623\n",
      "\n",
      "\t\t\tTotal time: 0.01 minutes\n",
      "    \n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre']\n",
      "\t\t\tLambdas:   [0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.3238 \n",
      "\t\t\tFinal Test R2: 0.3568\n",
      "\n",
      "\t\t\tDemean Val  R2: -0.0444\n",
      "\t\t\tDemean Test R2: 0.1544\n",
      "\n",
      "\t\t\tTotal time: 0.01 minutes\n",
      "    \n",
      "\tYear start: 2016\n",
      "\tHE: True\n",
      "\tAnomaly: True\n",
      "\t\tPass, one-hot encoding does not make sense with an anomaly model\n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre']\n",
      "\t\t\tLambdas:   [0.01, 0.001]\n",
      "\t\t\tOne-hot encoding: True\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.6225 \n",
      "\t\t\tFinal Test R2: 0.6943\n",
      "\n",
      "\t\t\tDemean Val  R2: -0.6271\n",
      "\t\t\tDemean Test R2: 0.1442\n",
      "\n",
      "\t\t\tTotal time: 0.14 minutes\n",
      "    \n",
      "\tHE: False\n",
      "\tAnomaly: True\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre']\n",
      "\t\t\tLambdas:   [0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: True\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.1283 \n",
      "\t\t\tFinal Test R2: 0.1438\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.0352\n",
      "\t\t\tDemean Test R2: 0.0701\n",
      "\n",
      "\t\t\tTotal time: 0.01 minutes\n",
      "    \n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre']\n",
      "\t\t\tLambdas:   [0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.3434 \n",
      "\t\t\tFinal Test R2: 0.3772\n",
      "\n",
      "\t\t\tDemean Val  R2: -0.3295\n",
      "\t\t\tDemean Test R2: -0.2086\n",
      "\n",
      "\t\t\tTotal time: 0.01 minutes\n",
      "    \n",
      "Variable: ['tmp']\n",
      "\tYear start: 2009\n",
      "\tHE: True\n",
      "\tAnomaly: True\n",
      "\t\tPass, one-hot encoding does not make sense with an anomaly model\n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['tmp']\n",
      "\t\t\tLambdas:   [0.01, 1e-05]\n",
      "\t\t\tOne-hot encoding: True\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.7051 \n",
      "\t\t\tFinal Test R2: 0.7201\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.2325\n",
      "\t\t\tDemean Test R2: 0.4358\n",
      "\n",
      "\t\t\tTotal time: 0.15 minutes\n",
      "    \n",
      "\tHE: False\n",
      "\tAnomaly: True\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['tmp']\n",
      "\t\t\tLambdas:   [0.001]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: True\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.3632 \n",
      "\t\t\tFinal Test R2: 0.3794\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.1545\n",
      "\t\t\tDemean Test R2: 0.1822\n",
      "\n",
      "\t\t\tTotal time: 0.01 minutes\n",
      "    \n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['tmp']\n",
      "\t\t\tLambdas:   [0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.5551 \n",
      "\t\t\tFinal Test R2: 0.5370\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.2696\n",
      "\t\t\tDemean Test R2: 0.3465\n",
      "\n",
      "\t\t\tTotal time: 0.01 minutes\n",
      "    \n",
      "\tYear start: 2016\n",
      "\tHE: True\n",
      "\tAnomaly: True\n",
      "\t\tPass, one-hot encoding does not make sense with an anomaly model\n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['tmp']\n",
      "\t\t\tLambdas:   [0.01, 0.001]\n",
      "\t\t\tOne-hot encoding: True\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.7724 \n",
      "\t\t\tFinal Test R2: 0.8177\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.0210\n",
      "\t\t\tDemean Test R2: 0.4869\n",
      "\n",
      "\t\t\tTotal time: 0.15 minutes\n",
      "    \n",
      "\tHE: False\n",
      "\tAnomaly: True\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['tmp']\n",
      "\t\t\tLambdas:   [0.001]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: True\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.4413 \n",
      "\t\t\tFinal Test R2: 0.4822\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.0947\n",
      "\t\t\tDemean Test R2: 0.1187\n",
      "\n",
      "\t\t\tTotal time: 0.01 minutes\n",
      "    \n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['tmp']\n",
      "\t\t\tLambdas:   [0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.6322 \n",
      "\t\t\tFinal Test R2: 0.6490\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.2895\n",
      "\t\t\tDemean Test R2: 0.3116\n",
      "\n",
      "\t\t\tTotal time: 0.01 minutes\n",
      "    \n",
      "Variable: ['ndvi']\n",
      "\tYear start: 2009\n",
      "\tHE: True\n",
      "\tAnomaly: True\n",
      "\t\tPass, one-hot encoding does not make sense with an anomaly model\n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['ndvi']\n",
      "\t\t\tLambdas:   [0.01, 1e-08]\n",
      "\t\t\tOne-hot encoding: True\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.6363 \n",
      "\t\t\tFinal Test R2: 0.5956\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.0450\n",
      "\t\t\tDemean Test R2: 0.1383\n",
      "\n",
      "\t\t\tTotal time: 0.14 minutes\n",
      "    \n",
      "\tHE: False\n",
      "\tAnomaly: True\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['ndvi']\n",
      "\t\t\tLambdas:   [0.001]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: True\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.2757 \n",
      "\t\t\tFinal Test R2: 0.1406\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.0500\n",
      "\t\t\tDemean Test R2: 0.0361\n",
      "\n",
      "\t\t\tTotal time: 0.01 minutes\n",
      "    \n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['ndvi']\n",
      "\t\t\tLambdas:   [0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.2196 \n",
      "\t\t\tFinal Test R2: 0.1874\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.1290\n",
      "\t\t\tDemean Test R2: 0.0411\n",
      "\n",
      "\t\t\tTotal time: 0.01 minutes\n",
      "    \n",
      "\tYear start: 2016\n",
      "\tHE: True\n",
      "\tAnomaly: True\n",
      "\t\tPass, one-hot encoding does not make sense with an anomaly model\n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.001]\n",
      "\t\t\tOne-hot encoding: True\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.7764 \n",
      "\t\t\tFinal Test R2: 0.8046\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.0373\n",
      "\t\t\tDemean Test R2: 0.3798\n",
      "\n",
      "\t\t\tTotal time: 0.15 minutes\n",
      "    \n",
      "\tHE: False\n",
      "\tAnomaly: True\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['ndvi']\n",
      "\t\t\tLambdas:   [0.001]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: True\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.4929 \n",
      "\t\t\tFinal Test R2: 0.4530\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.0754\n",
      "\t\t\tDemean Test R2: 0.0969\n",
      "\n",
      "\t\t\tTotal time: 0.01 minutes\n",
      "    \n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['ndvi']\n",
      "\t\t\tLambdas:   [0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.2711 \n",
      "\t\t\tFinal Test R2: 0.3241\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.2961\n",
      "\t\t\tDemean Test R2: 0.2917\n",
      "\n",
      "\t\t\tTotal time: 0.01 minutes\n",
      "    \n",
      "Variable: ['pre', 'tmp']\n",
      "\tYear start: 2009\n",
      "\tHE: True\n",
      "\tAnomaly: True\n",
      "\t\tPass, one-hot encoding does not make sense with an anomaly model\n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'tmp']\n",
      "\t\t\tLambdas:   [0.1, 0.01, 0.0001]\n",
      "\t\t\tOne-hot encoding: True\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.7181 \n",
      "\t\t\tFinal Test R2: 0.7343\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.2663\n",
      "\t\t\tDemean Test R2: 0.4583\n",
      "\n",
      "\t\t\tTotal time: 0.23 minutes\n",
      "    \n",
      "\tHE: False\n",
      "\tAnomaly: True\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'tmp']\n",
      "\t\t\tLambdas:   [0.01, 0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: True\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.3926 \n",
      "\t\t\tFinal Test R2: 0.3974\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.1654\n",
      "\t\t\tDemean Test R2: 0.1886\n",
      "\n",
      "\t\t\tTotal time: 0.03 minutes\n",
      "    \n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'tmp']\n",
      "\t\t\tLambdas:   [0.1, 0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.5858 \n",
      "\t\t\tFinal Test R2: 0.5700\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.2948\n",
      "\t\t\tDemean Test R2: 0.3646\n",
      "\n",
      "\t\t\tTotal time: 0.04 minutes\n",
      "    \n",
      "\tYear start: 2016\n",
      "\tHE: True\n",
      "\tAnomaly: True\n",
      "\t\tPass, one-hot encoding does not make sense with an anomaly model\n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'tmp']\n",
      "\t\t\tLambdas:   [0.1, 0.01, 0.01]\n",
      "\t\t\tOne-hot encoding: True\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.7746 \n",
      "\t\t\tFinal Test R2: 0.8011\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.1650\n",
      "\t\t\tDemean Test R2: 0.4905\n",
      "\n",
      "\t\t\tTotal time: 0.23 minutes\n",
      "    \n",
      "\tHE: False\n",
      "\tAnomaly: True\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'tmp']\n",
      "\t\t\tLambdas:   [0.01, 0.0001]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: True\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.4597 \n",
      "\t\t\tFinal Test R2: 0.5166\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.0933\n",
      "\t\t\tDemean Test R2: 0.1360\n",
      "\n",
      "\t\t\tTotal time: 0.03 minutes\n",
      "    \n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'tmp']\n",
      "\t\t\tLambdas:   [0.1, 0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.6869 \n",
      "\t\t\tFinal Test R2: 0.7224\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.3087\n",
      "\t\t\tDemean Test R2: 0.4784\n",
      "\n",
      "\t\t\tTotal time: 0.02 minutes\n",
      "    \n",
      "Variable: ['pre', 'ndvi']\n",
      "\tYear start: 2009\n",
      "\tHE: True\n",
      "\tAnomaly: True\n",
      "\t\tPass, one-hot encoding does not make sense with an anomaly model\n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.01, 1e-08]\n",
      "\t\t\tOne-hot encoding: True\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.6897 \n",
      "\t\t\tFinal Test R2: 0.6631\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.1857\n",
      "\t\t\tDemean Test R2: 0.3020\n",
      "\n",
      "\t\t\tTotal time: 0.23 minutes\n",
      "    \n",
      "\tHE: False\n",
      "\tAnomaly: True\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'ndvi']\n",
      "\t\t\tLambdas:   [0.001, 0.001]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: True\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.3606 \n",
      "\t\t\tFinal Test R2: 0.2588\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.1160\n",
      "\t\t\tDemean Test R2: 0.1185\n",
      "\n",
      "\t\t\tTotal time: 0.03 minutes\n",
      "    \n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.4486 \n",
      "\t\t\tFinal Test R2: 0.4469\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.1653\n",
      "\t\t\tDemean Test R2: 0.2235\n",
      "\n",
      "\t\t\tTotal time: 0.03 minutes\n",
      "    \n",
      "\tYear start: 2016\n",
      "\tHE: True\n",
      "\tAnomaly: True\n",
      "\t\tPass, one-hot encoding does not make sense with an anomaly model\n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.001, 0.0001]\n",
      "\t\t\tOne-hot encoding: True\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.7863 \n",
      "\t\t\tFinal Test R2: 0.8003\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.1109\n",
      "\t\t\tDemean Test R2: 0.4108\n",
      "\n",
      "\t\t\tTotal time: 0.22 minutes\n",
      "    \n",
      "\tHE: False\n",
      "\tAnomaly: True\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.001]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: True\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.5065 \n",
      "\t\t\tFinal Test R2: 0.4255\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.0887\n",
      "\t\t\tDemean Test R2: 0.1131\n",
      "\n",
      "\t\t\tTotal time: 0.02 minutes\n",
      "    \n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.001]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.4838 \n",
      "\t\t\tFinal Test R2: 0.4915\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.1050\n",
      "\t\t\tDemean Test R2: 0.1027\n",
      "\n",
      "\t\t\tTotal time: 0.02 minutes\n",
      "    \n",
      "Variable: ['tmp', 'ndvi']\n",
      "\tYear start: 2009\n",
      "\tHE: True\n",
      "\tAnomaly: True\n",
      "\t\tPass, one-hot encoding does not make sense with an anomaly model\n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['tmp', 'ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.001, 0.0001]\n",
      "\t\t\tOne-hot encoding: True\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.7393 \n",
      "\t\t\tFinal Test R2: 0.7345\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.3198\n",
      "\t\t\tDemean Test R2: 0.4665\n",
      "\n",
      "\t\t\tTotal time: 0.22 minutes\n",
      "    \n",
      "\tHE: False\n",
      "\tAnomaly: True\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['tmp', 'ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: True\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.4652 \n",
      "\t\t\tFinal Test R2: 0.4385\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.1617\n",
      "\t\t\tDemean Test R2: 0.1704\n",
      "\n",
      "\t\t\tTotal time: 0.03 minutes\n",
      "    \n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['tmp', 'ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.001]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.5985 \n",
      "\t\t\tFinal Test R2: 0.5953\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.3315\n",
      "\t\t\tDemean Test R2: 0.4048\n",
      "\n",
      "\t\t\tTotal time: 0.03 minutes\n",
      "    \n",
      "\tYear start: 2016\n",
      "\tHE: True\n",
      "\tAnomaly: True\n",
      "\t\tPass, one-hot encoding does not make sense with an anomaly model\n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['tmp', 'ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.001, 0.0001]\n",
      "\t\t\tOne-hot encoding: True\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.8134 \n",
      "\t\t\tFinal Test R2: 0.8385\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.2396\n",
      "\t\t\tDemean Test R2: 0.5628\n",
      "\n",
      "\t\t\tTotal time: 0.23 minutes\n",
      "    \n",
      "\tHE: False\n",
      "\tAnomaly: True\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['tmp', 'ndvi']\n",
      "\t\t\tLambdas:   [0.001, 0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: True\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.5748 \n",
      "\t\t\tFinal Test R2: 0.5782\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.1192\n",
      "\t\t\tDemean Test R2: 0.1436\n",
      "\n",
      "\t\t\tTotal time: 0.02 minutes\n",
      "    \n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['tmp', 'ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.001]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.6723 \n",
      "\t\t\tFinal Test R2: 0.7019\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.3730\n",
      "\t\t\tDemean Test R2: 0.4782\n",
      "\n",
      "\t\t\tTotal time: 0.02 minutes\n",
      "    \n",
      "Variable: ['pre', 'tmp', 'ndvi']\n",
      "\tYear start: 2009\n",
      "\tHE: True\n",
      "\tAnomaly: True\n",
      "\t\tPass, one-hot encoding does not make sense with an anomaly model\n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'tmp', 'ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.01, 0.01, 1e-05]\n",
      "\t\t\tOne-hot encoding: True\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.7545 \n",
      "\t\t\tFinal Test R2: 0.7516\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.3594\n",
      "\t\t\tDemean Test R2: 0.4993\n",
      "\n",
      "\t\t\tTotal time: 0.32 minutes\n",
      "    \n",
      "\tHE: False\n",
      "\tAnomaly: True\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'tmp', 'ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.01, 0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: True\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.4935 \n",
      "\t\t\tFinal Test R2: 0.4564\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.1792\n",
      "\t\t\tDemean Test R2: 0.1904\n",
      "\n",
      "\t\t\tTotal time: 0.06 minutes\n",
      "    \n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'tmp', 'ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.01, 0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.6281 \n",
      "\t\t\tFinal Test R2: 0.6393\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.3556\n",
      "\t\t\tDemean Test R2: 0.4343\n",
      "\n",
      "\t\t\tTotal time: 0.06 minutes\n",
      "    \n",
      "\tYear start: 2016\n",
      "\tHE: True\n",
      "\tAnomaly: True\n",
      "\t\tPass, one-hot encoding does not make sense with an anomaly model\n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'tmp', 'ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.001, 0.1, 1e-05]\n",
      "\t\t\tOne-hot encoding: True\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.8008 \n",
      "\t\t\tFinal Test R2: 0.8356\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.1744\n",
      "\t\t\tDemean Test R2: 0.6005\n",
      "\n",
      "\t\t\tTotal time: 0.31 minutes\n",
      "    \n",
      "\tHE: False\n",
      "\tAnomaly: True\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'tmp', 'ndvi']\n",
      "\t\t\tLambdas:   [0.1, 0.001, 0.01]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: True\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.5745 \n",
      "\t\t\tFinal Test R2: 0.5705\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.1206\n",
      "\t\t\tDemean Test R2: 0.1451\n",
      "\n",
      "\t\t\tTotal time: 0.06 minutes\n",
      "    \n",
      "\tAnomaly: False\n",
      "\n",
      "\t\tFinish:\n",
      "\t\t\tVariables: ['pre', 'tmp', 'ndvi']\n",
      "\t\t\tLambdas:   [0.01, 0.001, 0.1]\n",
      "\t\t\tOne-hot encoding: False\n",
      "\t\t\tAnomaly: False\n",
      "\n",
      "\t\t\tFinal Val  R2: 0.7006 \n",
      "\t\t\tFinal Test R2: 0.7515\n",
      "\n",
      "\t\t\tDemean Val  R2: 0.2988\n",
      "\t\t\tDemean Test R2: 0.5570\n",
      "\n",
      "\t\t\tTotal time: 0.06 minutes\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for var in clim:\n",
    "    print(f\"Variable: {var}\")\n",
    "    for yr in [2009, 2016]:\n",
    "        print(f\"\\tYear start: {yr}\")\n",
    "        for he in [True, False]:\n",
    "            print(f\"\\tHE: {he}\")\n",
    "            for anom in [True, False]:\n",
    "                print(f\"\\tAnomaly: {anom}\")\n",
    "                if he and anom:\n",
    "                    print(\n",
    "                        \"\\t\\tPass, one-hot encoding does not make sense with an anomaly model\"\n",
    "                    )\n",
    "                else:\n",
    "                    out = climate_model(\n",
    "                        variable_groups=var,\n",
    "                        year_start=yr,\n",
    "                        hot_encode=he,\n",
    "                        anomaly=anom,\n",
    "                        index_cols=[\"year\", \"district\", \"yield_mt\"],\n",
    "                    )\n",
    "                    output.append(out)\n",
    "\n",
    "results = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a32cc8f-6db7-4ce6-82e8-f4cf3aab6f28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variables</th>\n",
       "      <th>year_start</th>\n",
       "      <th>hot_encode</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>total_n</th>\n",
       "      <th>train_n</th>\n",
       "      <th>test_n</th>\n",
       "      <th>best_reg_param</th>\n",
       "      <th>mean_of_val_R2</th>\n",
       "      <th>val_R2</th>\n",
       "      <th>...</th>\n",
       "      <th>val_r2</th>\n",
       "      <th>train_R2</th>\n",
       "      <th>train_r</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>test_R2</th>\n",
       "      <th>test_r</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>demean_cv_R2</th>\n",
       "      <th>demean_cv_r</th>\n",
       "      <th>demean_cv_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['pre']</td>\n",
       "      <td>2009</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01, 0.0001]]</td>\n",
       "      <td>[[0.3277257775042445, 0.5824141187414437]]</td>\n",
       "      <td>0.584925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.586982</td>\n",
       "      <td>0.673345</td>\n",
       "      <td>0.820619</td>\n",
       "      <td>0.673416</td>\n",
       "      <td>0.607593</td>\n",
       "      <td>0.784196</td>\n",
       "      <td>0.614964</td>\n",
       "      <td>-0.087939</td>\n",
       "      <td>0.086422</td>\n",
       "      <td>0.007469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['pre']</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01]]</td>\n",
       "      <td>[[0.08960267369017674]]</td>\n",
       "      <td>0.092229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093043</td>\n",
       "      <td>0.120812</td>\n",
       "      <td>0.348091</td>\n",
       "      <td>0.121168</td>\n",
       "      <td>0.131660</td>\n",
       "      <td>0.369041</td>\n",
       "      <td>0.136192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['pre']</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01]]</td>\n",
       "      <td>[[0.321786743897409]]</td>\n",
       "      <td>0.323814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324037</td>\n",
       "      <td>0.344588</td>\n",
       "      <td>0.587108</td>\n",
       "      <td>0.344695</td>\n",
       "      <td>0.356793</td>\n",
       "      <td>0.597501</td>\n",
       "      <td>0.357007</td>\n",
       "      <td>-0.044408</td>\n",
       "      <td>0.242147</td>\n",
       "      <td>0.058635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['pre']</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001]]</td>\n",
       "      <td>[[0.32500092422759935, 0.6005759481906309]]</td>\n",
       "      <td>0.622504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631423</td>\n",
       "      <td>0.802148</td>\n",
       "      <td>0.896928</td>\n",
       "      <td>0.804480</td>\n",
       "      <td>0.694306</td>\n",
       "      <td>0.834720</td>\n",
       "      <td>0.696757</td>\n",
       "      <td>-0.627066</td>\n",
       "      <td>-0.371544</td>\n",
       "      <td>0.138045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['pre']</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01]]</td>\n",
       "      <td>[[0.11208094470342225]]</td>\n",
       "      <td>0.128262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128937</td>\n",
       "      <td>0.167730</td>\n",
       "      <td>0.410903</td>\n",
       "      <td>0.168841</td>\n",
       "      <td>0.143770</td>\n",
       "      <td>0.388156</td>\n",
       "      <td>0.150665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>['pre']</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01]]</td>\n",
       "      <td>[[0.317815981897002]]</td>\n",
       "      <td>0.343397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345237</td>\n",
       "      <td>0.394337</td>\n",
       "      <td>0.628025</td>\n",
       "      <td>0.394416</td>\n",
       "      <td>0.377233</td>\n",
       "      <td>0.614321</td>\n",
       "      <td>0.377391</td>\n",
       "      <td>-0.329451</td>\n",
       "      <td>-0.137914</td>\n",
       "      <td>0.019020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>['tmp']</td>\n",
       "      <td>2009</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01, 1e-05]]</td>\n",
       "      <td>[[0.5559933136470949, 0.7032702524465111]]</td>\n",
       "      <td>0.705109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.706553</td>\n",
       "      <td>0.771816</td>\n",
       "      <td>0.878597</td>\n",
       "      <td>0.771933</td>\n",
       "      <td>0.720124</td>\n",
       "      <td>0.851631</td>\n",
       "      <td>0.725275</td>\n",
       "      <td>0.232497</td>\n",
       "      <td>0.496481</td>\n",
       "      <td>0.246493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>['tmp']</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.001]]</td>\n",
       "      <td>[[0.34346137190266657]]</td>\n",
       "      <td>0.363192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.363984</td>\n",
       "      <td>0.395077</td>\n",
       "      <td>0.628576</td>\n",
       "      <td>0.395107</td>\n",
       "      <td>0.379433</td>\n",
       "      <td>0.624822</td>\n",
       "      <td>0.390402</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>['tmp']</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01]]</td>\n",
       "      <td>[[0.552363547644122]]</td>\n",
       "      <td>0.555149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555172</td>\n",
       "      <td>0.576410</td>\n",
       "      <td>0.759330</td>\n",
       "      <td>0.576582</td>\n",
       "      <td>0.536959</td>\n",
       "      <td>0.733677</td>\n",
       "      <td>0.538281</td>\n",
       "      <td>0.269600</td>\n",
       "      <td>0.535722</td>\n",
       "      <td>0.286998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>['tmp']</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001]]</td>\n",
       "      <td>[[0.6192899462388842, 0.7594750722874657]]</td>\n",
       "      <td>0.772396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.773940</td>\n",
       "      <td>0.873456</td>\n",
       "      <td>0.935209</td>\n",
       "      <td>0.874615</td>\n",
       "      <td>0.817738</td>\n",
       "      <td>0.905197</td>\n",
       "      <td>0.819381</td>\n",
       "      <td>0.021033</td>\n",
       "      <td>0.323420</td>\n",
       "      <td>0.104600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>['tmp']</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.001]]</td>\n",
       "      <td>[[0.4304913260271107]]</td>\n",
       "      <td>0.441335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441548</td>\n",
       "      <td>0.468315</td>\n",
       "      <td>0.684396</td>\n",
       "      <td>0.468398</td>\n",
       "      <td>0.482248</td>\n",
       "      <td>0.698206</td>\n",
       "      <td>0.487492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>['tmp']</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01]]</td>\n",
       "      <td>[[0.6151863495642378]]</td>\n",
       "      <td>0.632216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.632265</td>\n",
       "      <td>0.657021</td>\n",
       "      <td>0.810916</td>\n",
       "      <td>0.657585</td>\n",
       "      <td>0.649020</td>\n",
       "      <td>0.808263</td>\n",
       "      <td>0.653289</td>\n",
       "      <td>0.289468</td>\n",
       "      <td>0.540687</td>\n",
       "      <td>0.292342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>['ndvi']</td>\n",
       "      <td>2009</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01, 1e-08]]</td>\n",
       "      <td>[[0.22553958031513802, 0.6349264988217382]]</td>\n",
       "      <td>0.636312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637404</td>\n",
       "      <td>0.710712</td>\n",
       "      <td>0.843145</td>\n",
       "      <td>0.710894</td>\n",
       "      <td>0.595633</td>\n",
       "      <td>0.778781</td>\n",
       "      <td>0.606499</td>\n",
       "      <td>0.045040</td>\n",
       "      <td>0.272283</td>\n",
       "      <td>0.074138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>['ndvi']</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.001]]</td>\n",
       "      <td>[[0.2643220517239302]]</td>\n",
       "      <td>0.275685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275946</td>\n",
       "      <td>0.301838</td>\n",
       "      <td>0.549569</td>\n",
       "      <td>0.302026</td>\n",
       "      <td>0.140622</td>\n",
       "      <td>0.409199</td>\n",
       "      <td>0.167444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>['ndvi']</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01]]</td>\n",
       "      <td>[[0.215078677063197]]</td>\n",
       "      <td>0.219632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219720</td>\n",
       "      <td>0.242247</td>\n",
       "      <td>0.492479</td>\n",
       "      <td>0.242535</td>\n",
       "      <td>0.187410</td>\n",
       "      <td>0.433308</td>\n",
       "      <td>0.187756</td>\n",
       "      <td>0.129026</td>\n",
       "      <td>0.359384</td>\n",
       "      <td>0.129157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>['ndvi']</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001]]</td>\n",
       "      <td>[[0.25744743523510616, 0.7632153019026007]]</td>\n",
       "      <td>0.776423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776574</td>\n",
       "      <td>0.875685</td>\n",
       "      <td>0.937197</td>\n",
       "      <td>0.878338</td>\n",
       "      <td>0.804603</td>\n",
       "      <td>0.898677</td>\n",
       "      <td>0.807620</td>\n",
       "      <td>0.037282</td>\n",
       "      <td>0.303158</td>\n",
       "      <td>0.091905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>['ndvi']</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.001]]</td>\n",
       "      <td>[[0.4801450318708863]]</td>\n",
       "      <td>0.492882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494389</td>\n",
       "      <td>0.543259</td>\n",
       "      <td>0.737201</td>\n",
       "      <td>0.543466</td>\n",
       "      <td>0.452954</td>\n",
       "      <td>0.686441</td>\n",
       "      <td>0.471201</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>['ndvi']</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01]]</td>\n",
       "      <td>[[0.244861108267257]]</td>\n",
       "      <td>0.271104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272700</td>\n",
       "      <td>0.317410</td>\n",
       "      <td>0.563545</td>\n",
       "      <td>0.317583</td>\n",
       "      <td>0.324052</td>\n",
       "      <td>0.588325</td>\n",
       "      <td>0.346126</td>\n",
       "      <td>0.296150</td>\n",
       "      <td>0.557405</td>\n",
       "      <td>0.310700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>['pre', 'tmp']</td>\n",
       "      <td>2009</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.1, 0.01, 0.0001]]</td>\n",
       "      <td>[[0.48447855307209753, 0.5864791050228975, 0.7...</td>\n",
       "      <td>0.718134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719047</td>\n",
       "      <td>0.788175</td>\n",
       "      <td>0.887992</td>\n",
       "      <td>0.788530</td>\n",
       "      <td>0.734335</td>\n",
       "      <td>0.859217</td>\n",
       "      <td>0.738254</td>\n",
       "      <td>0.266313</td>\n",
       "      <td>0.526900</td>\n",
       "      <td>0.277624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>['pre', 'tmp']</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01, 0.01]]</td>\n",
       "      <td>[[0.19913701940712725, 0.37363111906098745]]</td>\n",
       "      <td>0.392568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.392649</td>\n",
       "      <td>0.440910</td>\n",
       "      <td>0.665222</td>\n",
       "      <td>0.442520</td>\n",
       "      <td>0.397393</td>\n",
       "      <td>0.635651</td>\n",
       "      <td>0.404052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>['pre', 'tmp']</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.1, 0.01]]</td>\n",
       "      <td>[[0.4807991010730441, 0.5833543044966605]]</td>\n",
       "      <td>0.585842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.585877</td>\n",
       "      <td>0.614869</td>\n",
       "      <td>0.784408</td>\n",
       "      <td>0.615297</td>\n",
       "      <td>0.570028</td>\n",
       "      <td>0.755464</td>\n",
       "      <td>0.570725</td>\n",
       "      <td>0.294753</td>\n",
       "      <td>0.562728</td>\n",
       "      <td>0.316663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>['pre', 'tmp']</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.1, 0.01, 0.01]]</td>\n",
       "      <td>[[0.5052548383406703, 0.6738933818363734, 0.76...</td>\n",
       "      <td>0.774578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775854</td>\n",
       "      <td>0.845276</td>\n",
       "      <td>0.922097</td>\n",
       "      <td>0.850264</td>\n",
       "      <td>0.801093</td>\n",
       "      <td>0.898263</td>\n",
       "      <td>0.806877</td>\n",
       "      <td>0.165001</td>\n",
       "      <td>0.456145</td>\n",
       "      <td>0.208068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>['pre', 'tmp']</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.0001]]</td>\n",
       "      <td>[[0.21844742000003992, 0.4487968984773298]]</td>\n",
       "      <td>0.459696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460779</td>\n",
       "      <td>0.509972</td>\n",
       "      <td>0.714205</td>\n",
       "      <td>0.510089</td>\n",
       "      <td>0.516576</td>\n",
       "      <td>0.721279</td>\n",
       "      <td>0.520244</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>['pre', 'tmp']</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.1, 0.01]]</td>\n",
       "      <td>[[0.5009054966917378, 0.670913395810407]]</td>\n",
       "      <td>0.686913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687049</td>\n",
       "      <td>0.724484</td>\n",
       "      <td>0.851868</td>\n",
       "      <td>0.725679</td>\n",
       "      <td>0.722417</td>\n",
       "      <td>0.853195</td>\n",
       "      <td>0.727942</td>\n",
       "      <td>0.308743</td>\n",
       "      <td>0.570704</td>\n",
       "      <td>0.325703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>['pre', 'ndvi']</td>\n",
       "      <td>2009</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01, 0.01, 1e-08]]</td>\n",
       "      <td>[[0.4018099488769525, 0.45089390161773213, 0.6...</td>\n",
       "      <td>0.689708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.690276</td>\n",
       "      <td>0.758389</td>\n",
       "      <td>0.870983</td>\n",
       "      <td>0.758611</td>\n",
       "      <td>0.663078</td>\n",
       "      <td>0.820958</td>\n",
       "      <td>0.673972</td>\n",
       "      <td>0.185676</td>\n",
       "      <td>0.444545</td>\n",
       "      <td>0.197620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>['pre', 'ndvi']</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.001, 0.001]]</td>\n",
       "      <td>[[0.1585579494564913, 0.34873830865858857]]</td>\n",
       "      <td>0.360569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361628</td>\n",
       "      <td>0.405340</td>\n",
       "      <td>0.636780</td>\n",
       "      <td>0.405488</td>\n",
       "      <td>0.258783</td>\n",
       "      <td>0.529775</td>\n",
       "      <td>0.280661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>['pre', 'ndvi']</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01, 0.01]]</td>\n",
       "      <td>[[0.3963664120737542, 0.44555018850510353]]</td>\n",
       "      <td>0.448587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.448631</td>\n",
       "      <td>0.478911</td>\n",
       "      <td>0.692317</td>\n",
       "      <td>0.479303</td>\n",
       "      <td>0.446868</td>\n",
       "      <td>0.670624</td>\n",
       "      <td>0.449736</td>\n",
       "      <td>0.165324</td>\n",
       "      <td>0.439643</td>\n",
       "      <td>0.193286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>['pre', 'ndvi']</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001, 0.0001]]</td>\n",
       "      <td>[[0.40933173229183695, 0.4629392954787269, 0.7...</td>\n",
       "      <td>0.786272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794674</td>\n",
       "      <td>0.903675</td>\n",
       "      <td>0.950671</td>\n",
       "      <td>0.903776</td>\n",
       "      <td>0.800254</td>\n",
       "      <td>0.895200</td>\n",
       "      <td>0.801383</td>\n",
       "      <td>0.110937</td>\n",
       "      <td>0.437815</td>\n",
       "      <td>0.191682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>['pre', 'ndvi']</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001]]</td>\n",
       "      <td>[[0.27179302348614504, 0.4960469855739006]]</td>\n",
       "      <td>0.506544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510041</td>\n",
       "      <td>0.588144</td>\n",
       "      <td>0.767078</td>\n",
       "      <td>0.588408</td>\n",
       "      <td>0.425502</td>\n",
       "      <td>0.654964</td>\n",
       "      <td>0.428978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>['pre', 'ndvi']</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001]]</td>\n",
       "      <td>[[0.4026155419394429, 0.4559516999439378]]</td>\n",
       "      <td>0.483837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487112</td>\n",
       "      <td>0.562675</td>\n",
       "      <td>0.750238</td>\n",
       "      <td>0.562856</td>\n",
       "      <td>0.491529</td>\n",
       "      <td>0.707057</td>\n",
       "      <td>0.499930</td>\n",
       "      <td>0.105026</td>\n",
       "      <td>0.418560</td>\n",
       "      <td>0.175193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>['tmp', 'ndvi']</td>\n",
       "      <td>2009</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01, 0.001, 0.0001]]</td>\n",
       "      <td>[[0.57831609480275, 0.5984901203103012, 0.7380...</td>\n",
       "      <td>0.739341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.740407</td>\n",
       "      <td>0.804263</td>\n",
       "      <td>0.896898</td>\n",
       "      <td>0.804426</td>\n",
       "      <td>0.734499</td>\n",
       "      <td>0.862403</td>\n",
       "      <td>0.743740</td>\n",
       "      <td>0.319785</td>\n",
       "      <td>0.574732</td>\n",
       "      <td>0.330317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>['tmp', 'ndvi']</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01, 0.01]]</td>\n",
       "      <td>[[0.37345818106618967, 0.4431327612374817]]</td>\n",
       "      <td>0.465151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465347</td>\n",
       "      <td>0.499137</td>\n",
       "      <td>0.707842</td>\n",
       "      <td>0.501040</td>\n",
       "      <td>0.438495</td>\n",
       "      <td>0.672924</td>\n",
       "      <td>0.452827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>['tmp', 'ndvi']</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01, 0.001]]</td>\n",
       "      <td>[[0.5749135050394183, 0.5951521156329985]]</td>\n",
       "      <td>0.598548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598833</td>\n",
       "      <td>0.636065</td>\n",
       "      <td>0.797653</td>\n",
       "      <td>0.636250</td>\n",
       "      <td>0.595325</td>\n",
       "      <td>0.773395</td>\n",
       "      <td>0.598140</td>\n",
       "      <td>0.331502</td>\n",
       "      <td>0.588314</td>\n",
       "      <td>0.346113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>['tmp', 'ndvi']</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001, 0.0001]]</td>\n",
       "      <td>[[0.6500788785199179, 0.6599605512837575, 0.80...</td>\n",
       "      <td>0.813376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.818970</td>\n",
       "      <td>0.913923</td>\n",
       "      <td>0.956047</td>\n",
       "      <td>0.914026</td>\n",
       "      <td>0.838477</td>\n",
       "      <td>0.917663</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.239563</td>\n",
       "      <td>0.526465</td>\n",
       "      <td>0.277165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>['tmp', 'ndvi']</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.001, 0.01]]</td>\n",
       "      <td>[[0.47662017805170176, 0.5669219178447976]]</td>\n",
       "      <td>0.574849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.575146</td>\n",
       "      <td>0.624195</td>\n",
       "      <td>0.790416</td>\n",
       "      <td>0.624758</td>\n",
       "      <td>0.578238</td>\n",
       "      <td>0.769435</td>\n",
       "      <td>0.592030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>['tmp', 'ndvi']</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001]]</td>\n",
       "      <td>[[0.6462135434654713, 0.6561010687076294]]</td>\n",
       "      <td>0.672292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672769</td>\n",
       "      <td>0.721118</td>\n",
       "      <td>0.849440</td>\n",
       "      <td>0.721548</td>\n",
       "      <td>0.701869</td>\n",
       "      <td>0.843120</td>\n",
       "      <td>0.710851</td>\n",
       "      <td>0.373019</td>\n",
       "      <td>0.623212</td>\n",
       "      <td>0.388393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>['pre', 'tmp', 'ndvi']</td>\n",
       "      <td>2009</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01, 0.01, 0.01, 1e-05]]</td>\n",
       "      <td>[[0.5267941059266128, 0.6054439310847656, 0.62...</td>\n",
       "      <td>0.754450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.755523</td>\n",
       "      <td>0.820945</td>\n",
       "      <td>0.906146</td>\n",
       "      <td>0.821101</td>\n",
       "      <td>0.751585</td>\n",
       "      <td>0.870941</td>\n",
       "      <td>0.758539</td>\n",
       "      <td>0.359438</td>\n",
       "      <td>0.607314</td>\n",
       "      <td>0.368830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>['pre', 'tmp', 'ndvi']</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01, 0.01, 0.01]]</td>\n",
       "      <td>[[0.24934987905064626, 0.400137079860259, 0.47...</td>\n",
       "      <td>0.493522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493528</td>\n",
       "      <td>0.541530</td>\n",
       "      <td>0.737143</td>\n",
       "      <td>0.543379</td>\n",
       "      <td>0.456442</td>\n",
       "      <td>0.682642</td>\n",
       "      <td>0.466001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>['pre', 'tmp', 'ndvi']</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>936</td>\n",
       "      <td>748</td>\n",
       "      <td>188</td>\n",
       "      <td>[[0.01, 0.01, 0.01]]</td>\n",
       "      <td>[[0.5234603613761596, 0.6024922463373249, 0.62...</td>\n",
       "      <td>0.628114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.628303</td>\n",
       "      <td>0.667345</td>\n",
       "      <td>0.817099</td>\n",
       "      <td>0.667651</td>\n",
       "      <td>0.639257</td>\n",
       "      <td>0.800029</td>\n",
       "      <td>0.640047</td>\n",
       "      <td>0.355594</td>\n",
       "      <td>0.611277</td>\n",
       "      <td>0.373660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>['pre', 'tmp', 'ndvi']</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001, 0.1, 1e-05]]</td>\n",
       "      <td>[[0.5526166611355223, 0.6862443448242243, 0.68...</td>\n",
       "      <td>0.800824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807923</td>\n",
       "      <td>0.909641</td>\n",
       "      <td>0.953785</td>\n",
       "      <td>0.909705</td>\n",
       "      <td>0.835635</td>\n",
       "      <td>0.914509</td>\n",
       "      <td>0.836327</td>\n",
       "      <td>0.174393</td>\n",
       "      <td>0.482337</td>\n",
       "      <td>0.232649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>['pre', 'tmp', 'ndvi']</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.1, 0.001, 0.01]]</td>\n",
       "      <td>[[0.3463398191464596, 0.48205260486036056, 0.5...</td>\n",
       "      <td>0.574508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574922</td>\n",
       "      <td>0.634736</td>\n",
       "      <td>0.797189</td>\n",
       "      <td>0.635510</td>\n",
       "      <td>0.570484</td>\n",
       "      <td>0.763028</td>\n",
       "      <td>0.582212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>['pre', 'tmp', 'ndvi']</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001, 0.1]]</td>\n",
       "      <td>[[0.5488000942253142, 0.6834571163021455, 0.68...</td>\n",
       "      <td>0.700604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701381</td>\n",
       "      <td>0.757647</td>\n",
       "      <td>0.870481</td>\n",
       "      <td>0.757737</td>\n",
       "      <td>0.751504</td>\n",
       "      <td>0.871583</td>\n",
       "      <td>0.759657</td>\n",
       "      <td>0.298824</td>\n",
       "      <td>0.585084</td>\n",
       "      <td>0.342323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 variables  year_start  hot_encode  anomaly  total_n  train_n  \\\n",
       "0                  ['pre']        2009        True    False      936      748   \n",
       "1                  ['pre']        2009       False     True      936      748   \n",
       "2                  ['pre']        2009       False    False      936      748   \n",
       "3                  ['pre']        2016        True    False      432      345   \n",
       "4                  ['pre']        2016       False     True      432      345   \n",
       "5                  ['pre']        2016       False    False      432      345   \n",
       "6                  ['tmp']        2009        True    False      936      748   \n",
       "7                  ['tmp']        2009       False     True      936      748   \n",
       "8                  ['tmp']        2009       False    False      936      748   \n",
       "9                  ['tmp']        2016        True    False      432      345   \n",
       "10                 ['tmp']        2016       False     True      432      345   \n",
       "11                 ['tmp']        2016       False    False      432      345   \n",
       "12                ['ndvi']        2009        True    False      936      748   \n",
       "13                ['ndvi']        2009       False     True      936      748   \n",
       "14                ['ndvi']        2009       False    False      936      748   \n",
       "15                ['ndvi']        2016        True    False      432      345   \n",
       "16                ['ndvi']        2016       False     True      432      345   \n",
       "17                ['ndvi']        2016       False    False      432      345   \n",
       "18          ['pre', 'tmp']        2009        True    False      936      748   \n",
       "19          ['pre', 'tmp']        2009       False     True      936      748   \n",
       "20          ['pre', 'tmp']        2009       False    False      936      748   \n",
       "21          ['pre', 'tmp']        2016        True    False      432      345   \n",
       "22          ['pre', 'tmp']        2016       False     True      432      345   \n",
       "23          ['pre', 'tmp']        2016       False    False      432      345   \n",
       "24         ['pre', 'ndvi']        2009        True    False      936      748   \n",
       "25         ['pre', 'ndvi']        2009       False     True      936      748   \n",
       "26         ['pre', 'ndvi']        2009       False    False      936      748   \n",
       "27         ['pre', 'ndvi']        2016        True    False      432      345   \n",
       "28         ['pre', 'ndvi']        2016       False     True      432      345   \n",
       "29         ['pre', 'ndvi']        2016       False    False      432      345   \n",
       "30         ['tmp', 'ndvi']        2009        True    False      936      748   \n",
       "31         ['tmp', 'ndvi']        2009       False     True      936      748   \n",
       "32         ['tmp', 'ndvi']        2009       False    False      936      748   \n",
       "33         ['tmp', 'ndvi']        2016        True    False      432      345   \n",
       "34         ['tmp', 'ndvi']        2016       False     True      432      345   \n",
       "35         ['tmp', 'ndvi']        2016       False    False      432      345   \n",
       "36  ['pre', 'tmp', 'ndvi']        2009        True    False      936      748   \n",
       "37  ['pre', 'tmp', 'ndvi']        2009       False     True      936      748   \n",
       "38  ['pre', 'tmp', 'ndvi']        2009       False    False      936      748   \n",
       "39  ['pre', 'tmp', 'ndvi']        2016        True    False      432      345   \n",
       "40  ['pre', 'tmp', 'ndvi']        2016       False     True      432      345   \n",
       "41  ['pre', 'tmp', 'ndvi']        2016       False    False      432      345   \n",
       "\n",
       "    test_n               best_reg_param  \\\n",
       "0      188             [[0.01, 0.0001]]   \n",
       "1      188                     [[0.01]]   \n",
       "2      188                     [[0.01]]   \n",
       "3       87              [[0.01, 0.001]]   \n",
       "4       87                     [[0.01]]   \n",
       "5       87                     [[0.01]]   \n",
       "6      188              [[0.01, 1e-05]]   \n",
       "7      188                    [[0.001]]   \n",
       "8      188                     [[0.01]]   \n",
       "9       87              [[0.01, 0.001]]   \n",
       "10      87                    [[0.001]]   \n",
       "11      87                     [[0.01]]   \n",
       "12     188              [[0.01, 1e-08]]   \n",
       "13     188                    [[0.001]]   \n",
       "14     188                     [[0.01]]   \n",
       "15      87              [[0.01, 0.001]]   \n",
       "16      87                    [[0.001]]   \n",
       "17      87                     [[0.01]]   \n",
       "18     188        [[0.1, 0.01, 0.0001]]   \n",
       "19     188               [[0.01, 0.01]]   \n",
       "20     188                [[0.1, 0.01]]   \n",
       "21      87          [[0.1, 0.01, 0.01]]   \n",
       "22      87             [[0.01, 0.0001]]   \n",
       "23      87                [[0.1, 0.01]]   \n",
       "24     188        [[0.01, 0.01, 1e-08]]   \n",
       "25     188             [[0.001, 0.001]]   \n",
       "26     188               [[0.01, 0.01]]   \n",
       "27      87      [[0.01, 0.001, 0.0001]]   \n",
       "28      87              [[0.01, 0.001]]   \n",
       "29      87              [[0.01, 0.001]]   \n",
       "30     188      [[0.01, 0.001, 0.0001]]   \n",
       "31     188               [[0.01, 0.01]]   \n",
       "32     188              [[0.01, 0.001]]   \n",
       "33      87      [[0.01, 0.001, 0.0001]]   \n",
       "34      87              [[0.001, 0.01]]   \n",
       "35      87              [[0.01, 0.001]]   \n",
       "36     188  [[0.01, 0.01, 0.01, 1e-05]]   \n",
       "37     188         [[0.01, 0.01, 0.01]]   \n",
       "38     188         [[0.01, 0.01, 0.01]]   \n",
       "39      87  [[0.01, 0.001, 0.1, 1e-05]]   \n",
       "40      87         [[0.1, 0.001, 0.01]]   \n",
       "41      87         [[0.01, 0.001, 0.1]]   \n",
       "\n",
       "                                       mean_of_val_R2    val_R2  ...  \\\n",
       "0          [[0.3277257775042445, 0.5824141187414437]]  0.584925  ...   \n",
       "1                             [[0.08960267369017674]]  0.092229  ...   \n",
       "2                               [[0.321786743897409]]  0.323814  ...   \n",
       "3         [[0.32500092422759935, 0.6005759481906309]]  0.622504  ...   \n",
       "4                             [[0.11208094470342225]]  0.128262  ...   \n",
       "5                               [[0.317815981897002]]  0.343397  ...   \n",
       "6          [[0.5559933136470949, 0.7032702524465111]]  0.705109  ...   \n",
       "7                             [[0.34346137190266657]]  0.363192  ...   \n",
       "8                               [[0.552363547644122]]  0.555149  ...   \n",
       "9          [[0.6192899462388842, 0.7594750722874657]]  0.772396  ...   \n",
       "10                             [[0.4304913260271107]]  0.441335  ...   \n",
       "11                             [[0.6151863495642378]]  0.632216  ...   \n",
       "12        [[0.22553958031513802, 0.6349264988217382]]  0.636312  ...   \n",
       "13                             [[0.2643220517239302]]  0.275685  ...   \n",
       "14                              [[0.215078677063197]]  0.219632  ...   \n",
       "15        [[0.25744743523510616, 0.7632153019026007]]  0.776423  ...   \n",
       "16                             [[0.4801450318708863]]  0.492882  ...   \n",
       "17                              [[0.244861108267257]]  0.271104  ...   \n",
       "18  [[0.48447855307209753, 0.5864791050228975, 0.7...  0.718134  ...   \n",
       "19       [[0.19913701940712725, 0.37363111906098745]]  0.392568  ...   \n",
       "20         [[0.4807991010730441, 0.5833543044966605]]  0.585842  ...   \n",
       "21  [[0.5052548383406703, 0.6738933818363734, 0.76...  0.774578  ...   \n",
       "22        [[0.21844742000003992, 0.4487968984773298]]  0.459696  ...   \n",
       "23          [[0.5009054966917378, 0.670913395810407]]  0.686913  ...   \n",
       "24  [[0.4018099488769525, 0.45089390161773213, 0.6...  0.689708  ...   \n",
       "25        [[0.1585579494564913, 0.34873830865858857]]  0.360569  ...   \n",
       "26        [[0.3963664120737542, 0.44555018850510353]]  0.448587  ...   \n",
       "27  [[0.40933173229183695, 0.4629392954787269, 0.7...  0.786272  ...   \n",
       "28        [[0.27179302348614504, 0.4960469855739006]]  0.506544  ...   \n",
       "29         [[0.4026155419394429, 0.4559516999439378]]  0.483837  ...   \n",
       "30  [[0.57831609480275, 0.5984901203103012, 0.7380...  0.739341  ...   \n",
       "31        [[0.37345818106618967, 0.4431327612374817]]  0.465151  ...   \n",
       "32         [[0.5749135050394183, 0.5951521156329985]]  0.598548  ...   \n",
       "33  [[0.6500788785199179, 0.6599605512837575, 0.80...  0.813376  ...   \n",
       "34        [[0.47662017805170176, 0.5669219178447976]]  0.574849  ...   \n",
       "35         [[0.6462135434654713, 0.6561010687076294]]  0.672292  ...   \n",
       "36  [[0.5267941059266128, 0.6054439310847656, 0.62...  0.754450  ...   \n",
       "37  [[0.24934987905064626, 0.400137079860259, 0.47...  0.493522  ...   \n",
       "38  [[0.5234603613761596, 0.6024922463373249, 0.62...  0.628114  ...   \n",
       "39  [[0.5526166611355223, 0.6862443448242243, 0.68...  0.800824  ...   \n",
       "40  [[0.3463398191464596, 0.48205260486036056, 0.5...  0.574508  ...   \n",
       "41  [[0.5488000942253142, 0.6834571163021455, 0.68...  0.700604  ...   \n",
       "\n",
       "      val_r2  train_R2   train_r  train_r2   test_R2    test_r   test_r2  \\\n",
       "0   0.586982  0.673345  0.820619  0.673416  0.607593  0.784196  0.614964   \n",
       "1   0.093043  0.120812  0.348091  0.121168  0.131660  0.369041  0.136192   \n",
       "2   0.324037  0.344588  0.587108  0.344695  0.356793  0.597501  0.357007   \n",
       "3   0.631423  0.802148  0.896928  0.804480  0.694306  0.834720  0.696757   \n",
       "4   0.128937  0.167730  0.410903  0.168841  0.143770  0.388156  0.150665   \n",
       "5   0.345237  0.394337  0.628025  0.394416  0.377233  0.614321  0.377391   \n",
       "6   0.706553  0.771816  0.878597  0.771933  0.720124  0.851631  0.725275   \n",
       "7   0.363984  0.395077  0.628576  0.395107  0.379433  0.624822  0.390402   \n",
       "8   0.555172  0.576410  0.759330  0.576582  0.536959  0.733677  0.538281   \n",
       "9   0.773940  0.873456  0.935209  0.874615  0.817738  0.905197  0.819381   \n",
       "10  0.441548  0.468315  0.684396  0.468398  0.482248  0.698206  0.487492   \n",
       "11  0.632265  0.657021  0.810916  0.657585  0.649020  0.808263  0.653289   \n",
       "12  0.637404  0.710712  0.843145  0.710894  0.595633  0.778781  0.606499   \n",
       "13  0.275946  0.301838  0.549569  0.302026  0.140622  0.409199  0.167444   \n",
       "14  0.219720  0.242247  0.492479  0.242535  0.187410  0.433308  0.187756   \n",
       "15  0.776574  0.875685  0.937197  0.878338  0.804603  0.898677  0.807620   \n",
       "16  0.494389  0.543259  0.737201  0.543466  0.452954  0.686441  0.471201   \n",
       "17  0.272700  0.317410  0.563545  0.317583  0.324052  0.588325  0.346126   \n",
       "18  0.719047  0.788175  0.887992  0.788530  0.734335  0.859217  0.738254   \n",
       "19  0.392649  0.440910  0.665222  0.442520  0.397393  0.635651  0.404052   \n",
       "20  0.585877  0.614869  0.784408  0.615297  0.570028  0.755464  0.570725   \n",
       "21  0.775854  0.845276  0.922097  0.850264  0.801093  0.898263  0.806877   \n",
       "22  0.460779  0.509972  0.714205  0.510089  0.516576  0.721279  0.520244   \n",
       "23  0.687049  0.724484  0.851868  0.725679  0.722417  0.853195  0.727942   \n",
       "24  0.690276  0.758389  0.870983  0.758611  0.663078  0.820958  0.673972   \n",
       "25  0.361628  0.405340  0.636780  0.405488  0.258783  0.529775  0.280661   \n",
       "26  0.448631  0.478911  0.692317  0.479303  0.446868  0.670624  0.449736   \n",
       "27  0.794674  0.903675  0.950671  0.903776  0.800254  0.895200  0.801383   \n",
       "28  0.510041  0.588144  0.767078  0.588408  0.425502  0.654964  0.428978   \n",
       "29  0.487112  0.562675  0.750238  0.562856  0.491529  0.707057  0.499930   \n",
       "30  0.740407  0.804263  0.896898  0.804426  0.734499  0.862403  0.743740   \n",
       "31  0.465347  0.499137  0.707842  0.501040  0.438495  0.672924  0.452827   \n",
       "32  0.598833  0.636065  0.797653  0.636250  0.595325  0.773395  0.598140   \n",
       "33  0.818970  0.913923  0.956047  0.914026  0.838477  0.917663  0.842105   \n",
       "34  0.575146  0.624195  0.790416  0.624758  0.578238  0.769435  0.592030   \n",
       "35  0.672769  0.721118  0.849440  0.721548  0.701869  0.843120  0.710851   \n",
       "36  0.755523  0.820945  0.906146  0.821101  0.751585  0.870941  0.758539   \n",
       "37  0.493528  0.541530  0.737143  0.543379  0.456442  0.682642  0.466001   \n",
       "38  0.628303  0.667345  0.817099  0.667651  0.639257  0.800029  0.640047   \n",
       "39  0.807923  0.909641  0.953785  0.909705  0.835635  0.914509  0.836327   \n",
       "40  0.574922  0.634736  0.797189  0.635510  0.570484  0.763028  0.582212   \n",
       "41  0.701381  0.757647  0.870481  0.757737  0.751504  0.871583  0.759657   \n",
       "\n",
       "    demean_cv_R2  demean_cv_r  demean_cv_r2  \n",
       "0      -0.087939     0.086422      0.007469  \n",
       "1            NaN          NaN           NaN  \n",
       "2      -0.044408     0.242147      0.058635  \n",
       "3      -0.627066    -0.371544      0.138045  \n",
       "4            NaN          NaN           NaN  \n",
       "5      -0.329451    -0.137914      0.019020  \n",
       "6       0.232497     0.496481      0.246493  \n",
       "7            NaN          NaN           NaN  \n",
       "8       0.269600     0.535722      0.286998  \n",
       "9       0.021033     0.323420      0.104600  \n",
       "10           NaN          NaN           NaN  \n",
       "11      0.289468     0.540687      0.292342  \n",
       "12      0.045040     0.272283      0.074138  \n",
       "13           NaN          NaN           NaN  \n",
       "14      0.129026     0.359384      0.129157  \n",
       "15      0.037282     0.303158      0.091905  \n",
       "16           NaN          NaN           NaN  \n",
       "17      0.296150     0.557405      0.310700  \n",
       "18      0.266313     0.526900      0.277624  \n",
       "19           NaN          NaN           NaN  \n",
       "20      0.294753     0.562728      0.316663  \n",
       "21      0.165001     0.456145      0.208068  \n",
       "22           NaN          NaN           NaN  \n",
       "23      0.308743     0.570704      0.325703  \n",
       "24      0.185676     0.444545      0.197620  \n",
       "25           NaN          NaN           NaN  \n",
       "26      0.165324     0.439643      0.193286  \n",
       "27      0.110937     0.437815      0.191682  \n",
       "28           NaN          NaN           NaN  \n",
       "29      0.105026     0.418560      0.175193  \n",
       "30      0.319785     0.574732      0.330317  \n",
       "31           NaN          NaN           NaN  \n",
       "32      0.331502     0.588314      0.346113  \n",
       "33      0.239563     0.526465      0.277165  \n",
       "34           NaN          NaN           NaN  \n",
       "35      0.373019     0.623212      0.388393  \n",
       "36      0.359438     0.607314      0.368830  \n",
       "37           NaN          NaN           NaN  \n",
       "38      0.355594     0.611277      0.373660  \n",
       "39      0.174393     0.482337      0.232649  \n",
       "40           NaN          NaN           NaN  \n",
       "41      0.298824     0.585084      0.342323  \n",
       "\n",
       "[42 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = results.anomaly == True\n",
    "cols = [\"demean_cv_R2\", \"demean_cv_r\", \"demean_cv_r2\"]\n",
    "results.loc[mask, cols] = np.nan\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2a71549-38bc-4ac9-a766-3f3b34a20c44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results as: climate_model_2023-03-22.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "file_name = f\"climate_model_{today}.csv\"\n",
    "print(f\"Saving results as: {file_name}\\n\\n\")\n",
    "results.to_csv(here(\"data\", \"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af8e6b69-af3f-4cc6-b3e5-15c379a6cb71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# climate_model(\n",
    "#     pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\")),\n",
    "#     year_start=2016,\n",
    "#     variable_groups=[\"pre\", \"tmp\", \"ndvi\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00013f9f-02aa-406c-bfeb-45b3a2797796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paramlist = list(itertools.product([True, False], [True, False]))\n",
    "paramlist = [list(elem) for elem in paramlist]\n",
    "paramlist = list(itertools.product(clim, paramlist))\n",
    "# paramlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "004faa42-d0d1-47c8-a839-3d78e7a37f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output = []\n",
    "# for ls in clim:\n",
    "#     print(ls)\n",
    "#     for he in HE:\n",
    "#         print(he)\n",
    "#         out = climate_model(\n",
    "#             variable_groups=ls,\n",
    "#             hot_encode=he,\n",
    "#             index_cols=[\"year\", \"district\", \"yield_mt\"],\n",
    "#         )\n",
    "#         output.append(out)\n",
    "# results = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c7157f-9143-4402-bae6-ee4ed7b38381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db396f8-bdbf-4a60-a890-88b0a649a109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paramlist = (i for i in paramlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44732bb7-40f7-405e-acff-fc4bd7e3c459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def climate_model(variable_groups, he_anom):\n",
    "    print(variable_groups, flush=True)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    multiprocessing.Pool().starmap(climate_model, paramlist)\n",
    "    mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b796de9-e8d0-4f7e-9b6b-6bcccaa88890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time    \n",
    "##### With progress bar\n",
    "workers = os.cpu_count()\n",
    "if __name__ == \"__main__\":\n",
    "    multiprocessing.Pool().starmap(climate_model, paramlist)\n",
    "    # with multiprocessing.Pool(processes=workers) as pool:\n",
    "    #     output = pool.starmap(climate_model, paramlist)\n",
    "    # output = p_tqdm.p_umap(climate_model, paramlist, num_cpus=workers)\n",
    "    # results = pd.concat(output).reset_index(drop=True)\n",
    "    # today = date.today().strftime(\"%Y-%m-%d\")\n",
    "    # file_name = f'results_{today}.csv'\n",
    "    # print(f\"Saving results as: {file_name}\\n\\n\")           \n",
    "    # results.to_csv(here(\"data\",\"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa19ec9-c7e8-4093-b549-426f060260a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df7999a-74a6-4cd8-b754-674c75ed00cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest Î» 1: 0.01\n",
      "\tVal R2 1: 0.5526\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest Î» 2: 0.001\n",
      "\tVal R2 2: 0.6862\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest Î» 3: 0.1\n",
      "\tVal R2 3: 0.6868\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest Î» 4: 1e-05\n",
      "\tVal R2 4: 0.7895\n",
      "\n",
      "\n",
      "Finish:\n",
      "    Variables: ['pre', 'tmp', 'ndvi']\n",
      "    Lambdas: [0.01, 0.001, 0.1, 1e-05]\n",
      "    One-hot encoding: True\n",
      "    Anomaly: False\n",
      "\n",
      "    Final Val  R2: 0.8008 \n",
      "    Final Test R2: 0.8356\n",
      "\n",
      "    Demean Val  R2: 0.1744\n",
      "    Demean Test R2: 0.6005\n",
      "\n",
      "    Total time: 0.30 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TESTING\n",
    "\n",
    "data = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "hot_encode = True\n",
    "anomaly = False\n",
    "variable_groups = [\"pre\", \"tmp\", \"ndvi\"]\n",
    "index_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "year_start = 2016\n",
    "n_splits = 5\n",
    "\n",
    "#########################################     READ DATA    #########################################\n",
    "data = data.dropna()\n",
    "\n",
    "keep_cols = []\n",
    "\n",
    "for var in variable_groups:\n",
    "    tmp = data.columns[data.columns.to_series().str.contains(var)].tolist()\n",
    "    keep_cols.append(tmp)\n",
    "\n",
    "keep_cols = [*index_cols, *[col for cols in keep_cols for col in cols]]\n",
    "\n",
    "data = data.loc[:, keep_cols]\n",
    "\n",
    "data = data[data.year >= year_start]\n",
    "\n",
    "crop_yield = data.copy().loc[:, tuple(index_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "########################################    STANDARDIZE FEATURES    #########################################\n",
    "data = data.set_index(index_cols)\n",
    "data_scaled = StandardScaler().fit_transform(data.values)\n",
    "data = pd.DataFrame(data_scaled, index=data.index).reset_index()\n",
    "data.columns = data.columns.astype(str)\n",
    "\n",
    "#########################################    HOT ENCODE    #########################################\n",
    "if hot_encode:\n",
    "    index_cols.remove(\"district\")\n",
    "    data = pd.get_dummies(data, columns=[\"district\"], drop_first=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = data.drop(index_cols, axis=1)\n",
    "y_all = np.log10(data.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV    #########################################\n",
    "### SETUP\n",
    "tic = time.time()\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "\n",
    "i = 0\n",
    "start = [i]\n",
    "end = [x_train.shape[1]]\n",
    "\n",
    "for var in variable_groups:\n",
    "    i += 12\n",
    "    start.append(i)\n",
    "    end.append(i)\n",
    "start.sort()\n",
    "end.sort()\n",
    "\n",
    "if not hot_encode:\n",
    "    start = start[0:-1]\n",
    "    end = end[0:-1]\n",
    "\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER(S)\n",
    "best_lambdas, best_scores, best_model = kfold_rr_multi_lambda_tuning(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    grid=alphas.get(\"alpha\"),\n",
    "    n_splits=n_splits,\n",
    "    start=start,\n",
    "    end=end,\n",
    "    static_lam=1,\n",
    "    verbose=2,\n",
    "    show_linalg_warning=False,\n",
    "    fit_model_after_tuning=True,\n",
    ")\n",
    "### PREDICT WITH BEST HYPERPARAMETER(S)\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################\n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"cv_prediction\"] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"] - train_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\n",
    "    \"cv_prediction\"\n",
    "] - train_split.groupby(\"district\")[\"cv_prediction\"].transform(\"mean\")\n",
    "\n",
    "test_split = pd.DataFrame(\n",
    "    np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"prediction\"].transform(\"mean\")\n",
    "\n",
    "# variable_groups.append(\"districts\")\n",
    "# group_lambdas = dict(zip(variable_groups, best_lambdas))\n",
    "# group_lambdas\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Finish:\n",
    "    Variables: {variable_groups}\n",
    "    Lambdas: {best_lambdas}\n",
    "    One-hot encoding: {hot_encode}\n",
    "    Anomaly: {anomaly}\n",
    "\n",
    "    Final Val  R2: {r2_score(y_train, val_predictions):0.4f} \n",
    "    Final Test R2: {r2_score(y_test, test_predictions):0.4f}\n",
    "\n",
    "    Demean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.4f}\n",
    "    Demean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.4f}\n",
    "\n",
    "    Total time: {(time.time()-tic)/60:0.2f} minutes\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffc9fc-32ea-47c7-93fc-0d0aea13d688",
   "metadata": {},
   "source": [
    "# NDVI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50874f01-370e-40eb-84c1-4f98234d2632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.7802 \n",
      "Test R2: 0.8085 \n",
      "\n",
      "Demean Val  R2: 0.0921 \n",
      "Demean Test R2: 0.4394\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains(\"ndvi\")]\n",
    "keep_cols = [*ndvi_cols, *drop_cols]\n",
    "climate_df = climate_df.loc[:, keep_cols]\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "hot_encode = True\n",
    "# hot_encode = False\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    HOT ENCODE    #########################################\n",
    "if hot_encode:\n",
    "    drop_cols.remove(\"district\")\n",
    "    climate_df = pd.get_dummies(climate_df, columns=[\"district\"], drop_first=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "climate_df = climate_df.set_index(drop_cols)\n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis=1)\n",
    "y_all = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring=\"r2\", cv=kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################\n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"cv_prediction\"] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"] - train_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\n",
    "    \"cv_prediction\"\n",
    "] - train_split.groupby(\"district\")[\"cv_prediction\"].transform(\"mean\")\n",
    "\n",
    "test_split = pd.DataFrame(\n",
    "    np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"prediction\"].transform(\"mean\")\n",
    "\n",
    "print(\n",
    "    f\"Val  R2: {r2_score(y_train, val_predictions):0.4f}\",\n",
    "    f\"\\nTest R2: {r2_score(y_test, test_predictions):0.4f}\",\n",
    "    f\"\\n\\nDemean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.4f}\",\n",
    "    f\"\\nDemean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.4f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3eef3-18bb-4b2b-8da8-02bc0230bda2",
   "metadata": {},
   "source": [
    "# Precipitation, Temperature, and NDVI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1840a92e-3eca-4039-a5d4-6a8367316314",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.8044 \n",
      "Test R2: 0.8297 \n",
      "\n",
      "Demean Val  R2: 0.1723 \n",
      "Demean Test R2: 0.5297\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "hot_encode = True\n",
    "# hot_encode = False\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    HOT ENCODE    #########################################\n",
    "if hot_encode:\n",
    "    drop_cols.remove(\"district\")\n",
    "    climate_df = pd.get_dummies(climate_df, columns=[\"district\"], drop_first=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "climate_df = climate_df.set_index(drop_cols)\n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis=1)\n",
    "y_all = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring=\"r2\", cv=kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################\n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"cv_prediction\"] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"] - train_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\n",
    "    \"cv_prediction\"\n",
    "] - train_split.groupby(\"district\")[\"cv_prediction\"].transform(\"mean\")\n",
    "\n",
    "test_split = pd.DataFrame(\n",
    "    np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"prediction\"].transform(\"mean\")\n",
    "\n",
    "print(\n",
    "    f\"Val  R2: {r2_score(y_train, val_predictions):0.4f}\",\n",
    "    f\"\\nTest R2: {r2_score(y_test, test_predictions):0.4f}\",\n",
    "    f\"\\n\\nDemean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.4f}\",\n",
    "    f\"\\nDemean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.4f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b577bc71-d766-42fe-adad-93286a5a19ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.792567922097443"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_reg.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3dfe35-025c-4b06-8b2b-23b4f223f11b",
   "metadata": {},
   "source": [
    "# NDVI Anomaly Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f20bb178-7708-45e3-aa68-124c88541b05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.4449\n",
      "Test R2: 0.4293\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains(\"ndvi\")]\n",
    "keep_cols = [*ndvi_cols, *drop_cols]\n",
    "climate_df = climate_df.loc[:, keep_cols]\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "climate_df = climate_df.set_index(drop_cols)\n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################\n",
    "climate_df[\"yield_mt\"] = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "climate_df.set_index([\"year\", \"district\"], inplace=True)\n",
    "var_cols = climate_df.columns\n",
    "climate_df = climate_df[var_cols] - climate_df.groupby([\"district\"], as_index=True)[\n",
    "    var_cols\n",
    "].transform(\"mean\")\n",
    "climate_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis=1)\n",
    "y_all = climate_df.yield_mt\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring=\"r2\", cv=kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################\n",
    "crop_yield[\"prediction\"] = best_model.predict(x_all)\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"cv_prediction\"] = val_predictions\n",
    "\n",
    "test_split = pd.DataFrame(\n",
    "    np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "print(\n",
    "    f\"Val  R2: {r2_score(y_train, val_predictions):0.4f}\\nTest R2: {r2_score(y_test, test_predictions):0.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c574bb5-882f-4588-ae97-44af906398a6",
   "metadata": {},
   "source": [
    "# Precipitation, Temperature, and NDVI  Anomaly model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "102e934f-c6e8-4222-9956-a621dda75298",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.5284\n",
      "Test R2: 0.5145\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "climate_df = climate_df.set_index(drop_cols)\n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################\n",
    "climate_df[\"yield_mt\"] = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "climate_df.set_index([\"year\", \"district\"], inplace=True)\n",
    "var_cols = climate_df.columns\n",
    "climate_df = climate_df[var_cols] - climate_df.groupby([\"district\"], as_index=True)[\n",
    "    var_cols\n",
    "].transform(\"mean\")\n",
    "climate_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis=1)\n",
    "y_all = climate_df.yield_mt\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring=\"r2\", cv=kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################\n",
    "crop_yield[\"prediction\"] = best_model.predict(x_all)\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"cv_prediction\"] = val_predictions\n",
    "\n",
    "test_split = pd.DataFrame(\n",
    "    np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "print(\n",
    "    f\"Val  R2: {r2_score(y_train, val_predictions):0.4f}\\nTest R2: {r2_score(y_test, test_predictions):0.4f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
