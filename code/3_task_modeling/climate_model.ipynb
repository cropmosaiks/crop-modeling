{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aaf7810-5612-46c5-8369-1e95387fe44f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q pyhere p_tqdm glum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0fcec9b-2f29-4ef1-b070-dd823431cbf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d64e464-31b8-4c88-bbf5-877a8fa59fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "from pyhere import here\n",
    "from datetime import date\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyarrow\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import p_tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    GridSearchCV,\n",
    "    cross_val_predict\n",
    ")\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "from task_modeling_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13b3750c-d520-4102-8fe8-dd4b4a4358f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def climate_model(\n",
    "    variable_groups=[\"pre\", \"tmp\", \"ndvi\"],\n",
    "    # he_anom=[True, False],\n",
    "    hot_encode=True,\n",
    "    anomaly=False,\n",
    "    index_cols=[\"year\", \"district\", \"yield_mt\"],\n",
    "    year_start=2016,\n",
    "    n_splits=5,\n",
    "):\n",
    "    #########################################     READ DATA    #########################################\n",
    "    data = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "    data = data.dropna()\n",
    "\n",
    "    # hot_encode = he_anom[0]\n",
    "    # anom = he_anom[1]\n",
    "\n",
    "    keep_cols = []\n",
    "\n",
    "    for var in variable_groups:\n",
    "        tmp = data.columns[data.columns.to_series().str.contains(var)].tolist()\n",
    "        keep_cols.append(tmp)\n",
    "\n",
    "    keep_cols = [*index_cols, *[col for cols in keep_cols for col in cols]]\n",
    "\n",
    "    data = data.loc[:, keep_cols]\n",
    "\n",
    "    data = data[data.year >= year_start]\n",
    "\n",
    "    crop_yield = data.copy().loc[:, tuple(index_cols)].reset_index(drop=True)\n",
    "    crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "    ########################################    STANDARDIZE FEATURES    #########################################\n",
    "    data = data.set_index(index_cols)\n",
    "    data_scaled = StandardScaler().fit_transform(data.values)\n",
    "    data = pd.DataFrame(data_scaled, index=data.index).reset_index()\n",
    "    data.columns = data.columns.astype(str)\n",
    "\n",
    "    #########################################    HOT ENCODE    #########################################\n",
    "    if hot_encode:\n",
    "        index_cols.remove(\"district\")\n",
    "        data = pd.get_dummies(data, columns=[\"district\"], drop_first=False)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    #########################################     K-FOLD SPLIT    #########################################\n",
    "    x_all = data.drop(index_cols, axis=1)\n",
    "    y_all = np.log10(data.yield_mt.to_numpy() + 1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_all, y_all, test_size=0.2, random_state=0\n",
    "    )\n",
    "\n",
    "    #########################################     K-FOLD CV    #########################################\n",
    "    ### SETUP\n",
    "    tic = time.time()\n",
    "    kfold = KFold(n_splits=n_splits)\n",
    "    alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "\n",
    "    i = 0\n",
    "    start = [i]\n",
    "    end = [x_train.shape[1]]\n",
    "\n",
    "    for var in variable_groups:\n",
    "        i += 12\n",
    "        start.append(i)\n",
    "        end.append(i)\n",
    "    start.sort()\n",
    "    end.sort()\n",
    "\n",
    "    if not hot_encode:\n",
    "        start = start[0:-1]\n",
    "        end = end[0:-1]\n",
    "\n",
    "    ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER(S)\n",
    "    best_lambdas, best_scores, best_model = kfold_rr_multi_lambda_tuning(\n",
    "        X=x_train,\n",
    "        y=y_train,\n",
    "        grid=alphas.get(\"alpha\"),\n",
    "        n_splits=n_splits,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        static_lam=1,\n",
    "        verbose=0,\n",
    "        show_linalg_warning=False,\n",
    "        fit_model_after_tuning=True,\n",
    "    )\n",
    "    ### PREDICT WITH BEST HYPERPARAMETER(S)\n",
    "    val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "    train_predictions = best_model.predict(x_train)\n",
    "    test_predictions = best_model.predict(x_test)\n",
    "\n",
    "    #########################################     DE-MEAN R2    #########################################\n",
    "    crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "    train_split = pd.DataFrame(\n",
    "        np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    "    )\n",
    "    train_split = train_split.join(\n",
    "        crop_yield.copy()[crop_yield.index.isin(x_train.index)]\n",
    "    )\n",
    "    train_split[\"cv_prediction\"] = np.maximum(val_predictions, 0)\n",
    "    train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"] - train_split.groupby(\n",
    "        \"district\"\n",
    "    )[\"log_yield\"].transform(\"mean\")\n",
    "    train_split[\"demean_cv_prediction\"] = train_split[\n",
    "        \"cv_prediction\"\n",
    "    ] - train_split.groupby(\"district\")[\"cv_prediction\"].transform(\"mean\")\n",
    "\n",
    "    test_split = pd.DataFrame(\n",
    "        np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    "    )\n",
    "    test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "    test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "    test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "    test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "    predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "    test_split[\"demean_test_yield\"] = test_split[\"log_yield\"] - test_split.groupby(\n",
    "        \"district\"\n",
    "    )[\"log_yield\"].transform(\"mean\")\n",
    "    test_split[\"demean_test_prediction\"] = test_split[\n",
    "        \"prediction\"\n",
    "    ] - test_split.groupby(\"district\")[\"prediction\"].transform(\"mean\")\n",
    "\n",
    "    print(\n",
    "        f\"\"\"\n",
    "Finish:\n",
    "    Variables: {variable_groups}\n",
    "    Lambdas:   {best_lambdas}\n",
    "    One-hot encoding: {hot_encode}\n",
    "    Anomaly: {anomaly}\n",
    "    \n",
    "    Final Val  R2: {r2_score(y_train, val_predictions):0.4f} \n",
    "    Final Test R2: {r2_score(y_test, test_predictions):0.4f}\n",
    "    \n",
    "    Demean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.4f}\n",
    "    Demean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.4f}\n",
    "    \n",
    "    Total time: {(time.time()-tic)/60:0.2f} minutes\n",
    "    \"\"\"\n",
    "    )\n",
    "    d = {\n",
    "        \"variables\": variable_groups,\n",
    "        \"year_start\": year_start,\n",
    "        \"hot_encode\": hot_encode,\n",
    "        \"anomaly\": anomaly,\n",
    "        \"total_n\": len(x_all),\n",
    "        \"train_n\": len(x_train),\n",
    "        \"test_n\": len(x_test),\n",
    "        \"best_reg_param\": [best_lambdas],\n",
    "        \"mean_of_val_R2\": [best_scores],\n",
    "        \"val_R2\": r2_score(y_train, val_predictions),\n",
    "        \"val_r\": pearsonr(val_predictions, y_train)[0],\n",
    "        \"val_r2\": pearsonr(val_predictions, y_train)[0] ** 2,\n",
    "        \"train_R2\": r2_score(y_train, train_predictions),\n",
    "        \"train_r\": pearsonr(train_predictions, y_train)[0],\n",
    "        \"train_r2\": pearsonr(train_predictions, y_train)[0] ** 2,\n",
    "        \"test_R2\": r2_score(y_test, test_predictions),\n",
    "        \"test_r\": pearsonr(test_predictions, y_test)[0],\n",
    "        \"test_r2\": pearsonr(test_predictions, y_test)[0] ** 2,\n",
    "        \"demean_cv_R2\": r2_score(\n",
    "            train_split.demean_cv_yield, train_split.demean_cv_prediction\n",
    "        ),\n",
    "        \"demean_cv_r\": pearsonr(\n",
    "            train_split.demean_cv_yield, train_split.demean_cv_prediction\n",
    "        )[0],\n",
    "        \"demean_cv_r2\": pearsonr(\n",
    "            train_split.demean_cv_yield, train_split.demean_cv_prediction\n",
    "        )[0]\n",
    "        ** 2,\n",
    "    }\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af8e6b69-af3f-4cc6-b3e5-15c379a6cb71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# climate_model(\n",
    "#     pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\")),\n",
    "#     year_start=2016,\n",
    "#     variable_groups=[\"pre\", \"tmp\", \"ndvi\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2216f1bb-8f61-417f-bbfc-6bba313014c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variables = [\"pre\", \"tmp\", \"ndvi\"]\n",
    "HE = [True, False]\n",
    "anom = [True, False]\n",
    "\n",
    "clim = list(itertools.combinations(variables, 2))\n",
    "clim.append([variables[0]])\n",
    "clim.append([variables[1]])\n",
    "clim.append([variables[2]])\n",
    "clim.append(variables)\n",
    "clim = [list(elem) for elem in clim]\n",
    "clim.sort(key=len)\n",
    "# clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00013f9f-02aa-406c-bfeb-45b3a2797796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paramlist = list(itertools.product([True, False], [True, False]))\n",
    "paramlist = [list(elem) for elem in paramlist]\n",
    "paramlist = list(itertools.product(clim, paramlist))\n",
    "# paramlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "004faa42-d0d1-47c8-a839-3d78e7a37f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pre']\n",
      "True\n",
      "\n",
      "Finish:\n",
      "    Variables: ['pre']\n",
      "    Lambdas:   [0.01, 0.001]\n",
      "    One-hot encoding: True\n",
      "    Anomaly: False\n",
      "    \n",
      "    Final Val  R2: 0.6225 \n",
      "    Final Test R2: 0.6943\n",
      "    \n",
      "    Demean Val  R2: -0.6271\n",
      "    Demean Test R2: 0.1442\n",
      "    \n",
      "    Total time: 0.16 minutes\n",
      "    \n",
      "False\n",
      "\n",
      "Finish:\n",
      "    Variables: ['pre']\n",
      "    Lambdas:   [0.01]\n",
      "    One-hot encoding: False\n",
      "    Anomaly: False\n",
      "    \n",
      "    Final Val  R2: 0.3434 \n",
      "    Final Test R2: 0.3772\n",
      "    \n",
      "    Demean Val  R2: -0.3295\n",
      "    Demean Test R2: -0.2086\n",
      "    \n",
      "    Total time: 0.02 minutes\n",
      "    \n",
      "['tmp']\n",
      "True\n",
      "\n",
      "Finish:\n",
      "    Variables: ['tmp']\n",
      "    Lambdas:   [0.01, 0.001]\n",
      "    One-hot encoding: True\n",
      "    Anomaly: False\n",
      "    \n",
      "    Final Val  R2: 0.7724 \n",
      "    Final Test R2: 0.8177\n",
      "    \n",
      "    Demean Val  R2: 0.0210\n",
      "    Demean Test R2: 0.4869\n",
      "    \n",
      "    Total time: 0.16 minutes\n",
      "    \n",
      "False\n",
      "\n",
      "Finish:\n",
      "    Variables: ['tmp']\n",
      "    Lambdas:   [0.01]\n",
      "    One-hot encoding: False\n",
      "    Anomaly: False\n",
      "    \n",
      "    Final Val  R2: 0.6322 \n",
      "    Final Test R2: 0.6490\n",
      "    \n",
      "    Demean Val  R2: 0.2895\n",
      "    Demean Test R2: 0.3116\n",
      "    \n",
      "    Total time: 0.02 minutes\n",
      "    \n",
      "['ndvi']\n",
      "True\n",
      "\n",
      "Finish:\n",
      "    Variables: ['ndvi']\n",
      "    Lambdas:   [0.01, 0.001]\n",
      "    One-hot encoding: True\n",
      "    Anomaly: False\n",
      "    \n",
      "    Final Val  R2: 0.7764 \n",
      "    Final Test R2: 0.8046\n",
      "    \n",
      "    Demean Val  R2: 0.0373\n",
      "    Demean Test R2: 0.3798\n",
      "    \n",
      "    Total time: 0.17 minutes\n",
      "    \n",
      "False\n",
      "\n",
      "Finish:\n",
      "    Variables: ['ndvi']\n",
      "    Lambdas:   [0.01]\n",
      "    One-hot encoding: False\n",
      "    Anomaly: False\n",
      "    \n",
      "    Final Val  R2: 0.2711 \n",
      "    Final Test R2: 0.3241\n",
      "    \n",
      "    Demean Val  R2: 0.2961\n",
      "    Demean Test R2: 0.2917\n",
      "    \n",
      "    Total time: 0.02 minutes\n",
      "    \n",
      "['pre', 'tmp']\n",
      "True\n",
      "\n",
      "Finish:\n",
      "    Variables: ['pre', 'tmp']\n",
      "    Lambdas:   [0.1, 0.01, 0.01]\n",
      "    One-hot encoding: True\n",
      "    Anomaly: False\n",
      "    \n",
      "    Final Val  R2: 0.7746 \n",
      "    Final Test R2: 0.8011\n",
      "    \n",
      "    Demean Val  R2: 0.1650\n",
      "    Demean Test R2: 0.4905\n",
      "    \n",
      "    Total time: 0.25 minutes\n",
      "    \n",
      "False\n",
      "\n",
      "Finish:\n",
      "    Variables: ['pre', 'tmp']\n",
      "    Lambdas:   [0.1, 0.01]\n",
      "    One-hot encoding: False\n",
      "    Anomaly: False\n",
      "    \n",
      "    Final Val  R2: 0.6869 \n",
      "    Final Test R2: 0.7224\n",
      "    \n",
      "    Demean Val  R2: 0.3087\n",
      "    Demean Test R2: 0.4784\n",
      "    \n",
      "    Total time: 0.04 minutes\n",
      "    \n",
      "['pre', 'ndvi']\n",
      "True\n",
      "\n",
      "Finish:\n",
      "    Variables: ['pre', 'ndvi']\n",
      "    Lambdas:   [0.01, 0.001, 0.0001]\n",
      "    One-hot encoding: True\n",
      "    Anomaly: False\n",
      "    \n",
      "    Final Val  R2: 0.7863 \n",
      "    Final Test R2: 0.8003\n",
      "    \n",
      "    Demean Val  R2: 0.1109\n",
      "    Demean Test R2: 0.4108\n",
      "    \n",
      "    Total time: 0.25 minutes\n",
      "    \n",
      "False\n",
      "\n",
      "Finish:\n",
      "    Variables: ['pre', 'ndvi']\n",
      "    Lambdas:   [0.01, 0.001]\n",
      "    One-hot encoding: False\n",
      "    Anomaly: False\n",
      "    \n",
      "    Final Val  R2: 0.4838 \n",
      "    Final Test R2: 0.4915\n",
      "    \n",
      "    Demean Val  R2: 0.1050\n",
      "    Demean Test R2: 0.1027\n",
      "    \n",
      "    Total time: 0.04 minutes\n",
      "    \n",
      "['tmp', 'ndvi']\n",
      "True\n",
      "\n",
      "Finish:\n",
      "    Variables: ['tmp', 'ndvi']\n",
      "    Lambdas:   [0.01, 0.001, 0.0001]\n",
      "    One-hot encoding: True\n",
      "    Anomaly: False\n",
      "    \n",
      "    Final Val  R2: 0.8134 \n",
      "    Final Test R2: 0.8385\n",
      "    \n",
      "    Demean Val  R2: 0.2396\n",
      "    Demean Test R2: 0.5628\n",
      "    \n",
      "    Total time: 0.26 minutes\n",
      "    \n",
      "False\n",
      "\n",
      "Finish:\n",
      "    Variables: ['tmp', 'ndvi']\n",
      "    Lambdas:   [0.01, 0.001]\n",
      "    One-hot encoding: False\n",
      "    Anomaly: False\n",
      "    \n",
      "    Final Val  R2: 0.6723 \n",
      "    Final Test R2: 0.7019\n",
      "    \n",
      "    Demean Val  R2: 0.3730\n",
      "    Demean Test R2: 0.4782\n",
      "    \n",
      "    Total time: 0.04 minutes\n",
      "    \n",
      "['pre', 'tmp', 'ndvi']\n",
      "True\n",
      "\n",
      "Finish:\n",
      "    Variables: ['pre', 'tmp', 'ndvi']\n",
      "    Lambdas:   [0.01, 0.001, 0.1, 1e-05]\n",
      "    One-hot encoding: True\n",
      "    Anomaly: False\n",
      "    \n",
      "    Final Val  R2: 0.8008 \n",
      "    Final Test R2: 0.8356\n",
      "    \n",
      "    Demean Val  R2: 0.1744\n",
      "    Demean Test R2: 0.6005\n",
      "    \n",
      "    Total time: 0.36 minutes\n",
      "    \n",
      "False\n",
      "\n",
      "Finish:\n",
      "    Variables: ['pre', 'tmp', 'ndvi']\n",
      "    Lambdas:   [0.01, 0.001, 0.1]\n",
      "    One-hot encoding: False\n",
      "    Anomaly: False\n",
      "    \n",
      "    Final Val  R2: 0.7006 \n",
      "    Final Test R2: 0.7515\n",
      "    \n",
      "    Demean Val  R2: 0.2988\n",
      "    Demean Test R2: 0.5570\n",
      "    \n",
      "    Total time: 0.08 minutes\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for ls in clim:\n",
    "    print(ls)\n",
    "    for he in HE:\n",
    "        print(he)\n",
    "        out = climate_model(\n",
    "            variable_groups=ls,\n",
    "            hot_encode=he,\n",
    "            index_cols=[\"year\", \"district\", \"yield_mt\"],\n",
    "        )\n",
    "        output.append(out)\n",
    "results = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41c7157f-9143-4402-bae6-ee4ed7b38381",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variables</th>\n",
       "      <th>year_start</th>\n",
       "      <th>hot_encode</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>total_n</th>\n",
       "      <th>train_n</th>\n",
       "      <th>test_n</th>\n",
       "      <th>best_reg_param</th>\n",
       "      <th>mean_of_val_R2</th>\n",
       "      <th>val_R2</th>\n",
       "      <th>...</th>\n",
       "      <th>val_r2</th>\n",
       "      <th>train_R2</th>\n",
       "      <th>train_r</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>test_R2</th>\n",
       "      <th>test_r</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>demean_cv_R2</th>\n",
       "      <th>demean_cv_r</th>\n",
       "      <th>demean_cv_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[pre]</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001]]</td>\n",
       "      <td>[[0.3250009242275994, 0.6005759481906308]]</td>\n",
       "      <td>0.622504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631423</td>\n",
       "      <td>0.802148</td>\n",
       "      <td>0.896928</td>\n",
       "      <td>0.804480</td>\n",
       "      <td>0.694306</td>\n",
       "      <td>0.834720</td>\n",
       "      <td>0.696757</td>\n",
       "      <td>-0.627066</td>\n",
       "      <td>-0.371544</td>\n",
       "      <td>0.138045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[pre]</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01]]</td>\n",
       "      <td>[[0.31781598189700205]]</td>\n",
       "      <td>0.343397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345237</td>\n",
       "      <td>0.394337</td>\n",
       "      <td>0.628025</td>\n",
       "      <td>0.394416</td>\n",
       "      <td>0.377233</td>\n",
       "      <td>0.614321</td>\n",
       "      <td>0.377391</td>\n",
       "      <td>-0.329451</td>\n",
       "      <td>-0.137914</td>\n",
       "      <td>0.019020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tmp]</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001]]</td>\n",
       "      <td>[[0.6192899462388841, 0.7594750722874657]]</td>\n",
       "      <td>0.772396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.773940</td>\n",
       "      <td>0.873456</td>\n",
       "      <td>0.935209</td>\n",
       "      <td>0.874615</td>\n",
       "      <td>0.817738</td>\n",
       "      <td>0.905197</td>\n",
       "      <td>0.819381</td>\n",
       "      <td>0.021033</td>\n",
       "      <td>0.323420</td>\n",
       "      <td>0.104600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[tmp]</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01]]</td>\n",
       "      <td>[[0.6151863495642378]]</td>\n",
       "      <td>0.632216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.632265</td>\n",
       "      <td>0.657021</td>\n",
       "      <td>0.810916</td>\n",
       "      <td>0.657585</td>\n",
       "      <td>0.649020</td>\n",
       "      <td>0.808263</td>\n",
       "      <td>0.653289</td>\n",
       "      <td>0.289468</td>\n",
       "      <td>0.540687</td>\n",
       "      <td>0.292342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ndvi]</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001]]</td>\n",
       "      <td>[[0.25744743523510616, 0.7632153019026009]]</td>\n",
       "      <td>0.776423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.776574</td>\n",
       "      <td>0.875685</td>\n",
       "      <td>0.937197</td>\n",
       "      <td>0.878338</td>\n",
       "      <td>0.804603</td>\n",
       "      <td>0.898677</td>\n",
       "      <td>0.807620</td>\n",
       "      <td>0.037282</td>\n",
       "      <td>0.303158</td>\n",
       "      <td>0.091905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[ndvi]</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01]]</td>\n",
       "      <td>[[0.244861108267257]]</td>\n",
       "      <td>0.271104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272700</td>\n",
       "      <td>0.317410</td>\n",
       "      <td>0.563545</td>\n",
       "      <td>0.317583</td>\n",
       "      <td>0.324052</td>\n",
       "      <td>0.588325</td>\n",
       "      <td>0.346126</td>\n",
       "      <td>0.296150</td>\n",
       "      <td>0.557405</td>\n",
       "      <td>0.310700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[pre, tmp]</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.1, 0.01, 0.01]]</td>\n",
       "      <td>[[0.5052548383406703, 0.6738933818363734, 0.76...</td>\n",
       "      <td>0.774578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.775854</td>\n",
       "      <td>0.845276</td>\n",
       "      <td>0.922097</td>\n",
       "      <td>0.850264</td>\n",
       "      <td>0.801093</td>\n",
       "      <td>0.898263</td>\n",
       "      <td>0.806877</td>\n",
       "      <td>0.165001</td>\n",
       "      <td>0.456145</td>\n",
       "      <td>0.208068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[pre, tmp]</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.1, 0.01]]</td>\n",
       "      <td>[[0.5009054966917378, 0.6709133958104069]]</td>\n",
       "      <td>0.686913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687049</td>\n",
       "      <td>0.724484</td>\n",
       "      <td>0.851868</td>\n",
       "      <td>0.725679</td>\n",
       "      <td>0.722417</td>\n",
       "      <td>0.853195</td>\n",
       "      <td>0.727942</td>\n",
       "      <td>0.308743</td>\n",
       "      <td>0.570704</td>\n",
       "      <td>0.325703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[pre, ndvi]</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001, 0.0001]]</td>\n",
       "      <td>[[0.40933173229183684, 0.4629392954787267, 0.7...</td>\n",
       "      <td>0.786272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.794674</td>\n",
       "      <td>0.903675</td>\n",
       "      <td>0.950671</td>\n",
       "      <td>0.903776</td>\n",
       "      <td>0.800254</td>\n",
       "      <td>0.895200</td>\n",
       "      <td>0.801383</td>\n",
       "      <td>0.110937</td>\n",
       "      <td>0.437815</td>\n",
       "      <td>0.191682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[pre, ndvi]</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001]]</td>\n",
       "      <td>[[0.40261554193944293, 0.4559516999439376]]</td>\n",
       "      <td>0.483837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487112</td>\n",
       "      <td>0.562675</td>\n",
       "      <td>0.750238</td>\n",
       "      <td>0.562856</td>\n",
       "      <td>0.491529</td>\n",
       "      <td>0.707057</td>\n",
       "      <td>0.499930</td>\n",
       "      <td>0.105026</td>\n",
       "      <td>0.418560</td>\n",
       "      <td>0.175193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[tmp, ndvi]</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001, 0.0001]]</td>\n",
       "      <td>[[0.6500788785199176, 0.6599605512837583, 0.80...</td>\n",
       "      <td>0.813376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.818970</td>\n",
       "      <td>0.913923</td>\n",
       "      <td>0.956047</td>\n",
       "      <td>0.914026</td>\n",
       "      <td>0.838477</td>\n",
       "      <td>0.917663</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.239563</td>\n",
       "      <td>0.526465</td>\n",
       "      <td>0.277165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[tmp, ndvi]</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001]]</td>\n",
       "      <td>[[0.6462135434654712, 0.6561010687076301]]</td>\n",
       "      <td>0.672292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.672769</td>\n",
       "      <td>0.721118</td>\n",
       "      <td>0.849440</td>\n",
       "      <td>0.721548</td>\n",
       "      <td>0.701869</td>\n",
       "      <td>0.843120</td>\n",
       "      <td>0.710851</td>\n",
       "      <td>0.373019</td>\n",
       "      <td>0.623212</td>\n",
       "      <td>0.388393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[pre, tmp, ndvi]</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001, 0.1, 1e-05]]</td>\n",
       "      <td>[[0.5526166611355222, 0.6862443448242248, 0.68...</td>\n",
       "      <td>0.800824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807923</td>\n",
       "      <td>0.909641</td>\n",
       "      <td>0.953785</td>\n",
       "      <td>0.909705</td>\n",
       "      <td>0.835635</td>\n",
       "      <td>0.914509</td>\n",
       "      <td>0.836327</td>\n",
       "      <td>0.174393</td>\n",
       "      <td>0.482337</td>\n",
       "      <td>0.232649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[pre, tmp, ndvi]</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[[0.01, 0.001, 0.1]]</td>\n",
       "      <td>[[0.5488000942253141, 0.6834571163021461, 0.68...</td>\n",
       "      <td>0.700604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701381</td>\n",
       "      <td>0.757647</td>\n",
       "      <td>0.870481</td>\n",
       "      <td>0.757737</td>\n",
       "      <td>0.751504</td>\n",
       "      <td>0.871583</td>\n",
       "      <td>0.759657</td>\n",
       "      <td>0.298824</td>\n",
       "      <td>0.585084</td>\n",
       "      <td>0.342323</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           variables  year_start  hot_encode  anomaly  total_n  train_n  \\\n",
       "0              [pre]        2016        True    False      432      345   \n",
       "1              [pre]        2016       False    False      432      345   \n",
       "2              [tmp]        2016        True    False      432      345   \n",
       "3              [tmp]        2016       False    False      432      345   \n",
       "4             [ndvi]        2016        True    False      432      345   \n",
       "5             [ndvi]        2016       False    False      432      345   \n",
       "6         [pre, tmp]        2016        True    False      432      345   \n",
       "7         [pre, tmp]        2016       False    False      432      345   \n",
       "8        [pre, ndvi]        2016        True    False      432      345   \n",
       "9        [pre, ndvi]        2016       False    False      432      345   \n",
       "10       [tmp, ndvi]        2016        True    False      432      345   \n",
       "11       [tmp, ndvi]        2016       False    False      432      345   \n",
       "12  [pre, tmp, ndvi]        2016        True    False      432      345   \n",
       "13  [pre, tmp, ndvi]        2016       False    False      432      345   \n",
       "\n",
       "    test_n               best_reg_param  \\\n",
       "0       87              [[0.01, 0.001]]   \n",
       "1       87                     [[0.01]]   \n",
       "2       87              [[0.01, 0.001]]   \n",
       "3       87                     [[0.01]]   \n",
       "4       87              [[0.01, 0.001]]   \n",
       "5       87                     [[0.01]]   \n",
       "6       87          [[0.1, 0.01, 0.01]]   \n",
       "7       87                [[0.1, 0.01]]   \n",
       "8       87      [[0.01, 0.001, 0.0001]]   \n",
       "9       87              [[0.01, 0.001]]   \n",
       "10      87      [[0.01, 0.001, 0.0001]]   \n",
       "11      87              [[0.01, 0.001]]   \n",
       "12      87  [[0.01, 0.001, 0.1, 1e-05]]   \n",
       "13      87         [[0.01, 0.001, 0.1]]   \n",
       "\n",
       "                                       mean_of_val_R2    val_R2  ...  \\\n",
       "0          [[0.3250009242275994, 0.6005759481906308]]  0.622504  ...   \n",
       "1                             [[0.31781598189700205]]  0.343397  ...   \n",
       "2          [[0.6192899462388841, 0.7594750722874657]]  0.772396  ...   \n",
       "3                              [[0.6151863495642378]]  0.632216  ...   \n",
       "4         [[0.25744743523510616, 0.7632153019026009]]  0.776423  ...   \n",
       "5                               [[0.244861108267257]]  0.271104  ...   \n",
       "6   [[0.5052548383406703, 0.6738933818363734, 0.76...  0.774578  ...   \n",
       "7          [[0.5009054966917378, 0.6709133958104069]]  0.686913  ...   \n",
       "8   [[0.40933173229183684, 0.4629392954787267, 0.7...  0.786272  ...   \n",
       "9         [[0.40261554193944293, 0.4559516999439376]]  0.483837  ...   \n",
       "10  [[0.6500788785199176, 0.6599605512837583, 0.80...  0.813376  ...   \n",
       "11         [[0.6462135434654712, 0.6561010687076301]]  0.672292  ...   \n",
       "12  [[0.5526166611355222, 0.6862443448242248, 0.68...  0.800824  ...   \n",
       "13  [[0.5488000942253141, 0.6834571163021461, 0.68...  0.700604  ...   \n",
       "\n",
       "      val_r2  train_R2   train_r  train_r2   test_R2    test_r   test_r2  \\\n",
       "0   0.631423  0.802148  0.896928  0.804480  0.694306  0.834720  0.696757   \n",
       "1   0.345237  0.394337  0.628025  0.394416  0.377233  0.614321  0.377391   \n",
       "2   0.773940  0.873456  0.935209  0.874615  0.817738  0.905197  0.819381   \n",
       "3   0.632265  0.657021  0.810916  0.657585  0.649020  0.808263  0.653289   \n",
       "4   0.776574  0.875685  0.937197  0.878338  0.804603  0.898677  0.807620   \n",
       "5   0.272700  0.317410  0.563545  0.317583  0.324052  0.588325  0.346126   \n",
       "6   0.775854  0.845276  0.922097  0.850264  0.801093  0.898263  0.806877   \n",
       "7   0.687049  0.724484  0.851868  0.725679  0.722417  0.853195  0.727942   \n",
       "8   0.794674  0.903675  0.950671  0.903776  0.800254  0.895200  0.801383   \n",
       "9   0.487112  0.562675  0.750238  0.562856  0.491529  0.707057  0.499930   \n",
       "10  0.818970  0.913923  0.956047  0.914026  0.838477  0.917663  0.842105   \n",
       "11  0.672769  0.721118  0.849440  0.721548  0.701869  0.843120  0.710851   \n",
       "12  0.807923  0.909641  0.953785  0.909705  0.835635  0.914509  0.836327   \n",
       "13  0.701381  0.757647  0.870481  0.757737  0.751504  0.871583  0.759657   \n",
       "\n",
       "    demean_cv_R2  demean_cv_r  demean_cv_r2  \n",
       "0      -0.627066    -0.371544      0.138045  \n",
       "1      -0.329451    -0.137914      0.019020  \n",
       "2       0.021033     0.323420      0.104600  \n",
       "3       0.289468     0.540687      0.292342  \n",
       "4       0.037282     0.303158      0.091905  \n",
       "5       0.296150     0.557405      0.310700  \n",
       "6       0.165001     0.456145      0.208068  \n",
       "7       0.308743     0.570704      0.325703  \n",
       "8       0.110937     0.437815      0.191682  \n",
       "9       0.105026     0.418560      0.175193  \n",
       "10      0.239563     0.526465      0.277165  \n",
       "11      0.373019     0.623212      0.388393  \n",
       "12      0.174393     0.482337      0.232649  \n",
       "13      0.298824     0.585084      0.342323  \n",
       "\n",
       "[14 rows x 21 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2a71549-38bc-4ac9-a766-3f3b34a20c44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results as: climate_model_2023-03-20.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "file_name = f\"climate_model_{today}.csv\"\n",
    "print(f\"Saving results as: {file_name}\\n\\n\")\n",
    "results.to_csv(here(\"data\", \"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db396f8-bdbf-4a60-a890-88b0a649a109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paramlist = (i for i in paramlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44732bb7-40f7-405e-acff-fc4bd7e3c459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def climate_model(variable_groups, he_anom):\n",
    "    print(variable_groups, flush=True)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    multiprocessing.Pool().starmap(climate_model, paramlist)\n",
    "    mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b796de9-e8d0-4f7e-9b6b-6bcccaa88890",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time    \n",
    "##### With progress bar\n",
    "workers = os.cpu_count()\n",
    "if __name__ == \"__main__\":\n",
    "    multiprocessing.Pool().starmap(climate_model, paramlist)\n",
    "    # with multiprocessing.Pool(processes=workers) as pool:\n",
    "    #     output = pool.starmap(climate_model, paramlist)\n",
    "    # output = p_tqdm.p_umap(climate_model, paramlist, num_cpus=workers)\n",
    "    # results = pd.concat(output).reset_index(drop=True)\n",
    "    # today = date.today().strftime(\"%Y-%m-%d\")\n",
    "    # file_name = f'results_{today}.csv'\n",
    "    # print(f\"Saving results as: {file_name}\\n\\n\")           \n",
    "    # results.to_csv(here(\"data\",\"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa19ec9-c7e8-4093-b549-426f060260a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df7999a-74a6-4cd8-b754-674c75ed00cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 1: 0.01\n",
      "\tVal R2 1: 0.5526\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 2: 0.001\n",
      "\tVal R2 2: 0.6862\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 3: 0.1\n",
      "\tVal R2 3: 0.6868\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 4: 1e-05\n",
      "\tVal R2 4: 0.7895\n",
      "\n",
      "\n",
      "Finish:\n",
      "    Variables: ['pre', 'tmp', 'ndvi']\n",
      "    Lambdas: [0.01, 0.001, 0.1, 1e-05]\n",
      "    One-hot encoding: True\n",
      "    Anomaly: False\n",
      "\n",
      "    Final Val  R2: 0.8008 \n",
      "    Final Test R2: 0.8356\n",
      "\n",
      "    Demean Val  R2: 0.1744\n",
      "    Demean Test R2: 0.6005\n",
      "\n",
      "    Total time: 0.30 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TESTING\n",
    "\n",
    "data = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "hot_encode = True\n",
    "anomaly = False\n",
    "variable_groups = [\"pre\", \"tmp\", \"ndvi\"]\n",
    "index_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "year_start = 2016\n",
    "n_splits = 5\n",
    "\n",
    "#########################################     READ DATA    #########################################\n",
    "data = data.dropna()\n",
    "\n",
    "keep_cols = []\n",
    "\n",
    "for var in variable_groups:\n",
    "    tmp = data.columns[data.columns.to_series().str.contains(var)].tolist()\n",
    "    keep_cols.append(tmp)\n",
    "\n",
    "keep_cols = [*index_cols, *[col for cols in keep_cols for col in cols]]\n",
    "\n",
    "data = data.loc[:, keep_cols]\n",
    "\n",
    "data = data[data.year >= year_start]\n",
    "\n",
    "crop_yield = data.copy().loc[:, tuple(index_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "########################################    STANDARDIZE FEATURES    #########################################\n",
    "data = data.set_index(index_cols)\n",
    "data_scaled = StandardScaler().fit_transform(data.values)\n",
    "data = pd.DataFrame(data_scaled, index=data.index).reset_index()\n",
    "data.columns = data.columns.astype(str)\n",
    "\n",
    "#########################################    HOT ENCODE    #########################################\n",
    "if hot_encode:\n",
    "    index_cols.remove(\"district\")\n",
    "    data = pd.get_dummies(data, columns=[\"district\"], drop_first=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = data.drop(index_cols, axis=1)\n",
    "y_all = np.log10(data.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV    #########################################\n",
    "### SETUP\n",
    "tic = time.time()\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "\n",
    "i = 0\n",
    "start = [i]\n",
    "end = [x_train.shape[1]]\n",
    "\n",
    "for var in variable_groups:\n",
    "    i += 12\n",
    "    start.append(i)\n",
    "    end.append(i)\n",
    "start.sort()\n",
    "end.sort()\n",
    "\n",
    "if not hot_encode:\n",
    "    start = start[0:-1]\n",
    "    end = end[0:-1]\n",
    "\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER(S)\n",
    "best_lambdas, best_scores, best_model = kfold_rr_multi_lambda_tuning(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    grid=alphas.get(\"alpha\"),\n",
    "    n_splits=n_splits,\n",
    "    start=start,\n",
    "    end=end,\n",
    "    static_lam=1,\n",
    "    verbose=2,\n",
    "    show_linalg_warning=False,\n",
    "    fit_model_after_tuning=True,\n",
    ")\n",
    "### PREDICT WITH BEST HYPERPARAMETER(S)\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################\n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"cv_prediction\"] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"] - train_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\n",
    "    \"cv_prediction\"\n",
    "] - train_split.groupby(\"district\")[\"cv_prediction\"].transform(\"mean\")\n",
    "\n",
    "test_split = pd.DataFrame(\n",
    "    np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"prediction\"].transform(\"mean\")\n",
    "\n",
    "# variable_groups.append(\"districts\")\n",
    "# group_lambdas = dict(zip(variable_groups, best_lambdas))\n",
    "# group_lambdas\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Finish:\n",
    "    Variables: {variable_groups}\n",
    "    Lambdas: {best_lambdas}\n",
    "    One-hot encoding: {hot_encode}\n",
    "    Anomaly: {anomaly}\n",
    "\n",
    "    Final Val  R2: {r2_score(y_train, val_predictions):0.4f} \n",
    "    Final Test R2: {r2_score(y_test, test_predictions):0.4f}\n",
    "\n",
    "    Demean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.4f}\n",
    "    Demean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.4f}\n",
    "\n",
    "    Total time: {(time.time()-tic)/60:0.2f} minutes\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffc9fc-32ea-47c7-93fc-0d0aea13d688",
   "metadata": {},
   "source": [
    "# NDVI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50874f01-370e-40eb-84c1-4f98234d2632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.7802 \n",
      "Test R2: 0.8085 \n",
      "\n",
      "Demean Val  R2: 0.0921 \n",
      "Demean Test R2: 0.4394\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains(\"ndvi\")]\n",
    "keep_cols = [*ndvi_cols, *drop_cols]\n",
    "climate_df = climate_df.loc[:, keep_cols]\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "hot_encode = True\n",
    "# hot_encode = False\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    HOT ENCODE    #########################################\n",
    "if hot_encode:\n",
    "    drop_cols.remove(\"district\")\n",
    "    climate_df = pd.get_dummies(climate_df, columns=[\"district\"], drop_first=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "climate_df = climate_df.set_index(drop_cols)\n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis=1)\n",
    "y_all = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring=\"r2\", cv=kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################\n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"cv_prediction\"] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"] - train_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\n",
    "    \"cv_prediction\"\n",
    "] - train_split.groupby(\"district\")[\"cv_prediction\"].transform(\"mean\")\n",
    "\n",
    "test_split = pd.DataFrame(\n",
    "    np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"prediction\"].transform(\"mean\")\n",
    "\n",
    "print(\n",
    "    f\"Val  R2: {r2_score(y_train, val_predictions):0.4f}\",\n",
    "    f\"\\nTest R2: {r2_score(y_test, test_predictions):0.4f}\",\n",
    "    f\"\\n\\nDemean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.4f}\",\n",
    "    f\"\\nDemean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.4f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3eef3-18bb-4b2b-8da8-02bc0230bda2",
   "metadata": {},
   "source": [
    "# Precipitation, Temperature, and NDVI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1840a92e-3eca-4039-a5d4-6a8367316314",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.8044 \n",
      "Test R2: 0.8297 \n",
      "\n",
      "Demean Val  R2: 0.1723 \n",
      "Demean Test R2: 0.5297\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "hot_encode = True\n",
    "# hot_encode = False\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    HOT ENCODE    #########################################\n",
    "if hot_encode:\n",
    "    drop_cols.remove(\"district\")\n",
    "    climate_df = pd.get_dummies(climate_df, columns=[\"district\"], drop_first=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "climate_df = climate_df.set_index(drop_cols)\n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis=1)\n",
    "y_all = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring=\"r2\", cv=kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################\n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"cv_prediction\"] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"] - train_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\n",
    "    \"cv_prediction\"\n",
    "] - train_split.groupby(\"district\")[\"cv_prediction\"].transform(\"mean\")\n",
    "\n",
    "test_split = pd.DataFrame(\n",
    "    np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"prediction\"].transform(\"mean\")\n",
    "\n",
    "print(\n",
    "    f\"Val  R2: {r2_score(y_train, val_predictions):0.4f}\",\n",
    "    f\"\\nTest R2: {r2_score(y_test, test_predictions):0.4f}\",\n",
    "    f\"\\n\\nDemean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.4f}\",\n",
    "    f\"\\nDemean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.4f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b577bc71-d766-42fe-adad-93286a5a19ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.792567922097443"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_reg.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3dfe35-025c-4b06-8b2b-23b4f223f11b",
   "metadata": {},
   "source": [
    "# NDVI Anomaly Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f20bb178-7708-45e3-aa68-124c88541b05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.4449\n",
      "Test R2: 0.4293\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains(\"ndvi\")]\n",
    "keep_cols = [*ndvi_cols, *drop_cols]\n",
    "climate_df = climate_df.loc[:, keep_cols]\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "climate_df = climate_df.set_index(drop_cols)\n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################\n",
    "climate_df[\"yield_mt\"] = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "climate_df.set_index([\"year\", \"district\"], inplace=True)\n",
    "var_cols = climate_df.columns\n",
    "climate_df = climate_df[var_cols] - climate_df.groupby([\"district\"], as_index=True)[\n",
    "    var_cols\n",
    "].transform(\"mean\")\n",
    "climate_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis=1)\n",
    "y_all = climate_df.yield_mt\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring=\"r2\", cv=kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################\n",
    "crop_yield[\"prediction\"] = best_model.predict(x_all)\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"cv_prediction\"] = val_predictions\n",
    "\n",
    "test_split = pd.DataFrame(\n",
    "    np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "print(\n",
    "    f\"Val  R2: {r2_score(y_train, val_predictions):0.4f}\\nTest R2: {r2_score(y_test, test_predictions):0.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c574bb5-882f-4588-ae97-44af906398a6",
   "metadata": {},
   "source": [
    "# Precipitation, Temperature, and NDVI  Anomaly model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "102e934f-c6e8-4222-9956-a621dda75298",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.5284\n",
      "Test R2: 0.5145\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "climate_df = climate_df.set_index(drop_cols)\n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################\n",
    "climate_df[\"yield_mt\"] = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "climate_df.set_index([\"year\", \"district\"], inplace=True)\n",
    "var_cols = climate_df.columns\n",
    "climate_df = climate_df[var_cols] - climate_df.groupby([\"district\"], as_index=True)[\n",
    "    var_cols\n",
    "].transform(\"mean\")\n",
    "climate_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis=1)\n",
    "y_all = climate_df.yield_mt\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring=\"r2\", cv=kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################\n",
    "crop_yield[\"prediction\"] = best_model.predict(x_all)\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"cv_prediction\"] = val_predictions\n",
    "\n",
    "test_split = pd.DataFrame(\n",
    "    np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "print(\n",
    "    f\"Val  R2: {r2_score(y_train, val_predictions):0.4f}\\nTest R2: {r2_score(y_test, test_predictions):0.4f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
