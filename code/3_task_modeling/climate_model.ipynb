{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fec0a85-fded-480e-bc42-c258cc39c736",
   "metadata": {},
   "source": [
    "# Benchmark Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0585a80-f431-4f2b-bcd1-396386652fab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "502744d3-ce9a-42bc-ac76-a5feae8bee55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pyarrow\n",
    "import concurrent.futures\n",
    "\n",
    "from pyhere import here\n",
    "from datetime import date\n",
    "from itertools import product, combinations\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    GridSearchCV,\n",
    "    cross_val_predict,\n",
    ")\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr, pearsonr, t\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "\n",
    "from task_modeling_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeecf0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable_groups = [\"ndvi\"]\n",
    "# hot_encode = False\n",
    "# anomaly = True\n",
    "# index_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "# year_start = 2016\n",
    "# n_splits = 5\n",
    "# split = 0\n",
    "# random_state = 42\n",
    "# return_oos_predictions = True\n",
    "\n",
    "\n",
    "# variable_groups_str = \"_\".join(variable_groups)\n",
    "\n",
    "# #########################################     READ DATA    #########################################\n",
    "# data = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "# data = data.dropna()\n",
    "\n",
    "# #########################################     FILTER DATA    #########################################\n",
    "# keep_cols = []\n",
    "\n",
    "# for var in variable_groups:\n",
    "#     tmp = data.columns[data.columns.to_series().str.contains(var)].tolist()\n",
    "#     keep_cols.append(tmp)\n",
    "\n",
    "# keep_cols = [*index_cols, *[col for cols in keep_cols for col in cols]]\n",
    "# data = data.loc[:, keep_cols]\n",
    "# data = data[data.year >= year_start]\n",
    "\n",
    "# data[\"log_yield\"] = np.log10(data[\"yield_mt\"] + 1)\n",
    "\n",
    "# data[\"demean_log_yield\"] = data.log_yield - data.groupby(\n",
    "#     \"district\"\n",
    "# ).log_yield.transform(\"mean\")\n",
    "\n",
    "# index_cols.append(\"log_yield\")\n",
    "# index_cols.append(\"demean_log_yield\")\n",
    "\n",
    "# #########################################    MAKE A COPY    #########################################\n",
    "# crop_yield = data.copy().loc[:, tuple(index_cols)].reset_index(drop=True)\n",
    "\n",
    "# #########################################     CALCULATE ANOMALY   #########################################\n",
    "# if anomaly:\n",
    "#     data.set_index([\"year\", \"district\"], inplace=True)\n",
    "#     var_cols = data.columns\n",
    "#     data = data[var_cols] - data.groupby([\"district\"], as_index=True)[\n",
    "#         var_cols\n",
    "#     ].transform(\"mean\")\n",
    "# else:\n",
    "#     pass\n",
    "\n",
    "# data.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# #########################################    HOT ENCODE    #########################################\n",
    "# if hot_encode:\n",
    "#     index_cols.remove(\"district\")\n",
    "#     data = pd.get_dummies(data, columns=[\"district\"], drop_first=False)\n",
    "# else:\n",
    "#     pass\n",
    "\n",
    "# #########################################     K-FOLD SPLIT    #########################################\n",
    "# x_all = data.drop(index_cols, axis=1)\n",
    "# if anomaly:\n",
    "#     y_all = data.demean_log_yield\n",
    "# else:\n",
    "#     y_all = data.log_yield\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#     x_all, y_all, test_size=0.2, random_state=random_state\n",
    "# )\n",
    "# kfold = KFold(n_splits=n_splits)\n",
    "\n",
    "# #########################################    STANDARDIZE FEATURES    #########################################\n",
    "# scaler = StandardScaler().fit(x_all)\n",
    "# x_train = pd.DataFrame(\n",
    "#     scaler.transform(x_train), columns=x_train.columns, index=x_train.index\n",
    "# )\n",
    "# x_test = pd.DataFrame(\n",
    "#     scaler.transform(x_test), columns=x_test.columns, index=x_test.index\n",
    "# )\n",
    "\n",
    "# #########################################     K-FOLD CV    #########################################\n",
    "# ### SETUP\n",
    "# tic = time.time()\n",
    "# alphas = {\"alpha\": np.logspace(-1, 1, base=10, num=3)}\n",
    "\n",
    "# ### LAMBDA INDICIES\n",
    "# i = 0\n",
    "# start = [i]\n",
    "# end = [x_train.shape[1]]\n",
    "\n",
    "# for var in variable_groups:\n",
    "#     i += 12\n",
    "#     start.append(i)\n",
    "#     end.append(i)\n",
    "# start.sort()\n",
    "# end.sort()\n",
    "\n",
    "# if not hot_encode:\n",
    "#     start = start[0:-1]\n",
    "#     end = end[0:-1]\n",
    "\n",
    "# ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER(S)\n",
    "# best_lambdas, best_scores, best_model = kfold_rr_multi_lambda_tuning(\n",
    "#     X=x_train,\n",
    "#     y=y_train,\n",
    "#     grid=alphas.get(\"alpha\"),\n",
    "#     n_splits=n_splits,\n",
    "#     start=start,\n",
    "#     end=end,\n",
    "#     static_lam=1,\n",
    "#     verbose=0,\n",
    "#     show_linalg_warning=False,\n",
    "#     fit_model_after_tuning=True,\n",
    "# )\n",
    "# ### PREDICT WITH BEST HYPERPARAMETER(S)\n",
    "# val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "# train_predictions = best_model.predict(x_train)\n",
    "# test_predictions = best_model.predict(x_test)\n",
    "\n",
    "# if anomaly:\n",
    "#     pass\n",
    "# else:\n",
    "#     val_predictions = np.maximum(val_predictions, 0)\n",
    "#     train_predictions = np.maximum(train_predictions, 0)\n",
    "#     test_predictions = np.maximum(test_predictions, 0)\n",
    "\n",
    "# #########################################     DE-MEAN TRAIN R2    #########################################\n",
    "# fold_list = []\n",
    "# for i in range(n_splits):\n",
    "#     idx = len(list(kfold.split(y_train))[i][1])\n",
    "#     fold = np.repeat(i + 1, idx).tolist()\n",
    "#     fold_list.append(fold)\n",
    "# fold_list = [item for sublist in fold_list for item in sublist]\n",
    "\n",
    "# train_split = pd.DataFrame(\n",
    "#     np.repeat(\"train\", len(x_train)), columns=[\"data_fold\"], index=x_train.index\n",
    "# )\n",
    "# train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "# train_split[\"oos_prediction\"] = val_predictions\n",
    "# train_split[\"val_fold\"] = fold_list\n",
    "\n",
    "# #########################################     DE-MEAN TEST R2    #########################################\n",
    "# test_split = pd.DataFrame(\n",
    "#     {\"data_fold\": np.repeat(\"test\", len(x_test))}, index=x_test.index\n",
    "# )\n",
    "# test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "# test_split[\"oos_prediction\"] = test_predictions\n",
    "# test_split[\"val_fold\"] = n_splits + 1\n",
    "\n",
    "# #########################################     OUT OF SAMPLE PREDICTIONS    #########################################\n",
    "# oos_preds = pd.concat([train_split, test_split])\n",
    "# oos_preds[[\"split\", \"random_state\"]] = split, random_state\n",
    "# oos_preds[\"variables\"] = variable_groups_str\n",
    "# oos_preds[\"anomaly\"] = anomaly\n",
    "# oos_preds[\"hot_encode\"] = hot_encode\n",
    "# oos_preds[\"year_start\"] = year_start\n",
    "# oos_preds[\"demean_oos_prediction\"] = oos_preds.oos_prediction - oos_preds.groupby(\n",
    "#     \"district\"\n",
    "# ).oos_prediction.transform(\"mean\")\n",
    "\n",
    "# #########################################     SCORES    #########################################\n",
    "# val_R2 = r2_score(y_train, val_predictions)\n",
    "# val_r = pearsonr(val_predictions, y_train)[0]\n",
    "# train_R2 = r2_score(y_train, train_predictions)\n",
    "# train_r = pearsonr(train_predictions, y_train)[0]\n",
    "# test_R2 = r2_score(y_test, test_predictions)\n",
    "# test_r = pearsonr(test_predictions, y_test)[0]\n",
    "\n",
    "# if anomaly:\n",
    "#     demean_cv_R2 = np.nan\n",
    "#     demean_cv_r = np.nan\n",
    "#     demean_test_R2 = np.nan\n",
    "#     demean_test_r = np.nan\n",
    "# else:\n",
    "#     test = oos_preds[oos_preds.data_fold == \"test\"]\n",
    "#     train = oos_preds[oos_preds.data_fold == \"train\"]\n",
    "#     demean_cv_R2 = r2_score(train.demean_log_yield, train.demean_oos_prediction)\n",
    "#     demean_cv_r = pearsonr(train.demean_log_yield, train.demean_oos_prediction)[0]\n",
    "#     demean_test_R2 = r2_score(test.demean_log_yield, test.demean_oos_prediction)\n",
    "#     demean_test_r = pearsonr(test.demean_log_yield, test.demean_oos_prediction)[0]\n",
    "\n",
    "# d = {\n",
    "#     \"split\": split,\n",
    "#     \"random_state\": random_state,\n",
    "#     \"variables\": \"_\".join(variable_groups),\n",
    "#     \"year_start\": year_start,\n",
    "#     \"hot_encode\": hot_encode,\n",
    "#     \"anomaly\": anomaly,\n",
    "#     \"total_n\": len(x_all),\n",
    "#     \"train_n\": len(x_train),\n",
    "#     \"test_n\": len(x_test),\n",
    "#     \"best_reg_param\": best_lambdas,\n",
    "#     \"mean_of_val_R2\": best_scores,\n",
    "#     \"val_R2\": val_R2,\n",
    "#     \"val_r\": val_r,\n",
    "#     \"val_r2\": val_r**2,\n",
    "#     \"train_R2\": train_R2,\n",
    "#     \"train_r\": train_r,\n",
    "#     \"train_r2\": train_r**2,\n",
    "#     \"test_R2\": test_R2,\n",
    "#     \"test_r\": test_r,\n",
    "#     \"test_r2\": test_r**2,\n",
    "#     \"demean_cv_R2\": demean_cv_R2,\n",
    "#     \"demean_cv_r\": demean_cv_r,\n",
    "#     \"demean_cv_r2\": demean_cv_r**2,\n",
    "#     \"demean_test_R2\": demean_test_R2,\n",
    "#     \"demean_test_r\": demean_test_r,\n",
    "#     \"demean_test_r2\": demean_test_r**2,\n",
    "# }\n",
    "# # if return_oos_predictions:\n",
    "# #     return d, oos_preds\n",
    "# # else:\n",
    "# #     return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37710abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355a715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2216f1bb-8f61-417f-bbfc-6bba313014c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pre'],\n",
       " ['tmp'],\n",
       " ['ndvi'],\n",
       " ['pre', 'tmp'],\n",
       " ['pre', 'ndvi'],\n",
       " ['tmp', 'ndvi'],\n",
       " ['pre', 'tmp', 'ndvi']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables = [\"pre\", \"tmp\", \"ndvi\"]\n",
    "climate_vars = list(combinations(variables, 2))\n",
    "climate_vars = (\n",
    "    climate_vars + [[variables[i]] for i in range(len(variables))] + [variables]\n",
    ")\n",
    "climate_vars = [list(elem) for elem in climate_vars]\n",
    "climate_vars.sort(key=len)\n",
    "climate_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acc679e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "n_splits = 10\n",
    "random.seed(42)\n",
    "random_seeds = [random.randint(0, 1_000_000) for _ in range(n_splits)]\n",
    "\n",
    "he_anom_combinations = [(True, False), (False, False), (False, True)]\n",
    "\n",
    "kwarg_list = [\n",
    "    {\n",
    "        \"variable_groups\": clim,\n",
    "        \"hot_encode\": he,\n",
    "        \"anomaly\": anom,\n",
    "        \"index_cols\": [\"year\", \"district\", \"yield_mt\"],\n",
    "        \"year_start\": 2016,\n",
    "        \"n_splits\": 5,\n",
    "        \"split\": split,\n",
    "        \"random_state\": random_state,\n",
    "        \"return_oos_predictions\": True,\n",
    "    }\n",
    "    for clim in climate_vars\n",
    "    for he, anom in he_anom_combinations\n",
    "    for split, random_state in enumerate(random_seeds)\n",
    "]\n",
    "\n",
    "len(kwarg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfea5468-aefd-4b02-be4a-f405eee83c22",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 210/210 [14:13<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results as: climate_model_10-splits_2023-07-05.csv\n",
      "\n",
      "\n",
      "Saving results as: climate_model_oos_predictions_10-splits_2023-07-05.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    output, oos_preds = [], []\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        # with ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(climate_model, **kwargs): kwargs for kwargs in kwarg_list\n",
    "        }\n",
    "        for future in tqdm(\n",
    "            as_completed(futures), total=len(futures), desc=\"Processing models\"\n",
    "        ):\n",
    "            out, oos = future.result()\n",
    "            output.append(out)\n",
    "            oos_preds.append(oos)\n",
    "\n",
    "    today = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    results = pd.DataFrame(output)\n",
    "    results_fn = f\"climate_model_{n_splits}-splits_{today}.csv\"\n",
    "    print(f\"Saving results as: {results_fn}\\n\\n\")\n",
    "    results.to_csv(here(\"data\", \"results\", results_fn), index=False)\n",
    "\n",
    "    oos_predictions = pd.concat(oos_preds)\n",
    "    oos_fn = f\"climate_model_oos_predictions_{n_splits}-splits_{today}.csv\"\n",
    "    print(f\"Saving results as: {oos_fn}\\n\\n\")\n",
    "    oos_predictions.to_csv(here(\"data\", \"results\", oos_fn), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e58286a9-9383-4e70-a4e8-5a84628636f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>random_state</th>\n",
       "      <th>variables</th>\n",
       "      <th>year_start</th>\n",
       "      <th>hot_encode</th>\n",
       "      <th>anomaly</th>\n",
       "      <th>total_n</th>\n",
       "      <th>train_n</th>\n",
       "      <th>test_n</th>\n",
       "      <th>best_reg_param</th>\n",
       "      <th>...</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>test_R2</th>\n",
       "      <th>test_r</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>demean_cv_R2</th>\n",
       "      <th>demean_cv_r</th>\n",
       "      <th>demean_cv_r2</th>\n",
       "      <th>demean_test_R2</th>\n",
       "      <th>demean_test_r</th>\n",
       "      <th>demean_test_r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>26225</td>\n",
       "      <td>ndvi</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298571</td>\n",
       "      <td>0.280308</td>\n",
       "      <td>0.542938</td>\n",
       "      <td>0.294781</td>\n",
       "      <td>0.157889</td>\n",
       "      <td>0.416331</td>\n",
       "      <td>0.173331</td>\n",
       "      <td>0.111030</td>\n",
       "      <td>0.348220</td>\n",
       "      <td>0.121257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>116739</td>\n",
       "      <td>pre</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170344</td>\n",
       "      <td>0.102114</td>\n",
       "      <td>0.348568</td>\n",
       "      <td>0.121499</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>777572</td>\n",
       "      <td>pre</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[0.1]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398278</td>\n",
       "      <td>0.324432</td>\n",
       "      <td>0.575593</td>\n",
       "      <td>0.331307</td>\n",
       "      <td>-0.281685</td>\n",
       "      <td>-0.151317</td>\n",
       "      <td>0.022897</td>\n",
       "      <td>-0.380065</td>\n",
       "      <td>-0.219479</td>\n",
       "      <td>0.048171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>670487</td>\n",
       "      <td>ndvi</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[0.01]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455168</td>\n",
       "      <td>0.570649</td>\n",
       "      <td>0.756902</td>\n",
       "      <td>0.572900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>146316</td>\n",
       "      <td>pre</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[0.01]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.394274</td>\n",
       "      <td>0.368904</td>\n",
       "      <td>0.609873</td>\n",
       "      <td>0.371945</td>\n",
       "      <td>-0.354622</td>\n",
       "      <td>-0.085783</td>\n",
       "      <td>0.007359</td>\n",
       "      <td>-0.536602</td>\n",
       "      <td>-0.147994</td>\n",
       "      <td>0.021902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>9</td>\n",
       "      <td>107473</td>\n",
       "      <td>pre_tmp_ndvi</td>\n",
       "      <td>2016</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[1000000000.0, 0.1, 0.1]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553652</td>\n",
       "      <td>0.632169</td>\n",
       "      <td>0.804545</td>\n",
       "      <td>0.647292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>9</td>\n",
       "      <td>107473</td>\n",
       "      <td>pre_tmp_ndvi</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[0.01, 0.01, 0.1, 0.01]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.902052</td>\n",
       "      <td>0.848890</td>\n",
       "      <td>0.921905</td>\n",
       "      <td>0.849908</td>\n",
       "      <td>0.059943</td>\n",
       "      <td>0.409038</td>\n",
       "      <td>0.167312</td>\n",
       "      <td>0.565475</td>\n",
       "      <td>0.763606</td>\n",
       "      <td>0.583093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>8</td>\n",
       "      <td>772246</td>\n",
       "      <td>pre_tmp_ndvi</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[0.1, 0.01, 0.01, 0.01]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914142</td>\n",
       "      <td>0.839197</td>\n",
       "      <td>0.919882</td>\n",
       "      <td>0.846182</td>\n",
       "      <td>0.213436</td>\n",
       "      <td>0.510816</td>\n",
       "      <td>0.260933</td>\n",
       "      <td>0.468034</td>\n",
       "      <td>0.686457</td>\n",
       "      <td>0.471223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>5</td>\n",
       "      <td>256787</td>\n",
       "      <td>pre_tmp_ndvi</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[0.01, 0.01, 0.001, 0.01]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.914419</td>\n",
       "      <td>0.866718</td>\n",
       "      <td>0.931686</td>\n",
       "      <td>0.868038</td>\n",
       "      <td>0.255763</td>\n",
       "      <td>0.550629</td>\n",
       "      <td>0.303192</td>\n",
       "      <td>0.221413</td>\n",
       "      <td>0.530530</td>\n",
       "      <td>0.281462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>6</td>\n",
       "      <td>234053</td>\n",
       "      <td>pre_tmp_ndvi</td>\n",
       "      <td>2016</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>432</td>\n",
       "      <td>345</td>\n",
       "      <td>87</td>\n",
       "      <td>[0.1, 0.01, 0.01, 0.01]</td>\n",
       "      <td>...</td>\n",
       "      <td>0.919001</td>\n",
       "      <td>0.812632</td>\n",
       "      <td>0.910563</td>\n",
       "      <td>0.829124</td>\n",
       "      <td>0.260299</td>\n",
       "      <td>0.560618</td>\n",
       "      <td>0.314293</td>\n",
       "      <td>0.466056</td>\n",
       "      <td>0.702023</td>\n",
       "      <td>0.492836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     split  random_state     variables  year_start  hot_encode  anomaly  \\\n",
       "0        2         26225          ndvi        2016       False    False   \n",
       "1        1        116739           pre        2016       False     True   \n",
       "2        3        777572           pre        2016       False    False   \n",
       "3        0        670487          ndvi        2016       False     True   \n",
       "4        7        146316           pre        2016       False    False   \n",
       "..     ...           ...           ...         ...         ...      ...   \n",
       "205      9        107473  pre_tmp_ndvi        2016       False     True   \n",
       "206      9        107473  pre_tmp_ndvi        2016        True    False   \n",
       "207      8        772246  pre_tmp_ndvi        2016        True    False   \n",
       "208      5        256787  pre_tmp_ndvi        2016        True    False   \n",
       "209      6        234053  pre_tmp_ndvi        2016        True    False   \n",
       "\n",
       "     total_n  train_n  test_n             best_reg_param  ...  train_r2  \\\n",
       "0        432      345      87                      [1.0]  ...  0.298571   \n",
       "1        432      345      87                      [0.1]  ...  0.170344   \n",
       "2        432      345      87                      [0.1]  ...  0.398278   \n",
       "3        432      345      87                     [0.01]  ...  0.455168   \n",
       "4        432      345      87                     [0.01]  ...  0.394274   \n",
       "..       ...      ...     ...                        ...  ...       ...   \n",
       "205      432      345      87   [1000000000.0, 0.1, 0.1]  ...  0.553652   \n",
       "206      432      345      87    [0.01, 0.01, 0.1, 0.01]  ...  0.902052   \n",
       "207      432      345      87    [0.1, 0.01, 0.01, 0.01]  ...  0.914142   \n",
       "208      432      345      87  [0.01, 0.01, 0.001, 0.01]  ...  0.914419   \n",
       "209      432      345      87    [0.1, 0.01, 0.01, 0.01]  ...  0.919001   \n",
       "\n",
       "      test_R2    test_r   test_r2  demean_cv_R2  demean_cv_r  demean_cv_r2  \\\n",
       "0    0.280308  0.542938  0.294781      0.157889     0.416331      0.173331   \n",
       "1    0.102114  0.348568  0.121499           NaN          NaN           NaN   \n",
       "2    0.324432  0.575593  0.331307     -0.281685    -0.151317      0.022897   \n",
       "3    0.570649  0.756902  0.572900           NaN          NaN           NaN   \n",
       "4    0.368904  0.609873  0.371945     -0.354622    -0.085783      0.007359   \n",
       "..        ...       ...       ...           ...          ...           ...   \n",
       "205  0.632169  0.804545  0.647292           NaN          NaN           NaN   \n",
       "206  0.848890  0.921905  0.849908      0.059943     0.409038      0.167312   \n",
       "207  0.839197  0.919882  0.846182      0.213436     0.510816      0.260933   \n",
       "208  0.866718  0.931686  0.868038      0.255763     0.550629      0.303192   \n",
       "209  0.812632  0.910563  0.829124      0.260299     0.560618      0.314293   \n",
       "\n",
       "     demean_test_R2  demean_test_r  demean_test_r2  \n",
       "0          0.111030       0.348220        0.121257  \n",
       "1               NaN            NaN             NaN  \n",
       "2         -0.380065      -0.219479        0.048171  \n",
       "3               NaN            NaN             NaN  \n",
       "4         -0.536602      -0.147994        0.021902  \n",
       "..              ...            ...             ...  \n",
       "205             NaN            NaN             NaN  \n",
       "206        0.565475       0.763606        0.583093  \n",
       "207        0.468034       0.686457        0.471223  \n",
       "208        0.221413       0.530530        0.281462  \n",
       "209        0.466056       0.702023        0.492836  \n",
       "\n",
       "[210 rows x 26 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be0b60b-49c6-4a4d-bac4-ab595fe0f469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oos_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5b40a-4f5e-41ad-a969-1c1debc38afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(output)\n",
    "results\n",
    "# mask = results.anomaly == True\n",
    "# cols = [\n",
    "#     \"demean_cv_R2\",\n",
    "#     \"demean_cv_r\",\n",
    "#     \"demean_cv_r2\",\n",
    "#     \"demean_test_R2\",\n",
    "#     \"demean_test_r\",\n",
    "#     \"demean_test_r2\",\n",
    "# ]\n",
    "# results.loc[mask, cols] = np.nan\n",
    "\n",
    "# today = date.today().strftime(\"%Y-%m-%d\")\n",
    "# file_name = f\"climate_model_{num_seeds}-splits_{today}.csv\"\n",
    "# print(f\"Saving results as: {file_name}\\n\\n\")\n",
    "# results.to_csv(here(\"data\", \"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733736a5-2b9e-4d91-821c-ab7f5962ca14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = results.copy()\n",
    "a = a[a.year_start == 2016]\n",
    "a = a[a.hot_encode]\n",
    "# a = a[a.variables.isin([\"tmp_ndvi\"])]\n",
    "\n",
    "b = get_mean_std_ste(\n",
    "    df=a,\n",
    "    groupby_columns=[\"variables\", \"year_start\", \"hot_encode\", \"anomaly\"],\n",
    "    target_columns=[\"val_R2\", \"test_R2\", \"demean_cv_R2\", \"demean_cv_r2\"],\n",
    ")\n",
    "b.sort_values([\"summary_var\", \"mean\"], ascending=False)\n",
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0087e232-9f79-49ab-8f8e-b6949a9225cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def climate_model(\n",
    "variable_groups = [\"ndvi\"]\n",
    "hot_encode = True\n",
    "anomaly = False\n",
    "index_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "year_start = 2016\n",
    "n_splits = 5\n",
    "split = 0\n",
    "random_state = 42\n",
    "return_oos_predictions = True\n",
    "# ):\n",
    "if variable_groups is None:\n",
    "    variable_groups_str = \"rcf\"\n",
    "else:\n",
    "    variable_groups_str = \"_\".join(variable_groups)\n",
    "#########################################     READ DATA    #########################################\n",
    "data = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "data = data.dropna()\n",
    "\n",
    "#########################################     FILTER DATA    #########################################\n",
    "keep_cols = []\n",
    "\n",
    "for var in variable_groups:\n",
    "    tmp = data.columns[data.columns.to_series().str.contains(var)].tolist()\n",
    "    keep_cols.append(tmp)\n",
    "\n",
    "keep_cols = [*index_cols, *[col for cols in keep_cols for col in cols]]\n",
    "data = data.loc[:, keep_cols]\n",
    "data = data[data.year >= year_start]\n",
    "\n",
    "#########################################    MAKE A COPY    #########################################\n",
    "crop_yield = data.copy().loc[:, tuple(index_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################\n",
    "if anomaly:\n",
    "    data[\"yield_mt\"] = np.log10(data.yield_mt.to_numpy() + 1)\n",
    "    data.set_index([\"year\", \"district\"], inplace=True)\n",
    "    var_cols = data.columns\n",
    "    data = data[var_cols] - data.groupby([\"district\"], as_index=True)[\n",
    "        var_cols\n",
    "    ].transform(\"mean\")\n",
    "\n",
    "else:\n",
    "    pass\n",
    "data.reset_index(drop=False, inplace=True)\n",
    "\n",
    "#########################################    HOT ENCODE    #########################################\n",
    "if hot_encode and not anomaly:\n",
    "    index_cols.remove(\"district\")\n",
    "    data = pd.get_dummies(data, columns=[\"district\"], drop_first=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = data.drop(index_cols, axis=1)\n",
    "y_all = np.log10(data.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=random_state\n",
    ")\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = pd.DataFrame(\n",
    "    scaler.transform(x_train), columns=x_train.columns, index=x_train.index\n",
    ")\n",
    "x_test = pd.DataFrame(\n",
    "    scaler.transform(x_test), columns=x_test.columns, index=x_test.index\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV    #########################################\n",
    "### SETUP\n",
    "tic = time.time()\n",
    "alphas = {\"alpha\": np.logspace(-1, 1, base=10, num=3)}\n",
    "\n",
    "### LAMBDA INDICIES\n",
    "i = 0\n",
    "start = [i]\n",
    "end = [x_train.shape[1]]\n",
    "\n",
    "for var in variable_groups:\n",
    "    i += 12\n",
    "    start.append(i)\n",
    "    end.append(i)\n",
    "start.sort()\n",
    "end.sort()\n",
    "\n",
    "if not hot_encode:\n",
    "    start = start[0:-1]\n",
    "    end = end[0:-1]\n",
    "\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER(S)\n",
    "best_lambdas, best_scores, best_model = kfold_rr_multi_lambda_tuning(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    grid=alphas.get(\"alpha\"),\n",
    "    n_splits=n_splits,\n",
    "    start=start,\n",
    "    end=end,\n",
    "    static_lam=1,\n",
    "    verbose=0,\n",
    "    show_linalg_warning=False,\n",
    "    fit_model_after_tuning=True,\n",
    ")\n",
    "### PREDICT WITH BEST HYPERPARAMETER(S)\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "if anomaly:\n",
    "    pass\n",
    "else:\n",
    "    val_predictions = np.maximum(val_predictions, 0)\n",
    "    train_predictions = np.maximum(train_predictions, 0)\n",
    "    test_predictions = np.maximum(test_predictions, 0)\n",
    "\n",
    "#########################################     DE-MEAN TRAIN R2    #########################################\n",
    "fold_list = []\n",
    "for i in range(n_splits):\n",
    "    idx = len(list(kfold.split(y_train))[i][1])\n",
    "    fold = np.repeat(i + 1, idx).tolist()\n",
    "    fold_list.append(fold)\n",
    "fold_list = [item for sublist in fold_list for item in sublist]\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"data_fold\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"oos_prediction\"] = val_predictions\n",
    "train_split[\"val_fold\"] = fold_list\n",
    "train_split = demean_by_group(\n",
    "    train_split, predicted=\"oos_prediction\", group=[\"district\"]\n",
    ")\n",
    "\n",
    "#########################################     DE-MEAN TEST R2    #########################################\n",
    "test_split = pd.DataFrame(\n",
    "    {\"data_fold\": np.repeat(\"test\", len(x_test))}, index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"oos_prediction\"] = test_predictions\n",
    "test_split[\"val_fold\"] = n_splits + 1\n",
    "test_split = demean_by_group(test_split, predicted=\"oos_prediction\", group=[\"district\"])\n",
    "\n",
    "#########################################     OUT OF SAMPLE PREDICTIONS    #########################################\n",
    "oos_preds = pd.concat([train_split, test_split])\n",
    "oos_preds[[\"split\", \"random_state\"]] = split, random_state\n",
    "oos_preds[\"variables\"] = variable_groups_str\n",
    "\n",
    "#########################################     SCORES    #########################################\n",
    "val_R2 = r2_score(y_train, val_predictions)\n",
    "val_r = pearsonr(val_predictions, y_train)[0]\n",
    "train_R2 = r2_score(y_train, train_predictions)\n",
    "train_r = pearsonr(train_predictions, y_train)[0]\n",
    "test_R2 = r2_score(y_test, test_predictions)\n",
    "test_r = pearsonr(test_predictions, y_test)[0]\n",
    "\n",
    "if anomaly:\n",
    "    demean_cv_R2 = np.nan\n",
    "    demean_cv_r = np.nan\n",
    "    demean_test_R2 = np.nan\n",
    "    demean_test_r = np.nan\n",
    "else:\n",
    "    demean_cv_R2 = r2_score(\n",
    "        train_split.demean_log_yield, train_split.demean_oos_prediction\n",
    "    )\n",
    "    demean_cv_r = pearsonr(\n",
    "        train_split.demean_log_yield, train_split.demean_oos_prediction\n",
    "    )[0]\n",
    "    demean_test_R2 = r2_score(\n",
    "        test_split.demean_log_yield, test_split.demean_oos_prediction\n",
    "    )\n",
    "    demean_test_r = pearsonr(\n",
    "        test_split.demean_log_yield, test_split.demean_oos_prediction\n",
    "    )[0]\n",
    "\n",
    "d = {\n",
    "    \"split\": split,\n",
    "    \"random_state\": random_state,\n",
    "    \"variables\": \"_\".join(variable_groups),\n",
    "    \"year_start\": year_start,\n",
    "    \"hot_encode\": hot_encode,\n",
    "    \"anomaly\": anomaly,\n",
    "    \"total_n\": len(x_all),\n",
    "    \"train_n\": len(x_train),\n",
    "    \"test_n\": len(x_test),\n",
    "    \"best_reg_param\": best_lambdas,\n",
    "    \"mean_of_val_R2\": best_scores,\n",
    "    \"val_R2\": val_R2,\n",
    "    \"val_r\": val_r,\n",
    "    \"val_r2\": val_r**2,\n",
    "    \"train_R2\": train_R2,\n",
    "    \"train_r\": train_r,\n",
    "    \"train_r2\": train_r**2,\n",
    "    \"test_R2\": test_R2,\n",
    "    \"test_r\": test_r,\n",
    "    \"test_r2\": test_r**2,\n",
    "    \"demean_cv_R2\": demean_cv_R2,\n",
    "    \"demean_cv_r\": demean_cv_r,\n",
    "    \"demean_cv_r2\": demean_cv_r**2,\n",
    "    \"demean_test_R2\": demean_test_R2,\n",
    "    \"demean_test_r\": demean_test_r,\n",
    "    \"demean_test_r2\": demean_test_r**2,\n",
    "}\n",
    "d, oos_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
