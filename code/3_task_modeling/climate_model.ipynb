{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aaf7810-5612-46c5-8369-1e95387fe44f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q pyhere p_tqdm glum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0fcec9b-2f29-4ef1-b070-dd823431cbf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d64e464-31b8-4c88-bbf5-877a8fa59fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from pyhere import here\n",
    "from datetime import date\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import pickle\n",
    "\n",
    "import pyarrow\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import p_tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    LeaveOneGroupOut,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    cross_val_predict,\n",
    ")\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "from task_modeling_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b3750c-d520-4102-8fe8-dd4b4a4358f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def climate_model(\n",
    "    data,\n",
    "    hot_encode=True,\n",
    "    anomaly=False,\n",
    "    variable_groups=[\"pre\", \"tmp\", \"ndvi\"],\n",
    "    index_cols=[\"year\", \"district\", \"yield_mt\"],\n",
    "    year_start=2016,\n",
    "    n_splits=5,\n",
    "):\n",
    "    #########################################     READ DATA    #########################################\n",
    "    data = data.dropna()\n",
    "\n",
    "    keep_cols = []\n",
    "\n",
    "    for var in variable_groups:\n",
    "        tmp = data.columns[data.columns.to_series().str.contains(var)].tolist()\n",
    "        keep_cols.append(tmp)\n",
    "\n",
    "    keep_cols = [*index_cols, *[col for cols in keep_cols for col in cols]]\n",
    "\n",
    "    data = data.loc[:, keep_cols]\n",
    "\n",
    "    data = data[data.year >= year_start]\n",
    "\n",
    "    crop_yield = data.copy().loc[:, tuple(index_cols)].reset_index(drop=True)\n",
    "    crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "    ########################################    STANDARDIZE FEATURES    #########################################\n",
    "    data = data.set_index(index_cols)\n",
    "    data_scaled = StandardScaler().fit_transform(data.values)\n",
    "    data = pd.DataFrame(data_scaled, index=data.index).reset_index()\n",
    "    data.columns = data.columns.astype(str)\n",
    "\n",
    "    #########################################    HOT ENCODE    #########################################\n",
    "    if hot_encode:\n",
    "        index_cols.remove(\"district\")\n",
    "        data = pd.get_dummies(data, columns=[\"district\"], drop_first=False)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    #########################################     K-FOLD SPLIT    #########################################\n",
    "    x_all = data.drop(index_cols, axis=1)\n",
    "    y_all = np.log10(data.yield_mt.to_numpy() + 1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_all, y_all, test_size=0.2, random_state=0\n",
    "    )\n",
    "\n",
    "    #########################################     K-FOLD CV    #########################################\n",
    "    ### SETUP\n",
    "    tic = time.time()\n",
    "    kfold = KFold(n_splits=n_splits)\n",
    "    alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "\n",
    "    i = 0\n",
    "    start = [i]\n",
    "    end = [x_train.shape[1]]\n",
    "\n",
    "    for var in variable_groups:\n",
    "        i += 12\n",
    "        start.append(i)\n",
    "        end.append(i)\n",
    "    start.sort()\n",
    "    end.sort()\n",
    "\n",
    "    if not hot_encode:\n",
    "        start = start[0:-1]\n",
    "        end = end[0:-1]\n",
    "\n",
    "    ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER(S)\n",
    "    best_lambdas, best_scores, best_model = kfold_rr_multi_lambda_tuning(\n",
    "        X=x_train,\n",
    "        y=y_train,\n",
    "        grid=alphas.get(\"alpha\"),\n",
    "        n_splits=n_splits,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        static_lam=1,\n",
    "        verbose=2,\n",
    "        show_linalg_warning=False,\n",
    "        fit_model_after_tuning=True,\n",
    "    )\n",
    "    ### PREDICT WITH BEST HYPERPARAMETER(S)\n",
    "    val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "    train_predictions = best_model.predict(x_train)\n",
    "    test_predictions = best_model.predict(x_test)\n",
    "\n",
    "    #########################################     DE-MEAN R2    #########################################\n",
    "    crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "    train_split = pd.DataFrame(\n",
    "        np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    "    )\n",
    "    train_split = train_split.join(\n",
    "        crop_yield.copy()[crop_yield.index.isin(x_train.index)]\n",
    "    )\n",
    "    train_split[\"cv_prediction\"] = np.maximum(val_predictions, 0)\n",
    "    train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"] - train_split.groupby(\n",
    "        \"district\"\n",
    "    )[\"log_yield\"].transform(\"mean\")\n",
    "    train_split[\"demean_cv_prediction\"] = train_split[\n",
    "        \"cv_prediction\"\n",
    "    ] - train_split.groupby(\"district\")[\"cv_prediction\"].transform(\"mean\")\n",
    "\n",
    "    test_split = pd.DataFrame(\n",
    "        np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    "    )\n",
    "    test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "    test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "    test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "    test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "    predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "    test_split[\"demean_test_yield\"] = test_split[\"log_yield\"] - test_split.groupby(\n",
    "        \"district\"\n",
    "    )[\"log_yield\"].transform(\"mean\")\n",
    "    test_split[\"demean_test_prediction\"] = test_split[\n",
    "        \"prediction\"\n",
    "    ] - test_split.groupby(\"district\")[\"prediction\"].transform(\"mean\")\n",
    "\n",
    "    print(\n",
    "        f\"\"\"\n",
    "Finish:\n",
    "    Variables: {variable_groups}\n",
    "    Lambdas:   {best_lambdas}\n",
    "    One-hot encoding: {hot_encode}\n",
    "    Anomaly: {anomaly}\n",
    "    \n",
    "    Final Val  R2: {r2_score(y_train, val_predictions):0.4f} \n",
    "    Final Test R2: {r2_score(y_test, test_predictions):0.4f}\n",
    "    \n",
    "    Demean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.4f}\n",
    "    Demean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.4f}\n",
    "    \n",
    "    Total time: {(time.time()-tic)/60:0.2f} minutes\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af8e6b69-af3f-4cc6-b3e5-15c379a6cb71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 1: 0.01\n",
      "\tVal R2 1: 0.5526\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 2: 0.001\n",
      "\tVal R2 2: 0.6862\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 3: 0.1\n",
      "\tVal R2 3: 0.6868\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 4: 1e-05\n",
      "\tVal R2 4: 0.7895\n",
      "\n",
      "\n",
      "Finish:\n",
      "    Variables: ['pre', 'tmp', 'ndvi']\n",
      "    Lambdas:   [0.01, 0.001, 0.1, 1e-05]\n",
      "    One-hot encoding: True\n",
      "    Anomaly: False\n",
      "    \n",
      "    Final Val  R2: 0.8008 \n",
      "    Final Test R2: 0.8356\n",
      "    \n",
      "    Demean Val  R2: 0.1744\n",
      "    Demean Test R2: 0.6005\n",
      "    \n",
      "    Total time: 0.30 minutes\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "climate_model(\n",
    "    pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\")),\n",
    "    year_start=2016,\n",
    "    variable_groups=[\"pre\", \"tmp\", \"ndvi\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4df7999a-74a6-4cd8-b754-674c75ed00cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 1: 0.01\n",
      "\tVal R2 1: 0.5526\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 2: 0.001\n",
      "\tVal R2 2: 0.6862\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 3: 0.1\n",
      "\tVal R2 3: 0.6868\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 4: 1e-05\n",
      "\tVal R2 4: 0.7895\n",
      "\n",
      "\n",
      "Finish:\n",
      "    Variables: ['pre', 'tmp', 'ndvi']\n",
      "    Lambdas: [0.01, 0.001, 0.1, 1e-05]\n",
      "    One-hot encoding: True\n",
      "    Anomaly: False\n",
      "\n",
      "    Final Val  R2: 0.8008 \n",
      "    Final Test R2: 0.8356\n",
      "\n",
      "    Demean Val  R2: 0.1744\n",
      "    Demean Test R2: 0.6005\n",
      "\n",
      "    Total time: 0.30 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### TESTING\n",
    "\n",
    "data = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "hot_encode = True\n",
    "anomaly = False\n",
    "variable_groups = [\"pre\", \"tmp\", \"ndvi\"]\n",
    "index_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "year_start = 2016\n",
    "n_splits = 5\n",
    "\n",
    "#########################################     READ DATA    #########################################\n",
    "data = data.dropna()\n",
    "\n",
    "keep_cols = []\n",
    "\n",
    "for var in variable_groups:\n",
    "    tmp = data.columns[data.columns.to_series().str.contains(var)].tolist()\n",
    "    keep_cols.append(tmp)\n",
    "\n",
    "keep_cols = [*index_cols, *[col for cols in keep_cols for col in cols]]\n",
    "\n",
    "data = data.loc[:, keep_cols]\n",
    "\n",
    "data = data[data.year >= year_start]\n",
    "\n",
    "crop_yield = data.copy().loc[:, tuple(index_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "########################################    STANDARDIZE FEATURES    #########################################\n",
    "data = data.set_index(index_cols)\n",
    "data_scaled = StandardScaler().fit_transform(data.values)\n",
    "data = pd.DataFrame(data_scaled, index=data.index).reset_index()\n",
    "data.columns = data.columns.astype(str)\n",
    "\n",
    "#########################################    HOT ENCODE    #########################################\n",
    "if hot_encode:\n",
    "    index_cols.remove(\"district\")\n",
    "    data = pd.get_dummies(data, columns=[\"district\"], drop_first=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = data.drop(index_cols, axis=1)\n",
    "y_all = np.log10(data.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV    #########################################\n",
    "### SETUP\n",
    "tic = time.time()\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "\n",
    "i = 0\n",
    "start = [i]\n",
    "end = [x_train.shape[1]]\n",
    "\n",
    "for var in variable_groups:\n",
    "    i += 12\n",
    "    start.append(i)\n",
    "    end.append(i)\n",
    "start.sort()\n",
    "end.sort()\n",
    "\n",
    "if not hot_encode:\n",
    "    start = start[0:-1]\n",
    "    end = end[0:-1]\n",
    "\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER(S)\n",
    "best_lambdas, best_scores, best_model = kfold_rr_multi_lambda_tuning(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    grid=alphas.get(\"alpha\"),\n",
    "    n_splits=n_splits,\n",
    "    start=start,\n",
    "    end=end,\n",
    "    static_lam=1,\n",
    "    verbose=2,\n",
    "    show_linalg_warning=False,\n",
    "    fit_model_after_tuning=True,\n",
    ")\n",
    "### PREDICT WITH BEST HYPERPARAMETER(S)\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################\n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"cv_prediction\"] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"] - train_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\n",
    "    \"cv_prediction\"\n",
    "] - train_split.groupby(\"district\")[\"cv_prediction\"].transform(\"mean\")\n",
    "\n",
    "test_split = pd.DataFrame(\n",
    "    np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"prediction\"].transform(\"mean\")\n",
    "\n",
    "# variable_groups.append(\"districts\")\n",
    "# group_lambdas = dict(zip(variable_groups, best_lambdas))\n",
    "# group_lambdas\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "Finish:\n",
    "    Variables: {variable_groups}\n",
    "    Lambdas: {best_lambdas}\n",
    "    One-hot encoding: {hot_encode}\n",
    "    Anomaly: {anomaly}\n",
    "\n",
    "    Final Val  R2: {r2_score(y_train, val_predictions):0.4f} \n",
    "    Final Test R2: {r2_score(y_test, test_predictions):0.4f}\n",
    "\n",
    "    Demean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.4f}\n",
    "    Demean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.4f}\n",
    "\n",
    "    Total time: {(time.time()-tic)/60:0.2f} minutes\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffc9fc-32ea-47c7-93fc-0d0aea13d688",
   "metadata": {},
   "source": [
    "# NDVI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50874f01-370e-40eb-84c1-4f98234d2632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.7802 \n",
      "Test R2: 0.8085 \n",
      "\n",
      "Demean Val  R2: 0.0921 \n",
      "Demean Test R2: 0.4394\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains(\"ndvi\")]\n",
    "keep_cols = [*ndvi_cols, *drop_cols]\n",
    "climate_df = climate_df.loc[:, keep_cols]\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "hot_encode = True\n",
    "# hot_encode = False\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    HOT ENCODE    #########################################\n",
    "if hot_encode:\n",
    "    drop_cols.remove(\"district\")\n",
    "    climate_df = pd.get_dummies(climate_df, columns=[\"district\"], drop_first=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "climate_df = climate_df.set_index(drop_cols)\n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis=1)\n",
    "y_all = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring=\"r2\", cv=kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################\n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"cv_prediction\"] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"] - train_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\n",
    "    \"cv_prediction\"\n",
    "] - train_split.groupby(\"district\")[\"cv_prediction\"].transform(\"mean\")\n",
    "\n",
    "test_split = pd.DataFrame(\n",
    "    np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"prediction\"].transform(\"mean\")\n",
    "\n",
    "print(\n",
    "    f\"Val  R2: {r2_score(y_train, val_predictions):0.4f}\",\n",
    "    f\"\\nTest R2: {r2_score(y_test, test_predictions):0.4f}\",\n",
    "    f\"\\n\\nDemean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.4f}\",\n",
    "    f\"\\nDemean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.4f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3eef3-18bb-4b2b-8da8-02bc0230bda2",
   "metadata": {},
   "source": [
    "# Precipitation, Temperature, and NDVI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1840a92e-3eca-4039-a5d4-6a8367316314",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.8044 \n",
      "Test R2: 0.8297 \n",
      "\n",
      "Demean Val  R2: 0.1723 \n",
      "Demean Test R2: 0.5297\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "hot_encode = True\n",
    "# hot_encode = False\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    HOT ENCODE    #########################################\n",
    "if hot_encode:\n",
    "    drop_cols.remove(\"district\")\n",
    "    climate_df = pd.get_dummies(climate_df, columns=[\"district\"], drop_first=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "climate_df = climate_df.set_index(drop_cols)\n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis=1)\n",
    "y_all = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring=\"r2\", cv=kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################\n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"cv_prediction\"] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"] - train_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\n",
    "    \"cv_prediction\"\n",
    "] - train_split.groupby(\"district\")[\"cv_prediction\"].transform(\"mean\")\n",
    "\n",
    "test_split = pd.DataFrame(\n",
    "    np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"log_yield\"].transform(\"mean\")\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"] - test_split.groupby(\n",
    "    \"district\"\n",
    ")[\"prediction\"].transform(\"mean\")\n",
    "\n",
    "print(\n",
    "    f\"Val  R2: {r2_score(y_train, val_predictions):0.4f}\",\n",
    "    f\"\\nTest R2: {r2_score(y_test, test_predictions):0.4f}\",\n",
    "    f\"\\n\\nDemean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.4f}\",\n",
    "    f\"\\nDemean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.4f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b577bc71-d766-42fe-adad-93286a5a19ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.792567922097443"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_reg.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3dfe35-025c-4b06-8b2b-23b4f223f11b",
   "metadata": {},
   "source": [
    "# NDVI Anomaly Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f20bb178-7708-45e3-aa68-124c88541b05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.4449\n",
      "Test R2: 0.4293\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains(\"ndvi\")]\n",
    "keep_cols = [*ndvi_cols, *drop_cols]\n",
    "climate_df = climate_df.loc[:, keep_cols]\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "climate_df = climate_df.set_index(drop_cols)\n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################\n",
    "climate_df[\"yield_mt\"] = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "climate_df.set_index([\"year\", \"district\"], inplace=True)\n",
    "var_cols = climate_df.columns\n",
    "climate_df = climate_df[var_cols] - climate_df.groupby([\"district\"], as_index=True)[\n",
    "    var_cols\n",
    "].transform(\"mean\")\n",
    "climate_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis=1)\n",
    "y_all = climate_df.yield_mt\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring=\"r2\", cv=kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################\n",
    "crop_yield[\"prediction\"] = best_model.predict(x_all)\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"cv_prediction\"] = val_predictions\n",
    "\n",
    "test_split = pd.DataFrame(\n",
    "    np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "print(\n",
    "    f\"Val  R2: {r2_score(y_train, val_predictions):0.4f}\\nTest R2: {r2_score(y_test, test_predictions):0.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c574bb5-882f-4588-ae97-44af906398a6",
   "metadata": {},
   "source": [
    "# Precipitation, Temperature, and NDVI  Anomaly model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "102e934f-c6e8-4222-9956-a621dda75298",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.5284\n",
      "Test R2: 0.5145\n"
     ]
    }
   ],
   "source": [
    "climate_df = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "climate_df = climate_df.dropna()\n",
    "drop_cols = [\"year\", \"district\", \"yield_mt\"]\n",
    "climate_df = climate_df[climate_df.year >= 2016]\n",
    "\n",
    "crop_yield = climate_df.copy().loc[:, tuple(drop_cols)].reset_index(drop=True)\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "climate_df = climate_df.set_index(drop_cols)\n",
    "climate_df_scaled = StandardScaler().fit_transform(climate_df.values)\n",
    "climate_df = pd.DataFrame(climate_df_scaled, index=climate_df.index).reset_index()\n",
    "climate_df.columns = climate_df.columns.astype(str)\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################\n",
    "climate_df[\"yield_mt\"] = np.log10(climate_df.yield_mt.to_numpy() + 1)\n",
    "climate_df.set_index([\"year\", \"district\"], inplace=True)\n",
    "var_cols = climate_df.columns\n",
    "climate_df = climate_df[var_cols] - climate_df.groupby([\"district\"], as_index=True)[\n",
    "    var_cols\n",
    "].transform(\"mean\")\n",
    "climate_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = climate_df.drop(drop_cols, axis=1)\n",
    "y_all = climate_df.yield_mt\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "### SETUP\n",
    "alphas = {\"alpha\": np.logspace(-8, 8, base=10, num=17)}\n",
    "kfold = KFold()\n",
    "ridge = Ridge(random_state=0)\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring=\"r2\", cv=kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)\n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################\n",
    "crop_yield[\"prediction\"] = best_model.predict(x_all)\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"split\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split[\"cv_prediction\"] = val_predictions\n",
    "\n",
    "test_split = pd.DataFrame(\n",
    "    np.repeat(\"test\", len(x_test)), columns=[\"split\"], index=x_test.index\n",
    ")\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "print(\n",
    "    f\"Val  R2: {r2_score(y_train, val_predictions):0.4f}\\nTest R2: {r2_score(y_test, test_predictions):0.4f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
