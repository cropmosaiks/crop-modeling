{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f576830d-9afa-47d8-9fac-8e43292ee090",
   "metadata": {},
   "source": [
    "# Modeling Crop Yield: Landsat + Sentinel\n",
    "## Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44646286-2094-4bd0-8609-ad5efc857abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import warnings\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from pyhere import here\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import pickle\n",
    "\n",
    "import pyarrow\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import p_tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneGroupOut, cross_val_score, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr,  pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09d78fb-ccb1-4da5-901f-ced9c633366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fn(file_name):\n",
    "    f            = file_name.split(sep=\"_\")\n",
    "    satellite    = f[0],\n",
    "    bands        = f[1].replace(\"bands-\", \"\")\n",
    "    country_code = f[2],\n",
    "    points       = f[3].replace(\"k-points\", \"\")\n",
    "    num_features = f[4].replace(\"-features\", \"\")\n",
    "    yrs          = f[5].replace(\"yr-\", \"\")\n",
    "    mns          = f[6].replace(\"mn-\", \"\")\n",
    "    limit_months = f[7].replace(\"lm-\", \"\")\n",
    "    crop_mask    = f[8].replace(\"cm-\", \"\")\n",
    "    weighted_avg = f[9].replace(\"wa-\", \"\")\n",
    "    \n",
    "    return satellite, bands, country_code, points, yrs, mns, num_features, limit_months, crop_mask, weighted_avg\n",
    "\n",
    "def merge(x, bases = (tuple, list)):\n",
    "    for e in x:\n",
    "        if type(e) in bases:\n",
    "            for e in merge(e, bases):\n",
    "                yield e\n",
    "        else:\n",
    "            yield e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d12eb13-13d9-4ac5-9128-495bcc51e385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "946"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(here(\"data\", \"random_features\", 'summary'))\n",
    "files = [f for f in files if f not in ('.gitkeep', '.ipynb_checkpoints')]\n",
    "paramlist = list(itertools.product(files, files))\n",
    "paramlist = [tuple(set(paramlist[i])) for i in range(len(paramlist))]\n",
    "paramlist = [x for x in paramlist if len(x) > 1] \n",
    "point_pattern = re.compile(\"20k-points\")\n",
    "wa_pattern = re.compile(\"cm-False\")\n",
    "paramlist = [t for t in paramlist if not (bool(point_pattern.search(t[0])) & bool(wa_pattern.search(t[0])))]\n",
    "paramlist = [t for t in paramlist if not (bool(point_pattern.search(t[1])) & bool(wa_pattern.search(t[1])))]\n",
    "paramlist = list(set(tuple(sorted(s)) for s in paramlist))\n",
    "len(paramlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae5e44c6-6764-431c-a369-3ea976fc1bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_2_sensors(params):\n",
    "#########################################     SET PARAMS    #########################################    \n",
    "    f1         = params[0]\n",
    "    f2         = params[1]\n",
    "\n",
    "    satellite1, bands1, country_code, points1, yrs1, mns1,\\\n",
    "    num_features1, limit_months1, crop_mask1, weighted_avg1 = split_fn(f1)\n",
    "    \n",
    "    satellite2, bands2, country_code, points2, yrs2, mns2,\\\n",
    "    num_features2, limit_months2, crop_mask2, weighted_avg2 = split_fn(f2)\n",
    "\n",
    "#########################################     READ DATA    #########################################\n",
    "    features_1 = pd.read_feather(here('data', 'random_features', 'summary', f1))\n",
    "    features_2 = pd.read_feather(here('data', 'random_features', 'summary', f2))\n",
    "    \n",
    "#########################################     CLEAN DATA    #########################################  \n",
    "    min_year = max(min(features_1.year), min(features_2.year))\n",
    "    max_year = min(max(features_1.year), max(features_2.year))\n",
    "    \n",
    "    features_1 = features_1[features_1.year >= min_year]\n",
    "    features_2 = features_2[features_2.year >= min_year]\n",
    "    \n",
    "    features_1 = features_1[features_1.year <= max_year]\n",
    "    features_2 = features_2[features_2.year <= max_year]\n",
    "    \n",
    "    features_1.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "    features_2.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "    \n",
    "#########################################     JOIN DATA    #########################################  \n",
    "    drop_cols = ['district', 'year', 'yield_mt']\n",
    "    \n",
    "    features_1 = features_1.set_index(drop_cols).add_prefix(\"f1_\")\n",
    "    features_2 = features_2.set_index(drop_cols).add_prefix(\"f2_\")\n",
    "    \n",
    "    features = features_1.join(features_2).reset_index()\n",
    "    features = features[~features.isna().any(axis = 1)]\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "    features = features.set_index(drop_cols) \n",
    "    features_scaled = StandardScaler().fit_transform(features.values)\n",
    "    features = pd.DataFrame(features_scaled, index=features.index).reset_index()\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################  \n",
    "    features['yield_mt'] = np.log10(features.yield_mt + 1)\n",
    "    features.set_index(['year', 'district'], inplace=True)\n",
    "    var_cols = features.columns\n",
    "    features = features[var_cols] - features.groupby(['district'], as_index=True)[var_cols].transform('mean')\n",
    "    features.reset_index(drop=False, inplace=True)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################    \n",
    "    x_all = features.drop(drop_cols, axis=1)\n",
    "    y_all = features.yield_mt\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################  \n",
    "    ### SETUP\n",
    "    alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "    kfold  = KFold()\n",
    "    ridge  = Ridge()    \n",
    "    \n",
    "    ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "    ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "    ridge_reg.fit(x_train, y_train)\n",
    "    best_model = ridge_reg.best_estimator_\n",
    "    ### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "    val_predictions = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "    train_predictions = best_model.predict(x_train)\n",
    "    test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "#########################################     SAVE RESULTS    #########################################\n",
    "    d = {\n",
    "        'country': country_code,\n",
    "        \n",
    "        'satellite_1'   : satellite1[0],\n",
    "        'bands_1'       : bands1,\n",
    "        'num_features_1': num_features1,\n",
    "        'points_1'      : points1, \n",
    "        'month_range_1' : mns1,\n",
    "        'limit_months_1': limit_months1,\n",
    "        'crop_mask_1'   : crop_mask1,\n",
    "        'weighted_avg_1': weighted_avg1,\n",
    "        \n",
    "        'satellite_2'   : satellite2[0],\n",
    "        'bands_2'       : bands2,\n",
    "        'num_features_2': num_features2,\n",
    "        'points_2'      : points2, \n",
    "        'month_range_2' : mns2,\n",
    "        'limit_months_2': limit_months2,\n",
    "        'crop_mask_2'   : crop_mask2,\n",
    "        'weighted_avg_2': weighted_avg2,\n",
    "\n",
    "        'total_n': len(x_all),\n",
    "        'train_n': len(x_train),\n",
    "        'test_n' : len(x_test),\n",
    "        \n",
    "        'best_reg_param': list(ridge_reg.best_params_.values())[0],\n",
    "        'mean_of_val_R2s': ridge_reg.best_score_,\n",
    "        'val_R2': r2_score(y_train, val_predictions),\n",
    "        'val_r' : pearsonr(val_predictions, y_train)[0],\n",
    "        'val_r2': pearsonr(val_predictions, y_train)[0] ** 2,\n",
    "        \n",
    "        'train_R2': r2_score(y_train, train_predictions),\n",
    "        'train_r' : pearsonr(train_predictions, y_train)[0],\n",
    "        'train_r2': pearsonr(train_predictions, y_train)[0] ** 2,\n",
    "        \n",
    "        'test_R2': r2_score(y_test, test_predictions),\n",
    "        'test_r' : pearsonr(test_predictions, y_test)[0],\n",
    "        'test_r2': pearsonr(test_predictions, y_test)[0] ** 2,\n",
    "    }\n",
    "    df = pd.DataFrame(data=d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f752cfdc-7cca-4ce2-9841-ac6118f0bbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4468d7215942402c9fb168f25e1cb9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/946 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results as: 2_sensor_anomaly_results_2022-12-19.csv\n",
      "\n",
      "\n",
      "CPU times: user 10.1 s, sys: 1.74 s, total: 11.9 s\n",
      "Wall time: 2h 3min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time     \n",
    "##### With progress bar\n",
    "workers = os.cpu_count()\n",
    "if __name__ == \"__main__\":\n",
    "    output = []\n",
    "    for result in p_tqdm.p_umap(model_2_sensors, paramlist):\n",
    "        output.append(result)\n",
    "    results = pd.concat(output).reset_index(drop=True)\n",
    "    today = date.today().strftime(\"%Y-%m-%d\")\n",
    "    file_name = f'2_sensor_anomaly_results_{today}.csv'\n",
    "    print(f\"Saving results as: {file_name}\\n\\n\")           \n",
    "    results.to_csv(here(\"data\",\"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eabd364e-abfa-4a19-ad66-30eedac56a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 40s\n",
      "Wall time: 30.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### TESTING  \n",
    "f1 = 'landsat-c2-l2_bands-r-g-b-nir-swir16-swir22_ZMB_20k-points_1024-features_yr-2009-2021_mn-4-9_lm-True_cm-True_wa-False_summary.feather'\n",
    "f2 = 'sentinel-2-l2a_bands-2-3-4_ZMB_4k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather'\n",
    "\n",
    "#########################################     SET PARAMS    #########################################    \n",
    "# f1         = params[0]\n",
    "# f2         = params[1]\n",
    "\n",
    "satellite1, bands1, country_code, points1, yrs1, mns1,\\\n",
    "num_features1, limit_months1, crop_mask1, weighted_avg1 = split_fn(f1)\n",
    "\n",
    "satellite2, bands2, country_code, points2, yrs2, mns2,\\\n",
    "num_features2, limit_months2, crop_mask2, weighted_avg2 = split_fn(f2)\n",
    "\n",
    "#########################################     READ DATA    #########################################\n",
    "features_1 = pd.read_feather(here('data', 'random_features', 'summary', f1))\n",
    "features_2 = pd.read_feather(here('data', 'random_features', 'summary', f2))\n",
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "\n",
    "#########################################     CLEAN DATA    #########################################  \n",
    "min_year = max(min(features_1.year), min(features_2.year))\n",
    "max_year = min(max(features_1.year), max(features_2.year))\n",
    "\n",
    "features_1 = features_1[features_1.year >= min_year]\n",
    "features_2 = features_2[features_2.year >= min_year]\n",
    "\n",
    "features_1 = features_1[features_1.year <= max_year]\n",
    "features_2 = features_2[features_2.year <= max_year]\n",
    "\n",
    "features_1.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "features_2.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "#########################################     JOIN FEATURES    #########################################  \n",
    "drop_cols = ['district', 'year', 'yield_mt']\n",
    "\n",
    "features_1 = features_1.set_index(drop_cols).add_prefix(\"f1_\")\n",
    "features_2 = features_2.set_index(drop_cols).add_prefix(\"f2_\")\n",
    "\n",
    "features = features_1.join(features_2).reset_index()\n",
    "features = features[~features.isna().any(axis = 1)]\n",
    "\n",
    "#########################################    JOIN CLIMATE VARS    ######################################### \n",
    "ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains('ndvi')]\n",
    "keep_cols = [*ndvi_cols, *drop_cols]\n",
    "# climate_df = climate_df.loc[:, keep_cols]\n",
    "\n",
    "features = features.set_index(drop_cols).join(climate_df.set_index(drop_cols)).reset_index()\n",
    "features = features[features.year <= max(climate_df.year)]\n",
    "\n",
    "crop_yield = features.copy().loc[:, tuple(drop_cols)]\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "features = features.set_index(drop_cols) \n",
    "features_scaled = StandardScaler().fit_transform(features.values)\n",
    "features = pd.DataFrame(features_scaled, index=features.index).reset_index()\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################  \n",
    "features['yield_mt'] = np.log10(features.yield_mt + 1)\n",
    "features.set_index(['year', 'district'], inplace=True)\n",
    "var_cols = features.columns\n",
    "features = features[var_cols] - features.groupby(['district'], as_index=True)[var_cols].transform('mean')\n",
    "features.reset_index(drop=False, inplace=True)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################    \n",
    "x_all = features.drop(drop_cols, axis=1)\n",
    "y_all = features.yield_mt\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################  \n",
    "### SETUP\n",
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "kfold  = KFold()\n",
    "ridge  = Ridge()    \n",
    "\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "ridge_reg.fit(x_train, y_train)\n",
    "best_model = ridge_reg.best_estimator_\n",
    "### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "val_predictions = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "#########################################     SAVE RESULTS    #########################################\n",
    "d = {\n",
    "    'country': country_code,\n",
    "\n",
    "    'satellite_1'   : satellite1[0],\n",
    "    'bands_1'       : bands1,\n",
    "    'num_features_1': num_features1,\n",
    "    'points_1'      : points1, \n",
    "    'month_range_1' : mns1,\n",
    "    'limit_months_1': limit_months1,\n",
    "    'crop_mask_1'   : crop_mask1,\n",
    "    'weighted_avg_1': weighted_avg1,\n",
    "\n",
    "    'satellite_2'   : satellite2[0],\n",
    "    'bands_2'       : bands2,\n",
    "    'num_features_2': num_features2,\n",
    "    'points_2'      : points2, \n",
    "    'month_range_2' : mns2,\n",
    "    'limit_months_2': limit_months2,\n",
    "    'crop_mask_2'   : crop_mask2,\n",
    "    'weighted_avg_2': weighted_avg2,\n",
    "\n",
    "    'total_n': len(x_all),\n",
    "    'train_n': len(x_train),\n",
    "    'test_n' : len(x_test),\n",
    "\n",
    "    'best_reg_param': list(ridge_reg.best_params_.values())[0],\n",
    "    'mean_of_val_R2s': ridge_reg.best_score_,\n",
    "    'val_R2': r2_score(y_train, val_predictions),\n",
    "    'val_r' : pearsonr(val_predictions, y_train)[0],\n",
    "    'val_r2': pearsonr(val_predictions, y_train)[0] ** 2,\n",
    "\n",
    "    'train_R2': r2_score(y_train, train_predictions),\n",
    "    'train_r' : pearsonr(train_predictions, y_train)[0],\n",
    "    'train_r2': pearsonr(train_predictions, y_train)[0] ** 2,\n",
    "\n",
    "    'test_R2': r2_score(y_test, test_predictions),\n",
    "    'test_r' : pearsonr(test_predictions, y_test)[0],\n",
    "    'test_r2': pearsonr(test_predictions, y_test)[0] ** 2,\n",
    "}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "181700ea-b6b2-4f3e-8be3-5df85a8853db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.6603\n",
      "Test R2: 0.4465\n"
     ]
    }
   ],
   "source": [
    "print(f'Val  R2: {r2_score(y_train, val_predictions):0.4f}\\nTest R2: {r2_score(y_test, test_predictions):0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84a1d628-bfbe-49d0-821d-7b3602ae0370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district</th>\n",
       "      <th>year</th>\n",
       "      <th>yield_anom</th>\n",
       "      <th>prediction</th>\n",
       "      <th>split</th>\n",
       "      <th>cv_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chadiza</td>\n",
       "      <td>2016</td>\n",
       "      <td>-0.045009</td>\n",
       "      <td>-0.045535</td>\n",
       "      <td>train</td>\n",
       "      <td>-0.043587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chadiza</td>\n",
       "      <td>2017</td>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.042360</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chadiza</td>\n",
       "      <td>2018</td>\n",
       "      <td>-0.142270</td>\n",
       "      <td>-0.137386</td>\n",
       "      <td>train</td>\n",
       "      <td>-0.117976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chadiza</td>\n",
       "      <td>2019</td>\n",
       "      <td>-0.008178</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>train</td>\n",
       "      <td>-0.030307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chadiza</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.055630</td>\n",
       "      <td>0.094204</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>Zambezi</td>\n",
       "      <td>2017</td>\n",
       "      <td>-0.040316</td>\n",
       "      <td>-0.033227</td>\n",
       "      <td>train</td>\n",
       "      <td>0.008366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>Zambezi</td>\n",
       "      <td>2018</td>\n",
       "      <td>-0.017917</td>\n",
       "      <td>-0.015648</td>\n",
       "      <td>train</td>\n",
       "      <td>0.097455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>Zambezi</td>\n",
       "      <td>2019</td>\n",
       "      <td>-0.097159</td>\n",
       "      <td>-0.094840</td>\n",
       "      <td>train</td>\n",
       "      <td>-0.097924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>Zambezi</td>\n",
       "      <td>2020</td>\n",
       "      <td>-0.006903</td>\n",
       "      <td>-0.007228</td>\n",
       "      <td>train</td>\n",
       "      <td>-0.033944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>Zambezi</td>\n",
       "      <td>2021</td>\n",
       "      <td>0.163478</td>\n",
       "      <td>0.145809</td>\n",
       "      <td>train</td>\n",
       "      <td>0.018950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>426 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    district  year  yield_anom  prediction  split  cv_predictions\n",
       "0    Chadiza  2016   -0.045009   -0.045535  train       -0.043587\n",
       "1    Chadiza  2017    0.085299    0.042360   test             NaN\n",
       "2    Chadiza  2018   -0.142270   -0.137386  train       -0.117976\n",
       "3    Chadiza  2019   -0.008178   -0.010000  train       -0.030307\n",
       "4    Chadiza  2020    0.055630    0.094204   test             NaN\n",
       "..       ...   ...         ...         ...    ...             ...\n",
       "421  Zambezi  2017   -0.040316   -0.033227  train        0.008366\n",
       "422  Zambezi  2018   -0.017917   -0.015648  train        0.097455\n",
       "423  Zambezi  2019   -0.097159   -0.094840  train       -0.097924\n",
       "424  Zambezi  2020   -0.006903   -0.007228  train       -0.033944\n",
       "425  Zambezi  2021    0.163478    0.145809  train        0.018950\n",
       "\n",
       "[426 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop_yield = features.copy().loc[:, tuple(drop_cols)]\n",
    "crop_yield.rename(columns={\"yield_mt\": \"yield_anom\"}, inplace = True)\n",
    "crop_yield[\"prediction\"] = best_model.predict(x_all)\n",
    "\n",
    "predictions = pd.concat([x_train, x_test])\n",
    "predictions['split'] = np.concatenate((np.repeat('train', len(x_train)), np.repeat('test', len(x_test))))\n",
    "predictions['cv_predictions'] = np.concatenate((val_predictions,  np.repeat(np.nan, len(x_test))))\n",
    "predictions = predictions[['split', 'cv_predictions']]\n",
    "predictions = crop_yield.join(predictions)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86efbabb-e03c-48ae-a134-4bb9b8185814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6588362519240656"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_pred = predictions[predictions.split == 'train']\n",
    "r2_score(cv_pred.yield_anom, cv_pred.cv_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5bc0193-8da5-46ee-9730-0ff8adea8d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44568278905099457"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = predictions[predictions.split == 'test']\n",
    "r2_score(test_pred.yield_anom, test_pred.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdc0fd7e-89b2-499b-a7d1-fbc77f7e3bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anomaly-predictions_fn-1_landsat-8-c2-l2_1-2-3-4-5-6-7_15_True_False_False_fn-2_sentinel-2-l2a_2-3-4-8_15_False_True_False.csv'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_1 = f'{satellite1[0]}_{bands1}_{points1}_{limit_months1}_{crop_mask1}_{weighted_avg1}'\n",
    "fn_2 = f'{satellite2[0]}_{bands2}_{points2}_{limit_months2}_{crop_mask2}_{weighted_avg2}'\n",
    "fn = f'anomaly-predictions_fn-1_{fn_1}_fn-2_{fn_2}.csv'\n",
    "    \n",
    "predictions_fn = here('data', 'results', fn)\n",
    "# predictions.to_csv(predictions_fn, index=False)\n",
    "\n",
    "fn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
