{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f576830d-9afa-47d8-9fac-8e43292ee090",
   "metadata": {},
   "source": [
    "# Modeling Crop Yield: Landsat + Sentinel\n",
    "## Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44646286-2094-4bd0-8609-ad5efc857abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import warnings\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from pyhere import here\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import pickle\n",
    "\n",
    "import pyarrow\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import p_tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneGroupOut, cross_val_score, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr,  pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09d78fb-ccb1-4da5-901f-ced9c633366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fn(file_name):\n",
    "    f            = file_name.split(sep=\"_\")\n",
    "    satellite    = f[0],\n",
    "    bands        = f[1].replace(\"bands-\", \"\")\n",
    "    country_code = f[2],\n",
    "    points       = f[3].replace(\"k-points\", \"\")\n",
    "    num_features = f[4].replace(\"-features\", \"\")\n",
    "    yrs          = f[5].replace(\"yr-\", \"\")\n",
    "    mns          = f[6].replace(\"mn-\", \"\")\n",
    "    limit_months = f[7].replace(\"lm-\", \"\")\n",
    "    crop_mask    = f[8].replace(\"cm-\", \"\")\n",
    "    weighted_avg = f[9].replace(\"wa-\", \"\")\n",
    "    \n",
    "    return satellite, bands, country_code, points, yrs, mns, num_features, limit_months, crop_mask, weighted_avg\n",
    "\n",
    "def merge(x, bases = (tuple, list)):\n",
    "    for e in x:\n",
    "        if type(e) in bases:\n",
    "            for e in merge(e, bases):\n",
    "                yield e\n",
    "        else:\n",
    "            yield e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d12eb13-13d9-4ac5-9128-495bcc51e385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "946"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(here(\"data\", \"random_features\", 'summary'))\n",
    "files = [f for f in files if f not in ('.gitkeep', '.ipynb_checkpoints')]\n",
    "paramlist = list(itertools.product(files, files))\n",
    "paramlist = [tuple(set(paramlist[i])) for i in range(len(paramlist))]\n",
    "paramlist = [x for x in paramlist if len(x) > 1] \n",
    "point_pattern = re.compile(\"20k-points\")\n",
    "wa_pattern = re.compile(\"cm-False\")\n",
    "paramlist = [t for t in paramlist if not (bool(point_pattern.search(t[0])) & bool(wa_pattern.search(t[0])))]\n",
    "paramlist = [t for t in paramlist if not (bool(point_pattern.search(t[1])) & bool(wa_pattern.search(t[1])))]\n",
    "paramlist = list(set(tuple(sorted(s)) for s in paramlist))\n",
    "len(paramlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae5e44c6-6764-431c-a369-3ea976fc1bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for params in paramlist[0:1]:\n",
    "def model_2_sensors(params):\n",
    "#########################################     SET PARAMS    #########################################    \n",
    "    f1         = params[0]\n",
    "    f2         = params[1]\n",
    "\n",
    "    satellite1, bands1, country_code, points1, yrs1, mns1,\\\n",
    "    num_features1, limit_months1, crop_mask1, weighted_avg1 = split_fn(f1)\n",
    "    \n",
    "    satellite2, bands2, country_code, points2, yrs2, mns2,\\\n",
    "    num_features2, limit_months2, crop_mask2, weighted_avg2 = split_fn(f2)\n",
    "    \n",
    "    alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "    kfold = KFold()\n",
    "    logo = LeaveOneGroupOut()\n",
    "    ridge = Ridge()    \n",
    "    \n",
    "#########################################     READ DATA    #########################################\n",
    "    features_1 = pd.read_feather(here('data', 'random_features', 'summary', f1))\n",
    "    features_2 = pd.read_feather(here('data', 'random_features', 'summary', f2))\n",
    "    \n",
    "#########################################     CLEAN DATA    #########################################  \n",
    "    min_year = max(min(features_1.year), min(features_2.year))\n",
    "    max_year = min(max(features_1.year), max(features_2.year))\n",
    "    \n",
    "    features_1 = features_1[features_1.year >= min_year]\n",
    "    features_2 = features_2[features_2.year >= min_year]\n",
    "    \n",
    "    features_1 = features_1[features_1.year <= max_year]\n",
    "    features_2 = features_2[features_2.year <= max_year]\n",
    "    \n",
    "    features_1.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "    features_2.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "    \n",
    "#########################################     JOIN DATA    #########################################  \n",
    "    drop_cols = ['district', 'year', 'yield_mt']\n",
    "    \n",
    "    features_1 = features_1.set_index(drop_cols).add_prefix(\"f1_\")\n",
    "    features_2 = features_2.set_index(drop_cols).add_prefix(\"f2_\")\n",
    "    \n",
    "    features = features_1.join(features_2).reset_index()\n",
    "    features = features[~features.isna().any(axis = 1)]\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "    features = features.set_index(drop_cols) \n",
    "    features_scaled = StandardScaler().fit_transform(features.values)\n",
    "    features = pd.DataFrame(features_scaled, index=features.index).reset_index()\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################  \n",
    "    features['yield_mt'] = np.log10(features.yield_mt + 1)\n",
    "    features.set_index(['year', 'district'], inplace=True)\n",
    "    var_cols = features.columns\n",
    "    features = features[var_cols] - features.groupby(['district'], as_index=True)[var_cols].transform('mean')\n",
    "    features.reset_index(drop=False, inplace=True)\n",
    "    \n",
    "#########################################     K-FOLD SPLIT    #########################################    \n",
    "    x_all = features.drop(drop_cols, axis=1)\n",
    "    y_all = features.yield_mt\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "    ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "    kfold_ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "    kfold_ridge_reg.fit(x_train, y_train)\n",
    "    kfold_best_model = kfold_ridge_reg.best_estimator_\n",
    "    ### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "    kfold_val_predictions = cross_val_predict(kfold_best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "    y_pred_train_k = kfold_best_model.predict(x_train)\n",
    "    y_pred_test_k  = kfold_best_model.predict(x_test)\n",
    "\n",
    "#########################################     LOGO ITERATOR   ###########################################\n",
    "    logo_val_results = []\n",
    "    logo_train_results = []\n",
    "    logo_test_results = []\n",
    "\n",
    "    for year in features.year.unique():\n",
    "#########################################     LOGO SPLIT   ###########################################\n",
    "        x_train_g = features[features.year != year].drop(drop_cols, axis=1)\n",
    "        y_train_g = features[features.year != year].yield_mt.ravel()\n",
    "        g_train_g = features[features.year != year].year.ravel()\n",
    "        d_train_g = features[features.year != year].district.ravel()\n",
    "\n",
    "        x_test_g = features[features.year == year].drop(drop_cols, axis=1)\n",
    "        y_test_g = features[features.year == year].yield_mt.ravel()\n",
    "        g_test_g = features[features.year == year].year.ravel()\n",
    "        d_test_g = features[features.year == year].district.ravel()\n",
    "\n",
    "#########################################     LOGO CV   ###########################################\n",
    "        ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "        logo_ridge_reg = GridSearchCV(ridge, alphas, scoring='r2', cv=logo)\n",
    "        logo_ridge_reg.fit(x_train_g, y_train_g, groups=g_train_g)\n",
    "        logo_best_model = logo_ridge_reg.best_estimator_\n",
    "        ### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "        logo_val_predictions = cross_val_predict(logo_best_model, X=x_train_g, y=y_train_g, groups=g_train_g, cv=logo) \n",
    "        logo_train_pred = logo_best_model.predict(x_train_g)\n",
    "        logo_test_pred  = logo_best_model.predict(x_test_g)\n",
    "\n",
    "#########################################     LOGO RESULTS   ###########################################\n",
    "        val_results = {'year': g_train_g, 'district': d_train_g, 'split': 'val', \n",
    "                       'observed': y_train_g, 'predicted': logo_val_predictions}\n",
    "\n",
    "        train_results = {'year': g_train_g, 'district': d_train_g,'split': 'train', \n",
    "                         'observed': y_train_g, 'predicted': logo_train_pred}\n",
    "\n",
    "        test_results = {'year': g_test_g, 'district': d_test_g, 'split': 'test', \n",
    "                        'observed': y_test_g, 'predicted': logo_test_pred}\n",
    "        \n",
    "        logo_val_results.append(val_results)\n",
    "        logo_train_results.append(train_results)\n",
    "        logo_test_results.append(test_results)\n",
    "\n",
    "#########################################     EXPLODE RESULTS   ###########################################\n",
    "    explode_cols = ['year', 'district', 'observed', 'predicted']\n",
    "    val_df   = pd.DataFrame(logo_val_results  ).explode(explode_cols) \n",
    "    train_df = pd.DataFrame(logo_train_results).explode(explode_cols) \n",
    "    test_df  = pd.DataFrame(logo_test_results ).explode(explode_cols)\n",
    "    \n",
    "    group_cols = ['year', 'district', 'split']\n",
    "    val_summary   =   val_df.groupby(group_cols, as_index=False).mean()\n",
    "    train_summary = train_df.groupby(group_cols, as_index=False).mean()\n",
    "\n",
    "#########################################     SAVE RESULTS    #########################################\n",
    "    d = {\n",
    "        'country': country_code,\n",
    "        \n",
    "        'satellite_1'   : satellite1[0],\n",
    "        'bands_1'       : bands1,\n",
    "        'num_features_1': num_features1,\n",
    "        'points_1'      : points1, \n",
    "        'month_range_1' : mns1,\n",
    "        'limit_months_1': limit_months1,\n",
    "        'crop_mask_1'   : crop_mask1,\n",
    "        'weighted_avg_1': weighted_avg1,\n",
    "        \n",
    "        'satellite_2'   : satellite2[0],\n",
    "        'bands_2'       : bands2,\n",
    "        'num_features_2': num_features2,\n",
    "        'points_2'      : points2, \n",
    "        'month_range_2' : mns2,\n",
    "        'limit_months_2': limit_months2,\n",
    "        'crop_mask_2'   : crop_mask2,\n",
    "        'weighted_avg_2': weighted_avg2,\n",
    "\n",
    "        'kfold_total_n': len(x_all),\n",
    "        'kfold_train_n': len(x_train),\n",
    "        'kfold_test_n' : len(x_test),\n",
    "        \n",
    "        'kfold_best_reg_param': list(kfold_ridge_reg.best_params_.values())[0],\n",
    "        'kfold_mean_of_val_R2s': kfold_ridge_reg.best_score_,\n",
    "        'kfold_val_R2': r2_score(y_train, kfold_val_predictions),\n",
    "        'kfold_val_r' : pearsonr(kfold_val_predictions, y_train)[0],\n",
    "        'kfold_val_r2': pearsonr(kfold_val_predictions, y_train)[0] ** 2,\n",
    "        \n",
    "        'kfold_train_R2': r2_score(y_train, y_pred_train_k),\n",
    "        'kfold_train_r' : pearsonr(y_pred_train_k, y_train)[0],\n",
    "        'kfold_train_r2': pearsonr(y_pred_train_k, y_train)[0] ** 2,\n",
    "        \n",
    "        'kfold_test_R2': r2_score(y_test, y_pred_test_k),\n",
    "        'kfold_test_r' : pearsonr(y_pred_test_k, y_test)[0],\n",
    "        'kfold_test_r2': pearsonr(y_pred_test_k, y_test)[0] ** 2,\n",
    "        \n",
    "        'logo_total_n': len(features),\n",
    "        'logo_train_n': len(train_df),\n",
    "        'logo_test_n' : len(test_df),    \n",
    "        \n",
    "        'logo_best_reg_param': list(logo_ridge_reg.best_params_.values())[0],      \n",
    "        'logo_summary_val_R2': r2_score(val_summary.observed, val_summary.predicted),\n",
    "        'logo_summary_val_r' : pearsonr(val_summary.observed, val_summary.predicted)[0],\n",
    "        'logo_val_R2' : r2_score(val_df.observed, val_df.predicted),\n",
    "        'logo_val_r'  : pearsonr(val_df.predicted, val_df.observed)[0],\n",
    "        'logo_val_r2' : pearsonr(val_df.predicted, val_df.observed)[0] ** 2,\n",
    "        \n",
    "        'logo_summary_train_R2': r2_score(train_summary.observed, train_summary.predicted),\n",
    "        'logo_summary_train_r' : pearsonr(train_summary.observed, train_summary.predicted)[0],\n",
    "        'logo_train_R2': r2_score(train_df.observed, train_df.predicted),\n",
    "        'logo_train_r' : pearsonr(train_df.predicted, train_df.observed)[0],\n",
    "        'logo_train_r2': pearsonr(train_df.predicted, train_df.observed)[0] ** 2,\n",
    "        \n",
    "        'logo_test_R2': r2_score(test_df.observed, test_df.predicted),\n",
    "        'logo_test_r' : pearsonr(test_df.predicted, test_df.observed)[0],\n",
    "        'logo_test_r2': pearsonr(test_df.predicted, test_df.observed)[0] ** 2,\n",
    "    }\n",
    "    print('done')\n",
    "    df = pd.DataFrame(data=d)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86798bf-b37f-414e-a59f-e0d0a1512d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "%%time    \n",
    "#### No progress bar\n",
    "workers = os.cpu_count()\n",
    "if __name__ == \"__main__\":\n",
    "    with multiprocessing.Pool(processes=workers) as pool:\n",
    "        output = []\n",
    "        for result in pool.imap_unordered(model_2_sensors, paramlist):\n",
    "            output.append(result)\n",
    "    results = pd.concat(output).reset_index(drop=True)\n",
    "    today = date.today().strftime(\"%Y-%m-%d\")\n",
    "    file_name = f'2_sensor_anomaly_results_{today}.csv'\n",
    "    print(f\"Saving results as: {file_name}\\n\\n\")           \n",
    "    results.to_csv(here(\"data\",\"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f16726-04d2-447b-a105-2a6c14742925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8bc41c-dcdc-4567-a7e8-73165b15f64b",
   "metadata": {},
   "source": [
    "###### max(results.logo_val_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752cfdc-7cca-4ce2-9841-ac6118f0bbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e163e896464d75a525712265ec2892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/946 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time     \n",
    "# ##### With progress bar\n",
    "# workers = os.cpu_count()\n",
    "# if __name__ == \"__main__\":\n",
    "#     output = []\n",
    "#     for result in p_tqdm.p_umap(model_2_sensors, paramlist):\n",
    "#         output.append(result)\n",
    "#     results = pd.concat(output).reset_index(drop=True)\n",
    "#     today = date.today().strftime(\"%Y-%m-%d\")\n",
    "#     file_name = f'2_sensor_anomaly_results_{today}.csv'\n",
    "#     print(f\"Saving results as: {file_name}\\n\\n\")           \n",
    "#     results.to_csv(here(\"data\",\"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e33c7-dc2c-4201-a3f3-1ccccc02badf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd364e-abfa-4a19-ad66-30eedac56a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #########################################     SET PARAMS    #########################################    \n",
    "# f1 = 'landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_20k-points_1000-features_yr-2013-2021_mn-4-9_lm-True_cm-True_wa-True_summary.feather'\n",
    "# f2 = 'sentinel-2-l2a_bands-2-3-4_ZMB_4k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-True_summary.feather'\n",
    "\n",
    "# satellite1, bands1, country_code, points1, yrs1, mns1,\\\n",
    "# num_features1, limit_months1, crop_mask1, weighted_avg1 = split_fn(f1)\n",
    "\n",
    "# satellite2, bands2, country_code, points2, yrs2, mns2,\\\n",
    "# num_features2, limit_months2, crop_mask2, weighted_avg2 = split_fn(f2)\n",
    "# alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "\n",
    "# #########################################     READ DATA    #########################################\n",
    "# features_1 = pd.read_feather(here('data', 'random_features', 'summary', f1))\n",
    "# features_2 = pd.read_feather(here('data', 'random_features', 'summary', f2))\n",
    "\n",
    "# #########################################     CLEAN DATA    #########################################  \n",
    "# min_year = max(min(features_1.year), min(features_2.year))\n",
    "# max_year = min(max(features_1.year), max(features_2.year))\n",
    "\n",
    "# features_1 = features_1[features_1.year >= min_year]\n",
    "# features_2 = features_2[features_2.year >= min_year]\n",
    "\n",
    "# features_1 = features_1[features_1.year <= max_year]\n",
    "# features_2 = features_2[features_2.year <= max_year]\n",
    "\n",
    "# features_1.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "# features_2.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "# drop_cols = ['district', 'year', 'yield_mt']\n",
    "\n",
    "# features_1 = features_1.set_index(drop_cols).add_prefix(\"f1_\")\n",
    "# features_2 = features_2.set_index(drop_cols).add_prefix(\"f2_\")\n",
    "\n",
    "# #########################################     JOIN DATA    #########################################  \n",
    "# features = features_1.join(features_2).reset_index()\n",
    "# features = features[~features.isna().any(axis = 1)]\n",
    "\n",
    "# #########################################    STANDARDIZE FEATURES    #########################################    \n",
    "# features = features.set_index(drop_cols) \n",
    "# features_scaled = StandardScaler().fit_transform(features.values)\n",
    "# features = pd.DataFrame(features_scaled, index=features.index).reset_index()\n",
    "\n",
    "# #########################################     CALCULATE ANOMALY   #########################################  \n",
    "# features['yield_mt'] = np.log10(features.yield_mt + 1)\n",
    "# features.set_index(['year', 'district'], inplace=True)\n",
    "# var_cols = features.columns\n",
    "# features = features[var_cols] - features.groupby(['district'], as_index=True)[var_cols].transform('mean')\n",
    "# features.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# #########################################     K-FOLD SPLIT    #########################################    \n",
    "# x_all = features.drop(drop_cols, axis=1)\n",
    "# y_all = features.yield_mt\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "# #########################################     K-FOLD CV    ###########################################\n",
    "# kfold = KFold()\n",
    "# ridge = Ridge()\n",
    "# ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "# kfold_ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "# kfold_ridge_reg.fit(x_train, y_train)\n",
    "# kfold_best_model = kfold_ridge_reg.best_estimator_\n",
    "# ### VALIDATION PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "# kfold_val_predictions = cross_val_predict(kfold_best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "# ### TRAIN AND TEST PREDICT\n",
    "# y_pred_train_k = kfold_best_model.predict(x_train)\n",
    "# y_pred_test_k  = kfold_best_model.predict(x_test)\n",
    "\n",
    "# #########################################     LOGO SPLIT   ###########################################\n",
    "# x_train_g = features[features.year < max(features.year)].drop(drop_cols, axis=1)\n",
    "# y_train_g = features[features.year < max(features.year)].yield_mt\n",
    "# g_train_g = features[features.year < max(features.year)].year.ravel()\n",
    "\n",
    "# x_test_g = features[features.year == max(features.year)].drop(drop_cols, axis=1)\n",
    "# y_test_g = features[features.year == max(features.year)].yield_mt\n",
    "# g_test_g = features[features.year == max(features.year)].year\n",
    "\n",
    "# #########################################     LOGO CV    ###########################################\n",
    "# logo = LeaveOneGroupOut()\n",
    "# ridge = Ridge()\n",
    "# ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "# logo_ridge_reg = GridSearchCV(ridge, alphas, scoring='r2', cv=logo)\n",
    "# logo_ridge_reg.fit(x_train_g, y_train_g, groups=g_train_g)\n",
    "# logo_best_model = logo_ridge_reg.best_estimator_\n",
    "# ### VALIDATION PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "# logo_val_predictions = cross_val_predict(logo_best_model, X=x_train_g, y=y_train_g, groups=g_train_g, cv=logo)   \n",
    "# ### TRAIN AND TEST PREDICT\n",
    "# logo_train_pred = logo_best_model.predict(x_train_g)\n",
    "# logo_test_pred  = logo_best_model.predict(x_test_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd7b198-d7b6-4c90-8c4e-4753eeab721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_score(y_train_g, logo_val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a2ccf-7a5d-42c9-97bf-aca1de326079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_score(y_test_g, logo_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "456197bb-fbbc-4930-8141-ce719e24bdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 22min 33s, sys: 1h 8min 22s, total: 1h 30min 55s\n",
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# #########################################     SET PARAMS    #########################################    \n",
    "# f1 = 'landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_20k-points_1000-features_yr-2013-2021_mn-4-9_lm-True_cm-True_wa-True_summary.feather'\n",
    "# f2 = 'sentinel-2-l2a_bands-2-3-4_ZMB_4k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-True_summary.feather'\n",
    "\n",
    "# satellite1, bands1, country_code, points1, yrs1, mns1,\\\n",
    "# num_features1, limit_months1, crop_mask1, weighted_avg1 = split_fn(f1)\n",
    "\n",
    "# satellite2, bands2, country_code, points2, yrs2, mns2,\\\n",
    "# num_features2, limit_months2, crop_mask2, weighted_avg2 = split_fn(f2)\n",
    "\n",
    "# alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "# kfold = KFold()\n",
    "# logo = LeaveOneGroupOut()\n",
    "# ridge = Ridge()    \n",
    "\n",
    "# #########################################     READ DATA    #########################################\n",
    "# features_1 = pd.read_feather(here('data', 'random_features', 'summary', f1))\n",
    "# features_2 = pd.read_feather(here('data', 'random_features', 'summary', f2))\n",
    "\n",
    "# #########################################     CLEAN DATA    #########################################  \n",
    "# min_year = max(min(features_1.year), min(features_2.year))\n",
    "# max_year = min(max(features_1.year), max(features_2.year))\n",
    "\n",
    "# features_1 = features_1[features_1.year >= min_year]\n",
    "# features_2 = features_2[features_2.year >= min_year]\n",
    "\n",
    "# features_1 = features_1[features_1.year <= max_year]\n",
    "# features_2 = features_2[features_2.year <= max_year]\n",
    "\n",
    "# features_1.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "# features_2.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "# #########################################     JOIN DATA    #########################################  \n",
    "# drop_cols = ['district', 'year', 'yield_mt']\n",
    "\n",
    "# features_1 = features_1.set_index(drop_cols).add_prefix(\"f1_\")\n",
    "# features_2 = features_2.set_index(drop_cols).add_prefix(\"f2_\")\n",
    "\n",
    "# features = features_1.join(features_2).reset_index()\n",
    "# features = features[~features.isna().any(axis = 1)]\n",
    "\n",
    "# #########################################    STANDARDIZE FEATURES    #########################################    \n",
    "# features = features.set_index(drop_cols) \n",
    "# features_scaled = StandardScaler().fit_transform(features.values)\n",
    "# features = pd.DataFrame(features_scaled, index=features.index).reset_index()\n",
    "\n",
    "# #########################################     CALCULATE ANOMALY   #########################################  \n",
    "# features['yield_mt'] = np.log10(features.yield_mt + 1)\n",
    "# features.set_index(['year', 'district'], inplace=True)\n",
    "# var_cols = features.columns\n",
    "# features = features[var_cols] - features.groupby(['district'], as_index=True)[var_cols].transform('mean')\n",
    "# features.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# #########################################     K-FOLD SPLIT    #########################################    \n",
    "# x_all = features.drop(drop_cols, axis=1)\n",
    "# y_all = features.yield_mt\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "# #########################################     K-FOLD CV   ###########################################\n",
    "# ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "# kfold_ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "# kfold_ridge_reg.fit(x_train, y_train)\n",
    "# kfold_best_model = kfold_ridge_reg.best_estimator_\n",
    "# ### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "# kfold_val_predictions = cross_val_predict(kfold_best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "# y_pred_train_k = kfold_best_model.predict(x_train)\n",
    "# y_pred_test_k  = kfold_best_model.predict(x_test)\n",
    "\n",
    "# #########################################     LOGO ITERATOR   ###########################################\n",
    "# logo_val_results = []\n",
    "# logo_train_results = []\n",
    "# logo_test_results = []\n",
    "\n",
    "# for year in features.year.unique():\n",
    "# #########################################     LOGO SPLIT   ###########################################\n",
    "#     x_train_g = features[features.year != year].drop(drop_cols, axis=1)\n",
    "#     y_train_g = features[features.year != year].yield_mt.ravel()\n",
    "#     g_train_g = features[features.year != year].year.ravel()\n",
    "#     d_train_g = features[features.year != year].district.ravel()\n",
    "\n",
    "#     x_test_g = features[features.year == year].drop(drop_cols, axis=1)\n",
    "#     y_test_g = features[features.year == year].yield_mt.ravel()\n",
    "#     g_test_g = features[features.year == year].year.ravel()\n",
    "#     d_test_g = features[features.year == year].district.ravel()\n",
    "\n",
    "# #########################################     LOGO CV   ###########################################\n",
    "#     ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "#     logo_ridge_reg = GridSearchCV(ridge, alphas, scoring='r2', cv=logo)\n",
    "#     logo_ridge_reg.fit(x_train_g, y_train_g, groups=g_train_g)\n",
    "#     logo_best_model = logo_ridge_reg.best_estimator_\n",
    "#     ### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "#     logo_val_predictions = cross_val_predict(logo_best_model, X=x_train_g, y=y_train_g, groups=g_train_g, cv=logo) \n",
    "#     logo_train_pred = logo_best_model.predict(x_train_g)\n",
    "#     logo_test_pred  = logo_best_model.predict(x_test_g)\n",
    "\n",
    "# #########################################     LOGO RESULTS   ###########################################\n",
    "#     val_results = {'year': g_train_g, 'district': d_train_g, 'split': 'val', \n",
    "#                    'observed': y_train_g, 'predicted': logo_val_predictions}\n",
    "\n",
    "#     train_results = {'year': g_train_g, 'district': d_train_g,'split': 'train', \n",
    "#                      'observed': y_train_g, 'predicted': logo_train_pred}\n",
    "\n",
    "#     test_results = {'year': g_test_g, 'district': d_test_g, 'split': 'test', \n",
    "#                     'observed': y_test_g, 'predicted': logo_test_pred}\n",
    "\n",
    "#     logo_val_results.append(val_results)\n",
    "#     logo_train_results.append(train_results)\n",
    "#     logo_test_results.append(test_results)\n",
    "\n",
    "# #########################################     EXPLODE RESULTS   ###########################################\n",
    "# explode_cols = ['year', 'district', 'observed', 'predicted']\n",
    "# val_df   = pd.DataFrame(logo_val_results  ).explode(explode_cols) \n",
    "# train_df = pd.DataFrame(logo_train_results).explode(explode_cols) \n",
    "# test_df  = pd.DataFrame(logo_test_results ).explode(explode_cols)\n",
    "\n",
    "# group_cols = ['year', 'district', 'split']\n",
    "# val_summary   =   val_df.groupby(group_cols, as_index=False).mean()\n",
    "# train_summary = train_df.groupby(group_cols, as_index=False).mean()\n",
    "\n",
    "# #########################################     SAVE RESULTS    #########################################\n",
    "# d = {\n",
    "#     'country': country_code,\n",
    "\n",
    "#     'satellite_1'   : satellite1[0],\n",
    "#     'bands_1'       : bands1,\n",
    "#     'num_features_1': num_features1,\n",
    "#     'points_1'      : points1, \n",
    "#     'month_range_1' : mns1,\n",
    "#     'limit_months_1': limit_months1,\n",
    "#     'crop_mask_1'   : crop_mask1,\n",
    "#     'weighted_avg_1': weighted_avg1,\n",
    "\n",
    "#     'satellite_2'   : satellite2[0],\n",
    "#     'bands_2'       : bands2,\n",
    "#     'num_features_2': num_features2,\n",
    "#     'points_2'      : points2, \n",
    "#     'month_range_2' : mns2,\n",
    "#     'limit_months_2': limit_months2,\n",
    "#     'crop_mask_2'   : crop_mask2,\n",
    "#     'weighted_avg_2': weighted_avg2,\n",
    "\n",
    "#     'kfold_total_n': len(x_all),\n",
    "#     'kfold_train_n': len(x_train),\n",
    "#     'kfold_test_n' : len(x_test),\n",
    "\n",
    "#     'kfold_best_reg_param': list(kfold_ridge_reg.best_params_.values())[0],\n",
    "#     'kfold_mean_of_val_R2s': kfold_ridge_reg.best_score_,\n",
    "#     'kfold_val_R2': r2_score(y_train, kfold_val_predictions),\n",
    "#     'kfold_val_r' : pearsonr(kfold_val_predictions, y_train)[0],\n",
    "#     'kfold_val_r2': pearsonr(kfold_val_predictions, y_train)[0] ** 2,\n",
    "\n",
    "#     'kfold_train_R2': r2_score(y_train, y_pred_train_k),\n",
    "#     'kfold_train_r' : pearsonr(y_pred_train_k, y_train)[0],\n",
    "#     'kfold_train_r2': pearsonr(y_pred_train_k, y_train)[0] ** 2,\n",
    "\n",
    "#     'kfold_test_R2': r2_score(y_test, y_pred_test_k),\n",
    "#     'kfold_test_r' : pearsonr(y_pred_test_k, y_test)[0],\n",
    "#     'kfold_test_r2': pearsonr(y_pred_test_k, y_test)[0] ** 2,\n",
    "\n",
    "#     'logo_total_n': len(features),\n",
    "#     'logo_train_n': len(train_df),\n",
    "#     'logo_test_n' : len(test_df),    \n",
    "\n",
    "#     'logo_best_reg_param': list(logo_ridge_reg.best_params_.values())[0],      \n",
    "#     'logo_summary_val_R2': r2_score(val_summary.observed, val_summary.predicted),\n",
    "#     'logo_summary_val_r' : pearsonr(val_summary.observed, val_summary.predicted)[0],\n",
    "#     'logo_val_R2' : r2_score(val_df.observed, val_df.predicted),\n",
    "#     'logo_val_r'  : pearsonr(val_df.predicted, val_df.observed)[0],\n",
    "#     'logo_val_r2' : pearsonr(val_df.predicted, val_df.observed)[0] ** 2,\n",
    "\n",
    "#     'logo_summary_train_R2': r2_score(train_summary.observed, train_summary.predicted),\n",
    "#     'logo_summary_train_r' : pearsonr(train_summary.observed, train_summary.predicted)[0],\n",
    "#     'logo_train_R2': r2_score(train_df.observed, train_df.predicted),\n",
    "#     'logo_train_r' : pearsonr(train_df.predicted, train_df.observed)[0],\n",
    "#     'logo_train_r2': pearsonr(train_df.predicted, train_df.observed)[0] ** 2,\n",
    "\n",
    "#     'logo_test_R2': r2_score(test_df.observed, test_df.predicted),\n",
    "#     'logo_test_r' : pearsonr(test_df.predicted, test_df.observed)[0],\n",
    "#     'logo_test_r2': pearsonr(test_df.predicted, test_df.observed)[0] ** 2,\n",
    "# }\n",
    "# print('done')\n",
    "# df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9ea0b-e367-4be7-873a-a4eceb04dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'''Val: {r2_score(val_df.observed, val_df.predicted):0.2f}\n",
    "# Train: {r2_score(train_df.observed, train_df.predicted):0.2f}\n",
    "# Test: {r2_score(test_df.observed, test_df.predicted):0.2f}\n",
    "\n",
    "# Val summary: {r2_score(val_summary.observed, val_summary.predicted):0.2f}\n",
    "# Train summary: {r2_score(train_summary.observed, train_summary.predicted):0.2f}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b697a86-3bd0-454f-9c6d-0fe7232a96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for year in features.year.unique():\n",
    "#     a = val_df[val_df.year == year]\n",
    "#     print(f'{year}: {r2_score(a.observed, a.predicted)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb50d18-f879-4df1-9428-5e483339641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for year in features.year.unique():\n",
    "#     a = train_df[train_df.year == year]\n",
    "#     print(f'{year}: {r2_score(a.observed, a.predicted)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aca0393-6c74-48a5-baa3-da4b268ef9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for year in features.year.unique():\n",
    "#     a = test_df[test_df.year == year]\n",
    "#     print(f'{year}: {r2_score(a.observed, a.predicted)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e33cd86-b20e-4e2e-bb0d-2d6279818aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775f391-9511-4850-a649-de7424e70c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = x_train_g.copy()\n",
    "# test = x_test_g.copy()\n",
    "# train['logo_cv_prediction'] = logo_val_predictions\n",
    "# train['split'], test['split'] = 'train', 'test'\n",
    "# train_test = pd.concat([train, test])[['split','logo_cv_prediction']].sort_index()\n",
    "# train_test['district'] = features.district\n",
    "# train_test['year'] = features.year\n",
    "# train_test['yield_mt'] = y_all\n",
    "# train_test['logo_prediction'] = logo_best_model.predict(x_all)\n",
    "# train_test = train_test[['district', 'year', 'split', 'yield_mt', 'logo_prediction', 'logo_cv_prediction']]\n",
    "# train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b359b6-d751-4769-9bbd-0e857783367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# train_test[train_test.split == 'train'].plot.scatter(x = 'yield_mt', y = 'logo_cv_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de276b13-542a-41d3-8738-fe7474cd0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(train_test.yield_mt, train_test.logo_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527bb67-17bb-49cb-b863-6fa8db7f96f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test[train_test.split == 'train'].plot.scatter(x = 'yield_mt', y = 'logo_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c60814-0a95-4636-ba9c-6d58100fca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test[train_test.split == 'test'].plot.scatter(x = 'yield_mt', y = 'logo_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7e84a7-cb33-4c50-9d83-ec2f28299738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
