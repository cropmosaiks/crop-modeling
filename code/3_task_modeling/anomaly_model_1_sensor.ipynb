{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fe56655",
   "metadata": {},
   "source": [
    "# Modeling Crop Yield Anomaly\n",
    "## Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4f19f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import warnings\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from pyhere import here\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "\n",
    "import pyarrow\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import p_tqdm\n",
    "\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneGroupOut, cross_val_score, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr,  pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f2ea8a-24ef-4e3b-921e-b2fb1d0c0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(string):\n",
    "    return string.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "point_pattern = re.compile(\"20k-points\")\n",
    "wa_pattern = re.compile(\"cm-False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973b7ec8-bb19-4986-837d-2cb708c3e4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = here(\"data\")\n",
    "directory = here(\"data\", \"random_features\", \"summary\")\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "files = os.listdir(directory)\n",
    "files = [f for f in files if f not in ('.gitkeep', '.ipynb_checkpoints')]\n",
    "files = [f for f in files if not (bool(point_pattern.search(f)) & bool(wa_pattern.search(f)))]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d399b71-eba4-41b2-8e7a-54772c4ee185",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_df = pd.read_csv(here('data', 'crop_yield', 'cfs_maize_districts_zambia_2009_2022.csv'))\n",
    "crop_df = crop_df.set_index(['district', 'year'])[['log_yield_mt_anomaly']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f68a6df-443f-4fb3-977d-facfc6adac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(params):\n",
    "#########################################     SET PARAMS    #########################################\n",
    "    file         = params\n",
    "    f            = file.split(sep=\"_\")\n",
    "    satellite    = f[0]\n",
    "    bands        = f[1].replace(\"bands-\", \"\")\n",
    "    country_code = f[2]\n",
    "    points       = f[3].replace(\"k-points\", \"\")\n",
    "    num_features = f[4].replace(\"-features\", \"\")\n",
    "    yrs          = f[5].replace(\"yr-\", \"\").split(sep=\"-\")\n",
    "    mns          = f[6].replace(\"mn-\", \"\").split(sep=\"-\")\n",
    "    limit_months = str2bool(f[7].replace(\"lm-\", \"\"))\n",
    "    crop_mask    = str2bool(f[8].replace(\"cm-\", \"\"))\n",
    "    weighted_avg = str2bool(f[9].replace(\"wa-\", \"\"))\n",
    "    years        = range(int(yrs[0]), int(yrs[1])+1)\n",
    "    month_range  = list(range(int(mns[0]), int(mns[1])+1))\n",
    "    \n",
    "    alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "    kfold = KFold()\n",
    "    logo = LeaveOneGroupOut()\n",
    "    ridge = Ridge() \n",
    "#########################################     READ DATA    #########################################\n",
    "    fn = f\"{directory}/{file}\"\n",
    "    features = pd.read_feather(fn)\n",
    "     \n",
    "    drop_cols = ['district', 'year', 'yield_mt', 'crop_perc']\n",
    "            \n",
    "    if weighted_avg:\n",
    "        drop_cols.remove(\"crop_perc\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################\n",
    "    features['yield_mt'] = np.log10(features['yield_mt'] + 1)\n",
    "    features.set_index(['year', 'district'], inplace=True)\n",
    "    var_cols = features.columns\n",
    "    features = features[var_cols] - features.groupby(['district'], as_index=True)[var_cols].transform('mean')\n",
    "    features.reset_index(drop=False, inplace=True)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "    x_all = features.drop(drop_cols, axis = 1) \n",
    "    y_all = features.yield_mt\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "    \n",
    "#########################################     K-FOLD  CV   ###########################################\n",
    "    ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "    kfold_ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "    kfold_ridge_reg.fit(x_train, y_train)\n",
    "    kfold_best_model = kfold_ridge_reg.best_estimator_\n",
    "    ### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "    kfold_val_predictions = cross_val_predict(kfold_best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "    y_pred_train_k = kfold_best_model.predict(x_train)\n",
    "    y_pred_test_k  = kfold_best_model.predict(x_test)\n",
    "\n",
    "#########################################     LOGO ITERATOR   ###########################################\n",
    "    logo_val_results   = []\n",
    "    logo_train_results = []\n",
    "    logo_test_results  = []\n",
    "\n",
    "    for year in features.year.unique():\n",
    "#########################################     LOGO SPLIT   ###########################################\n",
    "        logo_x_train = features[features.year != year].drop(drop_cols, axis=1)\n",
    "        logo_y_train = features[features.year != year].yield_mt.ravel()\n",
    "        logo_g_train = features[features.year != year].year.ravel()\n",
    "        logo_d_train = features[features.year != year].district.ravel()\n",
    "\n",
    "        logo_x_test = features[features.year == year].drop(drop_cols, axis=1)\n",
    "        logo_y_test = features[features.year == year].yield_mt.ravel()\n",
    "        logo_g_test = features[features.year == year].year.ravel()\n",
    "        logo_d_test = features[features.year == year].district.ravel()\n",
    "\n",
    "#########################################     LOGO CV   ###########################################\n",
    "        ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "        logo_ridge_reg = GridSearchCV(ridge, alphas, scoring='r2', cv=logo)\n",
    "        logo_ridge_reg.fit(logo_x_train, logo_y_train, groups=logo_g_train)\n",
    "        logo_best_model = logo_ridge_reg.best_estimator_\n",
    "        ### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "        logo_val_predictions = cross_val_predict(logo_best_model, X=logo_x_train, y=logo_y_train, groups=logo_g_train, cv=logo) \n",
    "        logo_train_pred = logo_best_model.predict(logo_x_train)\n",
    "        logo_test_pred  = logo_best_model.predict(logo_x_test)\n",
    "\n",
    "#########################################     LOGO RESULTS   ###########################################\n",
    "        val_results   = {'year': logo_g_train, 'district': logo_d_train, 'split': 'val', \n",
    "                         'observed': logo_y_train, 'predicted': logo_val_predictions}\n",
    "\n",
    "        train_results = {'year': logo_g_train, 'district': logo_d_train,'split': 'train', \n",
    "                         'observed': logo_y_train, 'predicted': logo_train_pred}\n",
    "\n",
    "        test_results  = {'year': logo_g_test, 'district': logo_d_test, 'split': 'test', \n",
    "                         'observed': logo_y_test, 'predicted': logo_test_pred}\n",
    "        \n",
    "        logo_val_results.append(val_results)\n",
    "        logo_train_results.append(train_results)\n",
    "        logo_test_results.append(test_results)\n",
    "\n",
    "#########################################     EXPLODE RESULTS   ###########################################\n",
    "    explode_cols = ['year', 'district', 'observed', 'predicted']\n",
    "    val_df   = pd.DataFrame(logo_val_results  ).explode(explode_cols) \n",
    "    train_df = pd.DataFrame(logo_train_results).explode(explode_cols) \n",
    "    test_df  = pd.DataFrame(logo_test_results ).explode(explode_cols)\n",
    "    \n",
    "    group_cols = ['year', 'district', 'split']\n",
    "    val_summary   =   val_df.groupby(group_cols, as_index=False).mean()\n",
    "    train_summary = train_df.groupby(group_cols, as_index=False).mean()\n",
    "    \n",
    "#########################################     SAVE RESULTS    #########################################\n",
    "    d = {\n",
    "        'country': country_code,\n",
    "        'satellite': satellite,\n",
    "        'bands': bands,\n",
    "        'num_features': num_features,\n",
    "        'points': points, \n",
    "        'month_range': f'{min(month_range)}-{max(month_range)}',\n",
    "        \n",
    "        'limit_months': limit_months,\n",
    "        'crop_mask': crop_mask,\n",
    "        'weighted_avg': weighted_avg,\n",
    "        \n",
    "        'kfold_total_n': len(x_all),\n",
    "        'kfold_train_n': len(x_train),\n",
    "        'kfold_test_n' : len(x_test),\n",
    "        \n",
    "        'kfold_best_reg_param': list(kfold_ridge_reg.best_params_.values())[0],\n",
    "        'kfold_mean_of_val_R2s': kfold_ridge_reg.best_score_,\n",
    "        'kfold_val_R2': r2_score(y_train, kfold_val_predictions),\n",
    "        'kfold_val_r' : pearsonr(kfold_val_predictions, y_train)[0],\n",
    "        'kfold_val_r2': pearsonr(kfold_val_predictions, y_train)[0] ** 2,\n",
    "        \n",
    "        'kfold_train_R2': r2_score(y_train, y_pred_train_k),\n",
    "        'kfold_train_r' : pearsonr(y_pred_train_k, y_train)[0],\n",
    "        'kfold_train_r2': pearsonr(y_pred_train_k, y_train)[0] ** 2,\n",
    "        \n",
    "        'kfold_test_R2': r2_score(y_test, y_pred_test_k),\n",
    "        'kfold_test_r' : pearsonr(y_pred_test_k, y_test)[0],\n",
    "        'kfold_test_r2': pearsonr(y_pred_test_k, y_test)[0] ** 2,\n",
    "        \n",
    "        'logo_total_n': len(features),\n",
    "        'logo_train_n': len(train_df),\n",
    "        'logo_test_n' : len(test_df),    \n",
    "        \n",
    "        'logo_best_reg_param': list(logo_ridge_reg.best_params_.values())[0],      \n",
    "        'logo_summary_val_R2': r2_score(val_summary.observed, val_summary.predicted),\n",
    "        'logo_summary_val_r' : pearsonr(val_summary.observed, val_summary.predicted)[0],\n",
    "        'logo_val_R2' : r2_score(val_df.observed, val_df.predicted),\n",
    "        'logo_val_r'  : pearsonr(val_df.predicted, val_df.observed)[0],\n",
    "        'logo_val_r2' : pearsonr(val_df.predicted, val_df.observed)[0] ** 2,\n",
    "        \n",
    "        'logo_summary_train_R2': r2_score(train_summary.observed, train_summary.predicted),\n",
    "        'logo_summary_train_r' : pearsonr(train_summary.observed, train_summary.predicted)[0],\n",
    "        'logo_train_R2': r2_score(train_df.observed, train_df.predicted),\n",
    "        'logo_train_r' : pearsonr(train_df.predicted, train_df.observed)[0],\n",
    "        'logo_train_r2': pearsonr(train_df.predicted, train_df.observed)[0] ** 2,\n",
    "        \n",
    "        'logo_test_R2': r2_score(test_df.observed, test_df.predicted),\n",
    "        'logo_test_r' : pearsonr(test_df.predicted, test_df.observed)[0],\n",
    "        'logo_test_r2': pearsonr(test_df.predicted, test_df.observed)[0] ** 2,\n",
    "    }\n",
    "    return pd.DataFrame(data=d, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce70284-aaf3-4f51-bf3f-7c4cea88d40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e0490daf2e4d70a2b44ca21ea53fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time     \n",
    "##### With progress bar\n",
    "workers = 44 ## os.cpu_count()\n",
    "if __name__ == \"__main__\":\n",
    "    output = []\n",
    "    for result in p_tqdm.p_map(model, files, num_cpus=workers):\n",
    "        output.append(result)\n",
    "    results = pd.concat(output).reset_index(drop=True)\n",
    "    today = date.today().strftime(\"%Y-%m-%d\")\n",
    "    file_name = f'anomaly_results_{today}.csv'\n",
    "    print(f\"Saving results as: {file_name}\\n\\n\")           \n",
    "    results.to_csv(here(\"data\",\"results\", file_name), INDEX = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815fa43a-b710-4e26-b5c9-52feb958d29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
