{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6636a1d7-9bae-472b-915c-9b41b0a48a61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'alpha' parameter of MultiTaskElasticNet must be a float in the range [0, inf). Got [0.0001, 0.01] instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m X[train_idx], y[train_idx]\n\u001b[0;32m     32\u001b[0m X_val, y_val \u001b[38;5;241m=\u001b[39m X[val_idx], y[val_idx]\n\u001b[1;32m---> 33\u001b[0m \u001b[43menet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m score \u001b[38;5;241m=\u001b[39m enet\u001b[38;5;241m.\u001b[39mscore(X_val, y_val)\n\u001b[0;32m     35\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mosaiks-env\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:2371\u001b[0m, in \u001b[0;36mMultiTaskElasticNet.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   2347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m   2348\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit MultiTaskElasticNet model with coordinate descent.\u001b[39;00m\n\u001b[0;32m   2349\u001b[0m \n\u001b[0;32m   2350\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2369\u001b[0m \u001b[38;5;124;03m    initial data in memory directly using that format.\u001b[39;00m\n\u001b[0;32m   2370\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2371\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2373\u001b[0m     \u001b[38;5;66;03m# Need to validate separately here.\u001b[39;00m\n\u001b[0;32m   2374\u001b[0m     \u001b[38;5;66;03m# We can't pass multi_output=True because that would allow y to be csr.\u001b[39;00m\n\u001b[0;32m   2375\u001b[0m     check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m   2376\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32],\n\u001b[0;32m   2377\u001b[0m         order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2378\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept,\n\u001b[0;32m   2379\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mosaiks-env\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    574\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 581\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mosaiks-env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:97\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m     )\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'alpha' parameter of MultiTaskElasticNet must be a float in the range [0, inf). Got [0.0001, 0.01] instead."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import MultiTaskElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# generate some example data\n",
    "X = np.random.rand(100, 10)\n",
    "y = np.random.rand(100, 2)\n",
    "\n",
    "# define the range of alpha values to test for each subset\n",
    "alpha_range1 = np.logspace(-4, 4, 9)\n",
    "alpha_range2 = np.logspace(-2, 2, 5)\n",
    "\n",
    "# define the cross-validation scheme\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# define the MultiTaskElasticNet model\n",
    "enet = MultiTaskElasticNet()\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'alpha': [alpha_range1, alpha_range2], 'l1_ratio': [0.1, 0.5, 0.9]}\n",
    "\n",
    "# perform the grid search using cross-validation\n",
    "best_score = -np.inf\n",
    "for alpha1 in alpha_range1:\n",
    "    for alpha2 in alpha_range2:\n",
    "        for l1_ratio in param_grid['l1_ratio']:\n",
    "            enet.set_params(alpha=[alpha1, alpha2], l1_ratio=l1_ratio)\n",
    "            scores = []\n",
    "            for train_idx, val_idx in kf.split(X, y):\n",
    "                X_train, y_train = X[train_idx], y[train_idx]\n",
    "                X_val, y_val = X[val_idx], y[val_idx]\n",
    "                enet.fit(X_train, y_train)\n",
    "                score = enet.score(X_val, y_val)\n",
    "                scores.append(score)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = {'alpha1': alpha1, 'alpha2': alpha2, 'l1_ratio': l1_ratio}\n",
    "\n",
    "# print the best hyperparameters and the corresponding mean score\n",
    "print(\"Best alpha1 value:\", best_params['alpha1'])\n",
    "print(\"Best alpha2 value:\", best_params['alpha2'])\n",
    "print(\"Best l1_ratio value:\", best_params['l1_ratio'])\n",
    "print(\"Best mean score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a25f74b-2a82-4d6c-a17a-c06e2fd3f9b1",
   "metadata": {},
   "source": [
    "In this updated example, we define a range of gamma values to test (i.e., the hyperparameter for the second L2 penalty), and we add 'gamma' to the param_grid dictionary. We then use a nested for loop to iterate through all combinations of alpha and gamma.\n",
    "\n",
    "Inside the nested loop, we use Ridge.set_params() to set the hyperparameters of the Ridge Regression model, and we use KFold.split() to split the data into training and validation sets for cross-validation. We then fit the Ridge Regression model on the training set and calculate the validation score. We repeat this process for each fold of the cross-validation and take the mean of the validation scores as the final score for this combination of hyperparameters.\n",
    "\n",
    "Finally, we check if the current score is better than the previous best score, and if so, we update the best_score and best_params variables. At the end of the loop, we print the best hyperparameters and corresponding mean score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e8c7689-6696-44c0-85a1-ace58dfd6a53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (80, 2) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m X[train_idx], y[train_idx]\n\u001b[0;32m     34\u001b[0m X_val, y_val \u001b[38;5;241m=\u001b[39m X[val_idx], y[val_idx]\n\u001b[1;32m---> 35\u001b[0m \u001b[43mreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m score \u001b[38;5;241m=\u001b[39m reg\u001b[38;5;241m.\u001b[39mscore(X_val, y_val)\n\u001b[0;32m     37\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(score)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mosaiks-env\\lib\\site-packages\\glum\\_glm.py:2355\u001b[0m, in \u001b[0;36mGeneralizedLinearRegressor.fit\u001b[1;34m(self, X, y, sample_weight, offset, weights_sum)\u001b[0m\n\u001b[0;32m   2343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_hyperparameters()\n\u001b[0;32m   2345\u001b[0m \u001b[38;5;66;03m# NOTE: This function checks if all the entries in X and y are\u001b[39;00m\n\u001b[0;32m   2346\u001b[0m \u001b[38;5;66;03m# finite. That can be expensive. But probably worthwhile.\u001b[39;00m\n\u001b[0;32m   2347\u001b[0m (\n\u001b[0;32m   2348\u001b[0m     X,\n\u001b[0;32m   2349\u001b[0m     y,\n\u001b[0;32m   2350\u001b[0m     sample_weight,\n\u001b[0;32m   2351\u001b[0m     offset,\n\u001b[0;32m   2352\u001b[0m     weights_sum,\n\u001b[0;32m   2353\u001b[0m     P1,\n\u001b[0;32m   2354\u001b[0m     P2,\n\u001b[1;32m-> 2355\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_up_and_check_fit_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2357\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2358\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2359\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2360\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2363\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, tm\u001b[38;5;241m.\u001b[39mMatrixBase)\n\u001b[0;32m   2364\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, np\u001b[38;5;241m.\u001b[39mndarray)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mosaiks-env\\lib\\site-packages\\glum\\_glm.py:1829\u001b[0m, in \u001b[0;36mGeneralizedLinearRegressorBase._set_up_and_check_fit_args\u001b[1;34m(self, X, y, sample_weight, offset, solver, force_all_finite)\u001b[0m\n\u001b[0;32m   1827\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1828\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1829\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1831\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1833\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1837\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;66;03m# Without converting y to float, deviance might raise\u001b[39;00m\n\u001b[0;32m   1840\u001b[0m \u001b[38;5;66;03m# ValueError: Integers to negative integer powers are not allowed.\u001b[39;00m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;66;03m# Also, y must not be sparse.\u001b[39;00m\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;66;03m# Make sure everything has the same precision as X\u001b[39;00m\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;66;03m# This will prevent accidental upcasting later and slow operations on\u001b[39;00m\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;66;03m# mixed-precision numbers\u001b[39;00m\n\u001b[0;32m   1845\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mosaiks-env\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mosaiks-env\\lib\\site-packages\\sklearn\\utils\\validation.py:1122\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[1;32m-> 1122\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43m_check_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mosaiks-env\\lib\\site-packages\\sklearn\\utils\\validation.py:1143\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1142\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m-> 1143\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1144\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[0;32m   1145\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mosaiks-env\\lib\\site-packages\\sklearn\\utils\\validation.py:1202\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1193\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1194\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1195\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected. Please change the shape of y to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1198\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1199\u001b[0m         )\n\u001b[0;32m   1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(xp\u001b[38;5;241m.\u001b[39mreshape(y, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m-> 1202\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[0;32m   1204\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (80, 2) instead."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import glum\n",
    "from glum import GeneralizedLinearRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# generate some example data\n",
    "X = np.random.rand(100, 10)\n",
    "y = np.random.rand(100, 2)\n",
    "\n",
    "# define the range of gamma values to test for each subset\n",
    "gamma_range1 = np.logspace(-4, 4, 9)\n",
    "gamma_range2 = np.logspace(-2, 2, 5)\n",
    "\n",
    "# define the cross-validation scheme\n",
    "# cv = glum.CrossValidation(n_splits=5)\n",
    "cv = KFold(n_splits=5)\n",
    "\n",
    "# define the GeneralizedLinearRegressor model with ElasticNet penalty\n",
    "reg = GeneralizedLinearRegressor(P2=ElasticNet())\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'gamma': [gamma_range1, gamma_range2], 'l1_ratio': [0.1, 0.5, 0.9]}\n",
    "\n",
    "# perform the grid search using cross-validation\n",
    "best_score = -np.inf\n",
    "for gamma1 in gamma_range1:\n",
    "    for gamma2 in gamma_range2:\n",
    "        for l1_ratio in param_grid['l1_ratio']:\n",
    "            reg.penalty = ElasticNet(alpha=[gamma1, gamma2], l1_ratio=l1_ratio)\n",
    "            scores = []\n",
    "            for train_idx, val_idx in cv.split(X, y):\n",
    "                X_train, y_train = X[train_idx], y[train_idx]\n",
    "                X_val, y_val = X[val_idx], y[val_idx]\n",
    "                reg.fit(X_train, y_train)\n",
    "                score = reg.score(X_val, y_val)\n",
    "                scores.append(score)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_params = {'gamma1': gamma1, 'gamma2': gamma2, 'l1_ratio': l1_ratio}\n",
    "\n",
    "# print the best hyperparameters and the corresponding mean score\n",
    "print(\"Best gamma1 value:\", best_params['gamma1'])\n",
    "print(\"Best gamma2 value:\", best_params['gamma2'])\n",
    "print(\"Best l1_ratio value:\", best_params['l1_ratio'])\n",
    "print(\"Best mean score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a296a9b-04c6-478e-a01b-5fdcaaa15fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cross_validated_ridge_regression(X, y, n_folds, penalty_params_list):\n",
    "    \"\"\"\n",
    "    Performs cross-validated ridge regression with multiple L2 penalization parameters.\n",
    "    \n",
    "    Args:\n",
    "    - X (ndarray): A numpy array of shape (n_samples, n_features) containing the input data.\n",
    "    - y (ndarray): A numpy array of shape (n_samples, n_targets) containing the target data.\n",
    "    - n_folds (int): The number of folds to use in cross-validation.\n",
    "    - penalty_params_list (list of tuples): A list of tuples specifying the penalty parameters to use for each subset\n",
    "        of features. Each tuple should have two elements: a numpy array of boolean values indicating which features to\n",
    "        include, and a float specifying the corresponding penalty parameter for those features.\n",
    "    \n",
    "    Returns:\n",
    "    - best_params (dict): A dictionary containing the best penalty parameters found by cross-validation for each subset\n",
    "        of features. The keys are the indices of the subsets (starting from 0), and the values are the corresponding\n",
    "        penalty parameters.\n",
    "    - best_score (float): The mean cross-validation score achieved by the best set of penalty parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize variables\n",
    "    n_samples, n_features = X.shape\n",
    "    n_targets = y.shape[1]\n",
    "    scores = np.zeros((len(penalty_params_list), n_folds))\n",
    "    best_params = {}\n",
    "    best_score = -np.inf\n",
    "    \n",
    "    # create indices for cross-validation\n",
    "    fold_indices = np.repeat(np.arange(n_folds), n_samples // n_folds + 1)[:n_samples]\n",
    "    \n",
    "    # loop over the subsets of features and their corresponding penalty parameters\n",
    "    for i, (feature_indices, penalty_param) in enumerate(penalty_params_list):\n",
    "        \n",
    "        # create the design matrix for the current subset of features\n",
    "        X_subset = X[:, feature_indices]\n",
    "        \n",
    "        # compute the Gram matrix for ridge regression\n",
    "        G = np.dot(X_subset.T, X_subset) + penalty_param * np.eye(X_subset.shape[1])\n",
    "        \n",
    "        # loop over the folds\n",
    "        for fold in range(n_folds):\n",
    "            \n",
    "            # get the indices for the current fold\n",
    "            train_indices = (fold_indices != fold)\n",
    "            test_indices = (fold_indices == fold)\n",
    "            \n",
    "            # split the data into training and test sets\n",
    "            X_train = X_subset[train_indices, :]\n",
    "            y_train = y[train_indices, :]\n",
    "            X_test = X_subset[test_indices, :]\n",
    "            y_test = y[test_indices, :]\n",
    "            \n",
    "            # compute the ridge regression coefficients using the training set\n",
    "            w = np.linalg.solve(G, np.dot(X_train.T, y_train))\n",
    "            \n",
    "            # compute the mean squared error on the test set\n",
    "            y_pred = np.dot(X_test, w)\n",
    "            mse = np.mean((y_pred - y_test)**2)\n",
    "            \n",
    "            # store the score for this fold\n",
    "            scores[i, fold] = -mse\n",
    "        \n",
    "        # compute the mean score over all folds for this set of penalty parameters\n",
    "        mean_score = np.mean(scores[i, :])\n",
    "        \n",
    "        # update the best score and best parameters if necessary\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_params[i] = penalty_param\n",
    "    \n",
    "    return best_params, best_score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
