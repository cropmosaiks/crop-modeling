{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f576830d-9afa-47d8-9fac-8e43292ee090",
   "metadata": {},
   "source": [
    "# Modeling Crop Yield: Landsat + Sentinel\n",
    "## Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44646286-2094-4bd0-8609-ad5efc857abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import warnings\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from pyhere import here\n",
    "from datetime import date\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import pickle\n",
    "\n",
    "import pyarrow\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import p_tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneGroupOut, cross_val_score, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr,  pearsonr\n",
    "\n",
    "from task_modeling_utils import *\n",
    "from prediction_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16825434-7870-4757-89e4-e3d1b2cf9ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1892"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = here(\"data\", \"random_features\", \"summary\")\n",
    "files = os.listdir(directory)\n",
    "files = list(f for f in files if f not in ('.gitkeep', '.ipynb_checkpoints'))\n",
    "paramlist = list(itertools.combinations(files, 2))\n",
    "paramlist = list(itertools.product(paramlist, [True, False]))\n",
    "paramlist = list(tuple(merge(paramlist[i])) for i in range(len(paramlist)))\n",
    "# paramlist = paramlist[0:249]\n",
    "# paramlist = paramlist[250:499]\n",
    "# paramlist = paramlist[500:749]\n",
    "# paramlist = paramlist[750:999]\n",
    "# paramlist = paramlist[1000:1249]\n",
    "# paramlist = paramlist[1250:1499]\n",
    "# paramlist = paramlist[1500:1749]\n",
    "# paramlist = paramlist[1750:1892]\n",
    "len(paramlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aabd25a-a0c4-4f29-9233-2f0f6f133d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "if i == 1:\n",
    "    paramlist = paramlist[0:249]\n",
    "elif i == 2:\n",
    "    paramlist = paramlist[250:499]\n",
    "elif i == 3:\n",
    "    paramlist = paramlist[500:749]\n",
    "elif i == 4:\n",
    "    paramlist = paramlist[750:999]\n",
    "elif i == 5:\n",
    "    paramlist = paramlist[1000:1249]\n",
    "elif i == 6:\n",
    "    paramlist = paramlist[1250:1499]\n",
    "elif i == 7:\n",
    "    paramlist = paramlist[1500:1749]\n",
    "elif i == 8:\n",
    "    paramlist = paramlist[1750:1892]\n",
    "len(paramlist)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae5e44c6-6764-431c-a369-3ea976fc1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_2_sensor(params):\n",
    "#########################################     SET PARAMS    #########################################    \n",
    "    f1         = params[0]\n",
    "    f2         = params[1]\n",
    "    hot_encode = params[2]\n",
    "\n",
    "    satellite1, bands1, country_code, points1, yrs1, mns1,\\\n",
    "    num_features1, limit_months1, crop_mask1, weighted_avg1 = split_fn(f1)\n",
    "\n",
    "    satellite2, bands2, country_code, points2, yrs2, mns2,\\\n",
    "    num_features2, limit_months2, crop_mask2, weighted_avg2 = split_fn(f2)\n",
    "\n",
    "#########################################     READ DATA    #########################################\n",
    "    features_1 = pd.read_feather(here('data', 'random_features', 'summary', f1))\n",
    "    features_2 = pd.read_feather(here('data', 'random_features', 'summary', f2))\n",
    "    climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "\n",
    "#########################################     CLEAN DATA    #########################################  \n",
    "    min_year = max(min(features_1.year), min(features_2.year))\n",
    "    max_year = min(max(features_1.year), max(features_2.year))\n",
    "\n",
    "    features_1 = features_1[features_1.year >= min_year]\n",
    "    features_2 = features_2[features_2.year >= min_year]\n",
    "\n",
    "    features_1 = features_1[features_1.year <= max_year]\n",
    "    features_2 = features_2[features_2.year <= max_year]\n",
    "\n",
    "    features_1.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "    features_2.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "#########################################     JOIN FEATURES    #########################################  \n",
    "    drop_cols = ['district', 'year', 'yield_mt']\n",
    "\n",
    "    features_1 = features_1.set_index(drop_cols).add_prefix(\"f1_\")\n",
    "    features_2 = features_2.set_index(drop_cols).add_prefix(\"f2_\")\n",
    "\n",
    "    features = features_1.join(features_2).reset_index()\n",
    "    features = features[~features.isna().any(axis = 1)]\n",
    "\n",
    "#########################################    JOIN CLIMATE VARS    ######################################### \n",
    "    ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains('ndvi')]\n",
    "    keep_cols = [*ndvi_cols, *drop_cols]\n",
    "    climate_df = climate_df.loc[:, keep_cols]\n",
    "\n",
    "    features = features.set_index(drop_cols).join(climate_df.set_index(drop_cols)).reset_index()\n",
    "    features = features[features.year <= max(climate_df.year)]\n",
    "\n",
    "    crop_yield = features.copy().loc[:, tuple(drop_cols)]\n",
    "    crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    HOT ENCODE    ######################################### \n",
    "    if hot_encode:\n",
    "        drop_cols.remove('district')\n",
    "        features = pd.get_dummies(features, columns = [\"district\"], drop_first = False)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "    features = features.set_index(drop_cols) \n",
    "    features_scaled = StandardScaler().fit_transform(features.values)\n",
    "    features = pd.DataFrame(features_scaled, index=features.index).reset_index()\n",
    "    features.columns = features.columns.astype(str)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "    x_all = features.drop(drop_cols, axis = 1) \n",
    "    y_all = np.log10(features.yield_mt.to_numpy() + 1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "#########################################     K-FOLD CV   ###########################################\n",
    "    ### SETUP\n",
    "    alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "    kfold = KFold()\n",
    "    ridge = Ridge()    \n",
    "    ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "    ridge_reg = GridSearchCV(ridge, alphas, scoring = 'r2', cv = kfold)\n",
    "    ridge_reg.fit(x_train, y_train)\n",
    "    best_model = ridge_reg.best_estimator_\n",
    "    ### PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "    val_predictions = cross_val_predict(best_model, X = x_train, y = y_train, cv = kfold)   \n",
    "    train_predictions = best_model.predict(x_train)\n",
    "    test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################    \n",
    "    crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "    train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "    train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "    train_split['cv_prediction'] = np.maximum(val_predictions, 0)\n",
    "    train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"]-train_split.groupby('district')['log_yield'].transform('mean')\n",
    "    train_split[\"demean_cv_prediction\"] = train_split[\"cv_prediction\"]-train_split.groupby('district')['cv_prediction'].transform('mean')\n",
    "\n",
    "    test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "    test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "    test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "    test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "    test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "    predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "#########################################     SAVE MODELS   #########################################  \n",
    "    # fn_1 = f'{satellite1[0]}_{bands1}_{points1}_{limit_months1}_{crop_mask1}_{weighted_avg1}'\n",
    "    # fn_2 = f'{satellite2[0]}_{bands2}_{points2}_{limit_months2}_{crop_mask2}_{weighted_avg2}'\n",
    "    # model_fn_suffix = f'fn-1_{fn_1}_fn-2_{fn_2}'\n",
    "    # k_model_fn = f'kfold-cv_rr-model_{model_fn_suffix}_he-{hot_encode}.pkl'\n",
    "\n",
    "    # with open(here('models', k_model_fn),'wb') as f:\n",
    "    #     pickle.dump(best_model, f)\n",
    "\n",
    "#########################################     SAVE RESULTS    #########################################\n",
    "    d = {\n",
    "        'country': country_code,\n",
    "\n",
    "        'satellite_1'   : satellite1[0],\n",
    "        'bands_1'       : bands1,\n",
    "        'num_features_1': num_features1,\n",
    "        'points_1'      : points1, \n",
    "        'month_range_1' : mns1,\n",
    "        'limit_months_1': limit_months1,\n",
    "        'crop_mask_1'   : crop_mask1,\n",
    "        'weighted_avg_1': weighted_avg1,\n",
    "\n",
    "        'satellite_2'   : satellite2[0],\n",
    "        'bands_2'       : bands2,\n",
    "        'num_features_2': num_features2,\n",
    "        'points_2'      : points2, \n",
    "        'month_range_2' : mns2,\n",
    "        'limit_months_2': limit_months2,\n",
    "        'crop_mask_2'   : crop_mask2,\n",
    "        'weighted_avg_2': weighted_avg2,\n",
    "\n",
    "        'hot_encode': hot_encode,\n",
    "\n",
    "        'total_n': len(x_all),\n",
    "        'train_n': len(x_train),\n",
    "        'test_n' : len(x_test),\n",
    "\n",
    "        'best_reg_param': list(ridge_reg.best_params_.values())[0],\n",
    "        'mean_of_val_R2': ridge_reg.best_score_,\n",
    "        'val_R2': r2_score(y_train, val_predictions),\n",
    "        'val_r' : pearsonr(val_predictions, y_train)[0],\n",
    "        'val_r2': pearsonr(val_predictions, y_train)[0] ** 2,\n",
    "\n",
    "        'train_R2': r2_score(y_train, train_predictions),\n",
    "        'train_r' : pearsonr(train_predictions, y_train)[0],\n",
    "        'train_r2': pearsonr(train_predictions, y_train)[0] ** 2,\n",
    "\n",
    "        'test_R2': r2_score(y_test, test_predictions),\n",
    "        'test_r' : pearsonr(test_predictions, y_test)[0],\n",
    "        'test_r2': pearsonr(test_predictions, y_test)[0] ** 2,\n",
    "\n",
    "        'demean_cv_R2': r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction),\n",
    "        'demean_cv_r':  pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0],\n",
    "        'demean_cv_r2': pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0] ** 2,\n",
    "    }\n",
    "    return pd.DataFrame(data=d, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f752cfdc-7cca-4ce2-9841-ac6118f0bbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time     \n",
    "# ##### With progress bar\n",
    "# workers = os.cpu_count()\n",
    "# if __name__ == \"__main__\":\n",
    "#     output = []\n",
    "#     for result in p_tqdm.p_map(model_2_sensor, paramlist):\n",
    "#         output.append(result)\n",
    "#     results = pd.concat(output).reset_index(drop=True)\n",
    "#     today = date.today().strftime(\"%Y-%m-%d\")\n",
    "#     file_name = f'2_sensor_results_{today}.csv'\n",
    "#     print(f\"Saving results as: {file_name}\\n\\n\")           \n",
    "#     results.to_csv(here(\"data\",\"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae1de936-ecb5-4d64-adf3-dc778656b05c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 766 ms\n",
      "Wall time: 913 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## TESTING  \n",
    "f1 = 'landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_15k-points_1000-features_yr-2013-2021_mn-4-9_lm-True_cm-False_wa-False_summary.feather'\n",
    "f2 = 'sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather'\n",
    "hot_encode = False\n",
    "\n",
    "#########################################     SET PARAMS    #########################################    \n",
    "# f1         = params[0]\n",
    "# f2         = params[1]\n",
    "# hot_encode = params[2]\n",
    "\n",
    "satellite1, bands1, country_code, points1, yrs1, mns1,\\\n",
    "num_features1, limit_months1, crop_mask1, weighted_avg1 = split_fn(f1)\n",
    "\n",
    "satellite2, bands2, country_code, points2, yrs2, mns2,\\\n",
    "num_features2, limit_months2, crop_mask2, weighted_avg2 = split_fn(f2)\n",
    "\n",
    "#########################################     READ DATA    #########################################\n",
    "features_1 = pd.read_feather(here('data', 'random_features', 'summary', f1))\n",
    "features_2 = pd.read_feather(here('data', 'random_features', 'summary', f2))\n",
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "\n",
    "#########################################     CLEAN DATA    #########################################  \n",
    "min_year = max(min(features_1.year), min(features_2.year))\n",
    "max_year = min(max(features_1.year), max(features_2.year))\n",
    "\n",
    "features_1 = features_1[features_1.year >= min_year]\n",
    "features_2 = features_2[features_2.year >= min_year]\n",
    "\n",
    "features_1 = features_1[features_1.year <= max_year]\n",
    "features_2 = features_2[features_2.year <= max_year]\n",
    "\n",
    "features_1.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "features_2.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "#########################################     JOIN FEATURES    #########################################  \n",
    "drop_cols = ['district', 'year', 'yield_mt']\n",
    "\n",
    "features_1 = features_1.set_index(drop_cols).add_prefix(\"f1_\")\n",
    "features_2 = features_2.set_index(drop_cols).add_prefix(\"f2_\")\n",
    "\n",
    "features = features_1.join(features_2).reset_index()\n",
    "features = features[~features.isna().any(axis = 1)]\n",
    "\n",
    "n_districts = len(features.district.unique())\n",
    "\n",
    "crop_yield = features.copy().loc[:, tuple(drop_cols)]\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    HOT ENCODE    ######################################### \n",
    "if hot_encode:\n",
    "    drop_cols.remove('district')\n",
    "    features = pd.get_dummies(features, columns = [\"district\"], drop_first = False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "features = features.set_index(drop_cols) \n",
    "features_scaled = StandardScaler().fit_transform(features.values)\n",
    "features = pd.DataFrame(features_scaled, index=features.index).reset_index()\n",
    "features.columns = features.columns.astype(str)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = features.drop(drop_cols, axis = 1) \n",
    "y_all = np.log10(features.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80eef660-804e-43b2-82cc-478d5ea40619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 6000), (18000, 18000))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0, features_1.shape[1]), (features_1.shape[1] + features_2.shape[1] , x_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d5a3c4b-b9b0-438e-b700-bf7d233c3781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0, 6000), (6000, 18000))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0, features_1.shape[1]), (features_1.shape[1], features_1.shape[1] + features_2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93888e2a-cb1d-43f7-a892-d18398547780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6000, 18000), (18000, 18000))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "( features_1.shape[1], features_1.shape[1] + features_2.shape[1] ), ( features_1.shape[1] + features_2.shape[1], x_train.shape[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f53a1bd-c290-4907-9e5f-f0e21ac21881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cullen\\anaconda3\\envs\\mosaiks-env\\lib\\site-packages\\numpy\\core\\shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best λ 1: 0.01\n",
      "    Val R2: 0.6450  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cullen\\anaconda3\\envs\\mosaiks-env\\lib\\site-packages\\numpy\\core\\shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best λ 2: 0.01\n",
      "    Val R2: 0.6450  \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_alpha_3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:119\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'best_alpha_3' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "solver_kwargs = {\n",
    "    \"X\": x_train,\n",
    "    \"y\": y_train,\n",
    "    \"locations\": x_train.index,\n",
    "    \"split_col\": x_train.reset_index().index,\n",
    "    \"lambdas\": np.logspace(-8, 8, base = 10, num = 17), \n",
    "    \"return_preds\": True,\n",
    "    \"return_model\": False,\n",
    "    \"svd_solve\": False,\n",
    "    \"allow_linalg_warning_instances\": True,\n",
    "    \"fit_model_after_tuning\": False,\n",
    "    \"intercept\": True,\n",
    "    \"num_folds\": 5,\n",
    "    \"random_state\": 0,\n",
    "}\n",
    "end_1 = features_1.shape[1]\n",
    "end_2 = features_2.shape[1] + end_1 \n",
    "end_all = x_train.shape[1]\n",
    "\n",
    "if hot_encode:\n",
    "    static_lam_idxs_1 = list(range(end_1, end_all))\n",
    "    static_lam_idxs_2 = [list(range(0, end_1)), list(range(end_2 , end_all))]\n",
    "    static_lam_idxs_3 = [list(range(0, end_1)), list(range(end_1, end_2))]\n",
    "    static_lam_idxs   = [list(range(end_1, end_2)), list(range(end_hot_encodeend_all))]\n",
    "    \n",
    "    kfold_results = kfold_solve_custom_split_col(\n",
    "        static_lam_val=0.01,\n",
    "        static_lam_idxs=static_lam_idxs_1,\n",
    "        **solver_kwargs\n",
    "    )\n",
    "    best_alpha_1_idx = interpret_kfold_results(kfold_results, \"r2_score\")[0][0][0]\n",
    "    best_alpha_1 = solver_kwargs.get(\"lambdas\")[best_alpha_1_idx]\n",
    "    preds = np.maximum(get_pred_truth_locs(kfold_results)[0].flatten(), 0)\n",
    "    truth = get_pred_truth_locs(kfold_results)[1].flatten()\n",
    "    print(\n",
    "    f\"\"\"Best \\u03BB 1: {best_alpha_1}\n",
    "    Val R2: {r2_score(truth, preds):0.4f}  \\n\"\"\"\n",
    "    )\n",
    "    kfold_results = kfold_solve_custom_split_col(\n",
    "        static_lam_val=[best_alpha_1, 0.01],\n",
    "        static_lam_idxs=static_lam_idxs_2,\n",
    "        **solver_kwargs\n",
    "    )\n",
    "    best_alpha_2_idx = interpret_kfold_results(kfold_results, \"r2_score\")[0][0][0]\n",
    "    best_alpha_2 = solver_kwargs.get(\"lambdas\")[best_alpha_2_idx]\n",
    "    preds = np.maximum(get_pred_truth_locs(kfold_results)[0].flatten(), 0)\n",
    "    truth = get_pred_truth_locs(kfold_results)[1].flatten()\n",
    "    print(\n",
    "    f\"\"\"Best \\u03BB 2: {best_alpha_2}\n",
    "    Val R2: {r2_score(truth, preds):0.4f}  \\n\"\"\"\n",
    "    )\n",
    "    kfold_results = kfold_solve_custom_split_col(\n",
    "        static_lam_val=[best_alpha_1, best_alpha_2],\n",
    "        static_lam_idxs=static_lam_idxs_3,\n",
    "        **solver_kwargs\n",
    "    )\n",
    "    best_alpha_3_idx = interpret_kfold_results(kfold_results, \"r2_score\")[0][0][0]\n",
    "    best_alpha_3 = solver_kwargs.get(\"lambdas\")[best_alpha_3_idx]\n",
    "    preds = np.maximum(get_pred_truth_locs(kfold_results)[0].flatten(), 0)\n",
    "    truth = get_pred_truth_locs(kfold_results)[1].flatten()\n",
    "    print(f\"Best \\u03BB 3: {best_alpha_3}\\n\")\n",
    "    model, intercept_term = custom_ridge(\n",
    "        X=x_train,\n",
    "        y=y_train,\n",
    "        lam=best_alpha_1, \n",
    "        intercept=True,\n",
    "        static_lam_val=[best_alpha_2, best_alpha_3], \n",
    "        static_lam_idxs=static_lam_idxs\n",
    "    )\n",
    "    pred_test = np.asarray(x_test).dot(model) + intercept_term \n",
    "    pred_test = np.maximum(pred_test, 0)\n",
    "    print(\n",
    "    f\"\"\"Best \\u03BB's': {[best_alpha_1, best_alpha_2, best_alpha_3]}\n",
    "    Final Val R2: {r2_score(truth, preds):0.4f}  \n",
    "    Test R2: {r2_score(y_test, pred_test):0.4f}\\n\"\"\"\n",
    "    )\n",
    "else:\n",
    "    static_lam_idxs_1 = list(range(end_1, end_all))\n",
    "    static_lam_idxs_2 = list(range(0, end_1))\n",
    "    \n",
    "    kfold_results = kfold_solve_custom_split_col(\n",
    "        static_lam_val=0.01,\n",
    "        static_lam_idxs=static_lam_idxs_1,\n",
    "        **solver_kwargs\n",
    "    )\n",
    "    best_alpha_1_idx = interpret_kfold_results(kfold_results, \"r2_score\")[0][0][0]\n",
    "    best_alpha_1 = solver_kwargs.get(\"lambdas\")[best_alpha_1_idx]\n",
    "    preds = np.maximum(get_pred_truth_locs(kfold_results)[0].flatten(), 0)\n",
    "    truth = get_pred_truth_locs(kfold_results)[1].flatten()\n",
    "    print(\n",
    "    f\"\"\"Best \\u03BB 1: {best_alpha_1}\n",
    "    Val R2: {r2_score(truth, preds):0.4f}  \\n\"\"\"\n",
    "    )\n",
    "    kfold_results = kfold_solve_custom_split_col(\n",
    "        static_lam_val=best_alpha_1,\n",
    "        static_lam_idxs=static_lam_idxs_2,\n",
    "        **solver_kwargs\n",
    "    )\n",
    "    best_alpha_2_idx = interpret_kfold_results(kfold_results, \"r2_score\")[0][0][0]\n",
    "    best_alpha_2 = solver_kwargs.get(\"lambdas\")[best_alpha_2_idx]\n",
    "    preds = np.maximum(get_pred_truth_locs(kfold_results)[0].flatten(), 0)\n",
    "    truth = get_pred_truth_locs(kfold_results)[1].flatten()\n",
    "    print(\n",
    "    f\"\"\"Best \\u03BB 2: {best_alpha_2}\n",
    "    Val R2: {r2_score(truth, preds):0.4f}  \\n\"\"\"\n",
    "    )\n",
    "    \n",
    "    model, intercept_term = custom_ridge(\n",
    "        X=x_train,\n",
    "        y=y_train,\n",
    "        lam=best_alpha_2, \n",
    "        intercept=True,\n",
    "        static_lam_val=best_alpha_1, \n",
    "        static_lam_idxs=static_lam_idxs_2\n",
    "    )\n",
    "    pred_test = np.asarray(x_test).dot(model) + intercept_term \n",
    "    pred_test = np.maximum(pred_test, 0)\n",
    "    print(\n",
    "    f\"\"\"Best \\u03BB's': {[best_alpha_1, best_alpha_2, best_alpha_3]}\n",
    "    Final Val R2: {r2_score(truth, preds):0.4f}  \n",
    "    Test R2: {r2_score(y_test, pred_test):0.4f}\\n\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8a5388-c2ba-47f4-9210-13b449d990c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d5d676c-94ab-49d9-96e9-b57601099056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cullen\\anaconda3\\envs\\mosaiks-env\\lib\\site-packages\\numpy\\core\\shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best λ 1: 0.01\n",
      "Val R2: 0.6450  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cullen\\anaconda3\\envs\\mosaiks-env\\lib\\site-packages\\numpy\\core\\shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best λ 2: 0.01\n",
      "Val R2: 0.6450  \n",
      "\n",
      "The optimal hyperparameter is the lowest of the acceptable (i.e. no precision warnings) hyperparameters supplied. It is index 0 of the orignal hyperparamters passed in. For reference, 17 of 17 hyperparamters are considered acceptable; their indices  are [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16].\n",
      "The optimal hyperparameter is the highest of the acceptable (i.e. no precision warnings) hyperparameters supplied. It is index 0 of the orignal hyperparamters passed in. For reference, 17 of 17 hyperparameters are considered acceptable; their indices  are [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16].\n",
      "Multiple optimal hyperparameters found for outcome 0. Indices: [[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]]\n",
      "The optimal hyperparameter is the lowest of the acceptable (i.e. no precision warnings) hyperparameters supplied. It is index 0 of the orignal hyperparamters passed in. For reference, 17 of 17 hyperparamters are considered acceptable; their indices  are [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16].\n",
      "The optimal hyperparameter is the highest of the acceptable (i.e. no precision warnings) hyperparameters supplied. It is index 0 of the orignal hyperparamters passed in. For reference, 17 of 17 hyperparameters are considered acceptable; their indices  are [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16].\n",
      "Multiple optimal hyperparameters found for outcome 0. Indices: [[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]]\n",
      "The optimal hyperparameter is the lowest of the acceptable (i.e. no precision warnings) hyperparameters supplied. It is index 0 of the orignal hyperparamters passed in. For reference, 17 of 17 hyperparamters are considered acceptable; their indices  are [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16].\n",
      "The optimal hyperparameter is the highest of the acceptable (i.e. no precision warnings) hyperparameters supplied. It is index 0 of the orignal hyperparamters passed in. For reference, 17 of 17 hyperparameters are considered acceptable; their indices  are [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16].\n",
      "Multiple optimal hyperparameters found for outcome 0. Indices: [[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]]\n",
      "Best λ 3: 1e-08\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cullen\\anaconda3\\envs\\mosaiks-env\\lib\\site-packages\\numpy\\core\\shape_base.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  ary = asanyarray(ary)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best λ's': [0.01, 0.01, 1e-08]\n",
      "Final Val R2: 0.6450  \n",
      "Test R2: 0.5442\n",
      "\n",
      "CPU times: total: 7h 18min 5s\n",
      "Wall time: 1h 57min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "solver_kwargs = {\n",
    "    \"X\": x_train,\n",
    "    \"y\": y_train,\n",
    "    \"locations\": x_train.index,\n",
    "    \"split_col\": x_train.reset_index().index,\n",
    "    \"lambdas\": np.logspace(-8, 8, base = 10, num = 17), \n",
    "    \"return_preds\": True,\n",
    "    \"return_model\": False,\n",
    "    \"svd_solve\": False,\n",
    "    \"allow_linalg_warning_instances\": True,\n",
    "    \"fit_model_after_tuning\": False,\n",
    "    \"intercept\": True,\n",
    "    \"num_folds\": 5,\n",
    "    \"random_state\": 0,\n",
    "}\n",
    "\n",
    "end_1 = features_1.shape[1]\n",
    "end_2 = features_1.shape[1] + features_2.shape[1]\n",
    "end_all = x_train.shape[1]\n",
    "\n",
    "static_lam_idxs_1 = list(range(end_1, end_all))\n",
    "static_lam_idxs_2 = [list(range(0, end_1)), list(range(end_2 , end_all))]\n",
    "static_lam_idxs_3 = [list(range(0, end_1)), list(range(end_1, end_2))]\n",
    "static_lam_idxs   = [list(range(end_1, end_2)), list(range(end_2, end_all))]\n",
    "\n",
    "kfold_results = kfold_solve_custom_split_col(\n",
    "    static_lam_val=0.01,\n",
    "    static_lam_idxs=static_lam_idxs_1,\n",
    "    **solver_kwargs\n",
    ")\n",
    "best_alpha_1_idx = interpret_kfold_results(kfold_results, \"r2_score\")[0][0][0]\n",
    "best_alpha_1 = solver_kwargs.get(\"lambdas\")[best_alpha_1_idx]\n",
    "preds = np.maximum(get_pred_truth_locs(kfold_results)[0].flatten(), 0)\n",
    "truth = get_pred_truth_locs(kfold_results)[1].flatten()\n",
    "print(\n",
    "f\"\"\"Best \\u03BB 1: {best_alpha_1}\n",
    "Val R2: {r2_score(truth, preds):0.4f}  \\n\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "kfold_results = kfold_solve_custom_split_col(\n",
    "    static_lam_val=[best_alpha_1, 0.01],\n",
    "    static_lam_idxs=static_lam_idxs_2,\n",
    "    **solver_kwargs\n",
    ")\n",
    "best_alpha_2_idx = interpret_kfold_results(kfold_results, \"r2_score\")[0][0][0]\n",
    "best_alpha_2 = solver_kwargs.get(\"lambdas\")[best_alpha_2_idx]\n",
    "preds = np.maximum(get_pred_truth_locs(kfold_results)[0].flatten(), 0)\n",
    "truth = get_pred_truth_locs(kfold_results)[1].flatten()\n",
    "print(\n",
    "f\"\"\"Best \\u03BB 2: {best_alpha_2}\n",
    "Val R2: {r2_score(truth, preds):0.4f}  \\n\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "kfold_results = kfold_solve_custom_split_col(\n",
    "    static_lam_val=[best_alpha_1, best_alpha_2],\n",
    "    static_lam_idxs=static_lam_idxs_3,\n",
    "    **solver_kwargs\n",
    ")\n",
    "best_alpha_3_idx = interpret_kfold_results(kfold_results, \"r2_score\")[0][0][0]\n",
    "best_alpha_3 = solver_kwargs.get(\"lambdas\")[best_alpha_3_idx]\n",
    "preds = np.maximum(get_pred_truth_locs(kfold_results)[0].flatten(), 0)\n",
    "truth = get_pred_truth_locs(kfold_results)[1].flatten()\n",
    "print(f\"Best \\u03BB 3: {best_alpha_3}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "model, intercept_term = custom_ridge(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    lam=best_alpha_1, \n",
    "    intercept=True,\n",
    "    static_lam_val=[best_alpha_2, best_alpha_3], \n",
    "    static_lam_idxs=static_lam_idxs\n",
    ")\n",
    "pred_test = np.asarray(x_test).dot(model) + intercept_term \n",
    "pred_test = np.maximum(pred_test, 0)\n",
    "\n",
    "print(\n",
    "f\"\"\"Best \\u03BB's': {[best_alpha_1, best_alpha_2, best_alpha_3]}\n",
    "Final Val R2: {r2_score(truth, preds):0.4f}  \n",
    "Test R2: {r2_score(y_test, pred_test):0.4f}\\n\"\"\"\n",
    ")\n",
    "\n",
    "#7167 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9939df7-a68c-4341-97d1-d03e640afd60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cbe00a-02d6-46ed-b738-f06626f174a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a01c1-78fb-4153-8918-8f05e6dfdc65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d02ebf-8599-4dc6-a7b8-1e232a726c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4568f0af-c9f5-4406-892b-b10a36823cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22159ac-e2fa-434d-b86d-e57ad4cb0ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e1832-53f7-4087-85c6-484087c4a41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1709a0ba-8579-48e8-8466-cd69e2d25890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839e933-6475-4957-920e-0132b0f44875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54ae806-dc12-4db8-a471-e732b8f84c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-08 "
     ]
    }
   ],
   "source": [
    "#########################################     K-FOLD CV    ###########################################\n",
    "### SETUP\n",
    "ridge  = Ridge()  \n",
    "kfold  = KFold(n_splits=5)\n",
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "tic = time.time()\n",
    "### LAMBDA INDICIES\n",
    "start = [0, features_1.shape[1], features_2.shape[1]]\n",
    "end   = [features_1.shape[1], features_2.shape[1], x_train.shape[1]] \n",
    "if hot_encode:\n",
    "    start.append(x_train.shape[1]-n_districts)\n",
    "    end.append(x_train.shape[1]-n_districts)\n",
    "    end.sort()\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER(S)\n",
    "tic = time.time()\n",
    "best_lambdas, best_scores, best_model = kfold_rr_multi_lambda_tuning(\n",
    "    X=x_train,\n",
    "    y=y_train, \n",
    "    grid=alphas.get('alpha'), \n",
    "    n_splits=5,\n",
    "    start=start,\n",
    "    end=end, \n",
    "    static_lam=1,\n",
    "    verbose=True,\n",
    "    show_linalg_warning=False,\n",
    "    fit_model_after_tuning=True\n",
    ")\n",
    "### PREDICT WITH BEST HYPERPARAMETER(S)\n",
    "val_predictions   = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)\n",
    "print(f\"File: {file}\\nOne-Hot Encoding: {True}\\nTotal time: {(time.time()-tic)/60:0.2f} minutes\")\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################    \n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split['cv_prediction'] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"]-train_split.groupby('district')['log_yield'].transform('mean')\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\"cv_prediction\"]-train_split.groupby('district')['cv_prediction'].transform('mean')\n",
    "\n",
    "test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "predictions = pd.concat([train_split, test_split])\n",
    "\n",
    "#########################################     SAVE RESULTS    #########################################\n",
    "d = {\n",
    "    'country': country_code,\n",
    "\n",
    "    'satellite_1'   : satellite1[0],\n",
    "    'bands_1'       : bands1,\n",
    "    'num_features_1': num_features1,\n",
    "    'points_1'      : points1, \n",
    "    'month_range_1' : mns1,\n",
    "    'limit_months_1': limit_months1,\n",
    "    'crop_mask_1'   : crop_mask1,\n",
    "    'weighted_avg_1': weighted_avg1,\n",
    "\n",
    "    'satellite_2'   : satellite2[0],\n",
    "    'bands_2'       : bands2,\n",
    "    'num_features_2': num_features2,\n",
    "    'points_2'      : points2, \n",
    "    'month_range_2' : mns2,\n",
    "    'limit_months_2': limit_months2,\n",
    "    'crop_mask_2'   : crop_mask2,\n",
    "    'weighted_avg_2': weighted_avg2,\n",
    "\n",
    "    'hot_encode': hot_encode,\n",
    "\n",
    "    'total_n': len(x_all),\n",
    "    'train_n': len(x_train),\n",
    "    'test_n' : len(x_test),\n",
    "\n",
    "    'best_reg_param': [best_lambdas],\n",
    "    'mean_of_val_R2': [best_scores],\n",
    "    'val_R2': r2_score(y_train, val_predictions),\n",
    "    'val_r' : pearsonr(val_predictions, y_train)[0],\n",
    "    'val_r2': pearsonr(val_predictions, y_train)[0] ** 2,\n",
    "\n",
    "    'train_R2': r2_score(y_train, train_predictions),\n",
    "    'train_r' : pearsonr(train_predictions, y_train)[0],\n",
    "    'train_r2': pearsonr(train_predictions, y_train)[0] ** 2,\n",
    "\n",
    "    'test_R2': r2_score(y_test, test_predictions),\n",
    "    'test_r' : pearsonr(test_predictions, y_test)[0],\n",
    "    'test_r2': pearsonr(test_predictions, y_test)[0] ** 2,\n",
    "\n",
    "    'demean_cv_R2': r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction),\n",
    "    'demean_cv_r':  pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0],\n",
    "    'demean_cv_r2': pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0] ** 2,\n",
    "}\n",
    "df = pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee025435-668d-4be8-a911-aecb3595926b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e2e083-6e78-4478-87e1-f3454bd8ec39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda7fa4-ee0e-423f-bcc1-3f2480490dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2974c4d-01d4-49b3-a4da-b5c4031a34cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING  \n",
    "f1 = 'landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_15k-points_1000-features_yr-2013-2021_mn-4-9_lm-True_cm-False_wa-False_summary.feather'\n",
    "f2 = 'sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather'\n",
    "hot_encode = True\n",
    "\n",
    "#########################################     SET PARAMS    #########################################    \n",
    "# f1         = params[0]\n",
    "# f2         = params[1]\n",
    "# hot_encode = params[2]\n",
    "\n",
    "satellite1, bands1, country_code, points1, yrs1, mns1,\\\n",
    "num_features1, limit_months1, crop_mask1, weighted_avg1 = split_fn(f1)\n",
    "\n",
    "satellite2, bands2, country_code, points2, yrs2, mns2,\\\n",
    "num_features2, limit_months2, crop_mask2, weighted_avg2 = split_fn(f2)\n",
    "\n",
    "#########################################     READ DATA    #########################################\n",
    "features_1 = pd.read_feather(here('data', 'random_features', 'summary', f1))\n",
    "features_2 = pd.read_feather(here('data', 'random_features', 'summary', f2))\n",
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "\n",
    "#########################################     CLEAN DATA    #########################################  \n",
    "min_year = max(min(features_1.year), min(features_2.year))\n",
    "max_year = min(max(features_1.year), max(features_2.year))\n",
    "\n",
    "features_1 = features_1[features_1.year >= min_year]\n",
    "features_2 = features_2[features_2.year >= min_year]\n",
    "\n",
    "features_1 = features_1[features_1.year <= max_year]\n",
    "features_2 = features_2[features_2.year <= max_year]\n",
    "\n",
    "features_1.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "features_2.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "#########################################     JOIN FEATURES    #########################################  \n",
    "drop_cols = ['district', 'year', 'yield_mt']\n",
    "\n",
    "features_1 = features_1.set_index(drop_cols).add_prefix(\"f1_\")\n",
    "features_2 = features_2.set_index(drop_cols).add_prefix(\"f2_\")\n",
    "\n",
    "features = features_1.join(features_2).reset_index()\n",
    "features = features[~features.isna().any(axis = 1)]\n",
    "\n",
    "#########################################    JOIN CLIMATE VARS    ######################################### \n",
    "ndvi_cols = climate_df.columns[climate_df.columns.to_series().str.contains('ndvi')]\n",
    "keep_cols = [*ndvi_cols, *drop_cols]\n",
    "climate_df = climate_df.loc[:, keep_cols]\n",
    "\n",
    "features = features.set_index(drop_cols).join(climate_df.set_index(drop_cols)).reset_index()\n",
    "features = features[features.year <= max(climate_df.year)]\n",
    "\n",
    "crop_yield = features.copy().loc[:, tuple(drop_cols)]\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "#########################################    HOT ENCODE    ######################################### \n",
    "if hot_encode:\n",
    "    drop_cols.remove('district')\n",
    "    features = pd.get_dummies(features, columns = [\"district\"], drop_first = False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################    \n",
    "features = features.set_index(drop_cols) \n",
    "features_scaled = StandardScaler().fit_transform(features.values)\n",
    "features = pd.DataFrame(features_scaled, index=features.index).reset_index()\n",
    "features.columns = features.columns.astype(str)\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = features.drop(drop_cols, axis = 1) \n",
    "y_all = np.log10(features.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33ac5127-8b6a-41a6-9aae-c3e2f052e7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1.0 10.0 100.0 1000.0 10000.0 100000.0 1000000.0 10000000.0 100000000.0 \n",
      "\tBest λ 1: 10.0\n",
      "\tVal R2 1: 0.7727\n",
      "1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1.0 10.0 100.0 1000.0 10000.0 100000.0 1000000.0 10000000.0 100000000.0 \n",
      "\tBest λ 2: 0.1\n",
      "\tVal R2 2: 0.7764\n",
      "1e-08 1e-07 1e-06 1e-05 0.0001 0.001 0.01 0.1 1.0 10.0 100.0 1000.0 10000.0 100000.0 1000000.0 10000000.0 100000000.0 \n",
      "\tBest λ 3: 0.1\n",
      "\tVal R2 3: 0.8082\n",
      "Total time: 269.30 minutes\n",
      "Final Val R2: 0.8091\n",
      "Final Test R2: 0.7675\n",
      "CPU times: total: 15h 38min 52s\n",
      "Wall time: 4h 32min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "best_lam, res, model = kfold_rr_multi_lambda_tuning(\n",
    "    x_train, y_train, \n",
    "    grid=np.logspace(-8, 8, base = 10, num = 17), \n",
    "    start=[0, x_train.shape[1]-(72+12), x_train.shape[1]-72],\n",
    "    end=[x_train.shape[1]-(72+12), x_train.shape[1]-72, x_train.shape[1]], \n",
    "    static_lam=1, \n",
    "    verbose=True, \n",
    "    fit_model_after_tuning=True\n",
    ")\n",
    "kfold = KFold(n_splits=5)\n",
    "val_predictions = cross_val_predict(model, X = x_train, y = y_train, cv = kfold) \n",
    "print(f\"\"\"Final Val R2: {r2_score(y_train, val_predictions):0.4f}\n",
    "Final Test R2: {r2_score(y_test, model.predict(x_test)):0.4f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96a000dc-3862-408f-8fac-aadf8891f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from prediction_utils import *\n",
    "from prediction_utils_1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3760579d-78db-42bf-bb91-f92fadc7c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_kwargs = {\n",
    "    \"X\": x_train,\n",
    "    \"y\": y_train,\n",
    "    \"locations\": x_train.index,\n",
    "    \"split_col\": x_train.reset_index().index,\n",
    "    \"lambdas\": np.logspace(-8, 8, base = 10, num = 17), \n",
    "    \"return_preds\": True,\n",
    "    \"return_model\": False,\n",
    "    \"svd_solve\": False,\n",
    "    \"allow_linalg_warning_instances\": True,\n",
    "    \"fit_model_after_tuning\": False,\n",
    "    \"intercept\": True,\n",
    "    \"num_folds\": 5,\n",
    "    \"random_state\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b2c61e-d27c-441e-9ade-bf27add9efd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dbee424-20b7-40b6-8dc2-8a050256189e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on fold (of 5): 0 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:3\u001b[0m\n",
      "File \u001b[1;32m~\\Desktop\\GitHub\\crop-modeling\\code\\3_task_modeling\\prediction_utils.py:536\u001b[0m, in \u001b[0;36mkfold_solve_custom_split_col\u001b[1;34m(X, y, locations, split_col, sample_weights, solve_function, num_folds, return_preds, return_model, fit_model_after_tuning, random_state, **kwargs_solve)\u001b[0m\n\u001b[0;32m    533\u001b[0m kfold_y_test\u001b[38;5;241m.\u001b[39mappend(y_val)\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# call solve func\u001b[39;00m\n\u001b[1;32m--> 536\u001b[0m solve_results \u001b[38;5;241m=\u001b[39m \u001b[43msolve_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_preds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_preds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_solve\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;66;03m# record performance metrics\u001b[39;00m\n\u001b[0;32m    547\u001b[0m kfold_metrics_test\u001b[38;5;241m.\u001b[39mappend(solve_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics_test\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\Desktop\\GitHub\\crop-modeling\\code\\3_task_modeling\\prediction_utils.py:328\u001b[0m, in \u001b[0;36mridge_regression\u001b[1;34m(X_train, X_test, y_train, y_test, svd_solve, lambdas, return_preds, return_model, clip_bounds, intercept, static_lam_val, static_lam_idxs, allow_linalg_warning_instances)\u001b[0m\n\u001b[0;32m    326\u001b[0m warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malways\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    327\u001b[0m lambda_warning \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m model, intercept_term \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_ridge\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mlam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambdan\u001b[49m\u001b[43m,\u001b[49m\u001b[43mintercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintercept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# No need to input X and Y\u001b[39;49;00m\n\u001b[0;32m    329\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mXtX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mXtX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mXty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#Directly feed XtX and Xty to avoid repitition\u001b[39;49;00m\n\u001b[0;32m    330\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mX_offset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_offset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mstatic_lam_val\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstatic_lam_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mstatic_lam_idxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstatic_lam_idxs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;66;03m# if there is a warning\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(w) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\Desktop\\GitHub\\crop-modeling\\code\\3_task_modeling\\prediction_utils.py:206\u001b[0m, in \u001b[0;36mcustom_ridge\u001b[1;34m(X, y, lam, intercept, static_lam_val, static_lam_idxs, XtX, Xty, X_offset, y_offset)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(static_lam_idxs):\n\u001b[0;32m    204\u001b[0m         eye[idx,idx] \u001b[38;5;241m=\u001b[39m static_lam_val[i]\n\u001b[1;32m--> 206\u001b[0m model \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39msolve(\u001b[43mXtX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meye\u001b[49m, \n\u001b[0;32m    207\u001b[0m Xty, sym_pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    209\u001b[0m intercept_term \u001b[38;5;241m=\u001b[39m y_offset \u001b[38;5;241m-\u001b[39m X_offset\u001b[38;5;241m.\u001b[39mdot(model)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, intercept_term\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "stat_lam_idxs_1 = list(range(features_1.shape[1] , x_train.shape[1]))\n",
    "\n",
    "kfold_results = kfold_solve_custom_split_col(\n",
    "    static_lam_val=1,\n",
    "    static_lam_idxs=stat_lam_idxs,\n",
    "    **solver_kwargs\n",
    ")\n",
    "best_alpha_1_idx = interpret_kfold_results(kfold_results, \"r2_score\")[0][0][0]\n",
    "best_alpha_1 = list(solver_kwargs.get('lambdas'))[best_alpha_1_idx]\n",
    "preds = np.maximum(get_pred_truth_locs(kfold_results)[0].flatten(), 0)\n",
    "truth = get_pred_truth_locs(kfold_results)[1].flatten()\n",
    "locations = get_pred_truth_locs(kfold_results)[2].flatten()\n",
    "print(\n",
    "f\"\"\"Best alpha: {best_alpha_1}\n",
    "Val R2: {r2_score(truth, preds)}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "417c87dc-7bec-4f47-ab29-d0d62bdea55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solver_kwargs = {\n",
    "#     # set of possible hyperparameters to search over in cross-validation\n",
    "#     \"lambdas\": np.logspace(-7, 7, base = 10, num = 15), # [1e-3,1e-2, 1e-1, 1e0,1e1,1e2,1e3,1e4,1e5,1e6],\n",
    "#     # do you want to return the predictions from the model?\n",
    "#     \"return_preds\": True,\n",
    "#     # input the bounds used to clip predictions\n",
    "#     \"return_model\": False,\n",
    "#     # do you want to use an SVD solve or standard linear regression? (NB: SVD is much slower)\n",
    "#     \"svd_solve\": False,\n",
    "#     # do you want to allow hyperparameters to be chosen even if they lead to warnings about matrix invertibility?\n",
    "#     \"allow_linalg_warning_instances\": True,\n",
    "#     \"fit_model_after_tuning\": False,\n",
    "#     \"static_lam_val\": best_alpha_1,\n",
    "#     # 'static_lam_idxs': list(range(x_train.shape[1]-72, x_train.shape[1])),\n",
    "#     \"static_lam_idxs\": list(range(0, x_train.shape[1]-72)),\n",
    "#     \"intercept\": True\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8180c5a6-4be6-4978-b2f1-199a21b0728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# kfold_results = kfold_solve_custom_split_col(\n",
    "#     X=x_train,\n",
    "#     y=y_train,\n",
    "#     locations=x_train.index,\n",
    "#     split_col=x_train.reset_index().index,\n",
    "#     num_folds=5,\n",
    "#     random_state=1991,\n",
    "#     **solver_kwargs\n",
    "# )\n",
    "# best_alpha_2_idx = interpret_kfold_results(kfold_results, \"r2_score\")[0][0][0]\n",
    "# best_alpha_2 = list(solver_kwargs.get('lambdas'))[best_alpha_2_idx]\n",
    "# preds = np.maximum(get_pred_truth_locs(kfold_results)[0].flatten(), 0)\n",
    "# truth = get_pred_truth_locs(kfold_results)[1].flatten()\n",
    "# locations = get_pred_truth_locs(kfold_results)[2].flatten()\n",
    "# print(\n",
    "# f\"\"\"Best alpha: {best_alpha_2}\n",
    "# Val R2: {r2_score(truth, preds)}\"\"\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a7efbc4-f3d3-4740-9a6b-653275b441d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_train_pred_scatterplot(task = \"Validation\", y_test = truth, preds_test = preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14fd1950-3276-4a3f-b1a5-bcee618a7f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_alpha_2_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c221fb92-ea96-458a-8224-83823b506e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold_results.get('models')[0][0][best_alpha_2_idx][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3114d837-5bae-4787-93c8-df3c424995bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, intercept_term = custom_ridge(\n",
    "#     X=x_train,\n",
    "#     y=y_train,\n",
    "#     lam=best_alpha_1, \n",
    "#     intercept=True,\n",
    "#     static_lam_val = best_alpha_2,\n",
    "#     static_lam_idxs =list(range(x_train.shape[1]-72, x_train.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "01b907a3-eb64-437c-b488-40b09f490e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_test = np.asarray(x_test).dot(model) + intercept_term \n",
    "# pred_test = np.maximum(pred_test, 0)\n",
    "# pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f40073f7-7ff2-430e-89a6-250df4155fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_score(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb7985e-f9ed-4178-a75f-981aeadb1a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b4e8724-de9e-4a91-9737-4f3ae02edea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split[\"demean_test_yield\"] = test_split[\"log_yield\"]-test_split.groupby('district')['log_yield'].transform('mean')\n",
    "test_split[\"demean_test_prediction\"] = test_split[\"prediction\"]-test_split.groupby('district')['prediction'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dad33d66-2672-4131-b118-cfc9c46738f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val  R2: 0.7563 \n",
      "Test R2: 0.6467 \n",
      "\n",
      "Demean Val  R2: 0.1603 \n",
      "Demean Test R2: 0.3323\n"
     ]
    }
   ],
   "source": [
    "print(f'Val  R2: {r2_score(y_train, val_predictions):0.4f}',\n",
    "      f'\\nTest R2: {r2_score(y_test, test_predictions):0.4f}',\n",
    "     f'\\n\\nDemean Val  R2: {r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction):0.4f}',\n",
    "     f'\\nDemean Test R2: {r2_score(test_split.demean_test_yield, test_split.demean_test_prediction):0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0011bbfb-c791-4144-9363-15932f4a9d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn_1 = f'{satellite1[0]}_{bands1}_{points1}_{limit_months1}_{crop_mask1}_{weighted_avg1}'\n",
    "# fn_2 = f'{satellite2[0]}_{bands2}_{points2}_{limit_months2}_{crop_mask2}_{weighted_avg2}'\n",
    "# suffix = f'fn-1_{fn_1}_fn-2_{fn_2}'\n",
    "# fn = f'predictions_{suffix}_he-{hot_encode}.csv'\n",
    "    \n",
    "# predictions_fn = here('data', 'results', fn)\n",
    "# predictions.to_csv(predictions_fn, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d63d90-690e-48a2-8df4-6a6fffda9406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
