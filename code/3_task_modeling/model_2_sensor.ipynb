{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f576830d-9afa-47d8-9fac-8e43292ee090",
   "metadata": {},
   "source": [
    "# Modeling Crop Yield: Landsat + Sentinel\n",
    "## Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425d2ec4-79b2-4fd1-a800-14db43e42ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44646286-2094-4bd0-8609-ad5efc857abc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## import warnings\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from pyhere import here\n",
    "from datetime import date\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "import geopandas\n",
    "import pickle\n",
    "\n",
    "import pyarrow\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import p_tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    LeaveOneGroupOut,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    "    cross_val_predict,\n",
    ")\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "from task_modeling_utils import *\n",
    "from prediction_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a054b69-93f3-4442-8013-266b4dba2063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Cullen\\\\Desktop\\\\GitHub\\\\crop-modeling\\\\code\\\\3_task_modeling\\\\..\\\\..\\\\data\\\\results\\\\2_sensor_results_1_2023-03-17.csv',\n",
       " 'C:\\\\Users\\\\Cullen\\\\Desktop\\\\GitHub\\\\crop-modeling\\\\code\\\\3_task_modeling\\\\..\\\\..\\\\data\\\\results\\\\2_sensor_results_2_2023-03-18.csv',\n",
       " 'C:\\\\Users\\\\Cullen\\\\Desktop\\\\GitHub\\\\crop-modeling\\\\code\\\\3_task_modeling\\\\..\\\\..\\\\data\\\\results\\\\2_sensor_results_3_2023-03-18.csv',\n",
       " 'C:\\\\Users\\\\Cullen\\\\Desktop\\\\GitHub\\\\crop-modeling\\\\code\\\\3_task_modeling\\\\..\\\\..\\\\data\\\\results\\\\2_sensor_results_4_2023-03-19.csv',\n",
       " 'C:\\\\Users\\\\Cullen\\\\Desktop\\\\GitHub\\\\crop-modeling\\\\code\\\\3_task_modeling\\\\..\\\\..\\\\data\\\\results\\\\2_sensor_results_5_2023-03-21.csv',\n",
       " 'C:\\\\Users\\\\Cullen\\\\Desktop\\\\GitHub\\\\crop-modeling\\\\code\\\\3_task_modeling\\\\..\\\\..\\\\data\\\\results\\\\2_sensor_results_6_2023-03-20.csv',\n",
       " 'C:\\\\Users\\\\Cullen\\\\Desktop\\\\GitHub\\\\crop-modeling\\\\code\\\\3_task_modeling\\\\..\\\\..\\\\data\\\\results\\\\2_sensor_results_7_2023-03-23.csv',\n",
       " 'C:\\\\Users\\\\Cullen\\\\Desktop\\\\GitHub\\\\crop-modeling\\\\code\\\\3_task_modeling\\\\..\\\\..\\\\data\\\\results\\\\2_sensor_results_8_2023-03-23.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_pattern = str(here(\"data\", \"results\", \"2_sensor_results_*_*.csv\"))\n",
    "files = glob.glob(pathname=file_pattern)\n",
    "results = merge_files(files)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85213ee8-c687-412c-b373-52acd6262357",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year_range</th>\n",
       "      <th>satellite_1</th>\n",
       "      <th>bands_1</th>\n",
       "      <th>num_features_1</th>\n",
       "      <th>points_1</th>\n",
       "      <th>month_range_1</th>\n",
       "      <th>limit_months_1</th>\n",
       "      <th>crop_mask_1</th>\n",
       "      <th>weighted_avg_1</th>\n",
       "      <th>satellite_2</th>\n",
       "      <th>bands_2</th>\n",
       "      <th>num_features_2</th>\n",
       "      <th>points_2</th>\n",
       "      <th>month_range_2</th>\n",
       "      <th>limit_months_2</th>\n",
       "      <th>crop_mask_2</th>\n",
       "      <th>weighted_avg_2</th>\n",
       "      <th>hot_encode</th>\n",
       "      <th>total_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>2016-2021</td>\n",
       "      <td>landsat-c2-l2</td>\n",
       "      <td>r-g-b-nir-swir16-swir22</td>\n",
       "      <td>1024</td>\n",
       "      <td>20</td>\n",
       "      <td>1-12</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>sentinel-2-l2a</td>\n",
       "      <td>2-3-4-8</td>\n",
       "      <td>1000</td>\n",
       "      <td>15</td>\n",
       "      <td>1-12</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year_range    satellite_1                  bands_1  num_features_1  \\\n",
       "1396  2016-2021  landsat-c2-l2  r-g-b-nir-swir16-swir22            1024   \n",
       "\n",
       "      points_1 month_range_1  limit_months_1  crop_mask_1  weighted_avg_1  \\\n",
       "1396        20          1-12           False         True           False   \n",
       "\n",
       "         satellite_2  bands_2  num_features_2  points_2 month_range_2  \\\n",
       "1396  sentinel-2-l2a  2-3-4-8            1000        15          1-12   \n",
       "\n",
       "      limit_months_2  crop_mask_2  weighted_avg_2  hot_encode  total_n  \n",
       "1396           False         True           False        True      414  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top = results.test_R2.sort_values().index[-1]\n",
    "# results.iloc[top:top+1, 1:20]\n",
    "\n",
    "top = results.val_R2.sort_values().index[-1]\n",
    "# results.iloc[top:top+1, 24:-1]\n",
    "results.iloc[top : top + 1, 1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b95303d9-ab3d-433c-aff7-def47d525597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# f1 = 'landsat-c2-l2_bands-r-g-b-nir-swir16-swir22_ZMB_20k-points_1024-features_yr-2009-2021_mn-1-12_lm-False_cm-True_wa-False_summary.feather'\n",
    "# f2 = 'sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16825434-7870-4757-89e4-e3d1b2cf9ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# directory = here(\"data\", \"random_features\", \"summary\")\n",
    "# files = os.listdir(directory)\n",
    "# files = list(f for f in files if f not in ('.gitkeep', '.ipynb_checkpoints'))\n",
    "# paramlist = list(itertools.combinations(files, 2))\n",
    "# paramlist = list(itertools.product(paramlist, [True, False]))\n",
    "# paramlist = list(tuple(merge(paramlist[i])) for i in range(len(paramlist)))\n",
    "# paramlist = sorted(paramlist, key=lambda tup: tup[2])\n",
    "# paramlist = paramlist[3:4]\n",
    "# len(paramlist)\n",
    "# paramlist = (i for i in paramlist)\n",
    "# paramlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76f0fa66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_2_sensor(paramlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c140e3-6424-45c3-be90-4b16c7f9d690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9050b1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from mpi4py.futures import MPIPoolExecutor\n",
    "# i = 1\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     max_workers = 1 # int(os.environ.get(\"SLURM_NTASKS\", 4)) - 1\n",
    "\n",
    "#     executor = MPIPoolExecutor(max_workers=max_workers)\n",
    "#     output = executor.starmap(model_2_sensor, paramlist)\n",
    "#     results = pd.DataFrame(output)\n",
    "#     executor.shutdown()\n",
    "\n",
    "#     today = date.today().strftime(\"%Y-%m-%d\")\n",
    "#     file_name = f'2_sensor_results_{i}_{today}.csv'\n",
    "#     print(f\"Saving results as: {file_name}\\n\\n\")\n",
    "#     results.to_csv(here(\"data\",\"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51c2a277-0c9c-4d65-b634-f94bb9dddd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f752cfdc-7cca-4ce2-9841-ac6118f0bbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# ##### With progress bar\n",
    "# workers = os.cpu_count()\n",
    "# if __name__ == \"__main__\":\n",
    "#     output = []\n",
    "#     for result in p_tqdm.p_map(model_2_sensor, paramlist):\n",
    "#         output.append(result)\n",
    "#     results = pd.concat(output).reset_index(drop=True)\n",
    "#     today = date.today().strftime(\"%Y-%m-%d\")\n",
    "#     file_name = f'2_sensor_results_{today}.csv'\n",
    "#     print(f\"Saving results as: {file_name}\\n\\n\")\n",
    "#     results.to_csv(here(\"data\",\"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9148d8-af99-4e68-8280-658e1ac15b86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin with paramters:\n",
      "\tlandsat-c2-l2_bands-r-g-b-nir-swir16-swir22_ZMB_20k-points_1024-features_yr-2009-2021_mn-1-12_lm-False_cm-True_wa-False_summary.feather\n",
      "\tsentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather\n",
      "\tOne-hot encoding: True\n",
      "\n",
      "[0, 12288, 24288, 24300, 24312]\n",
      "   [12288, 24288, 24300, 24312, 24381]\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 1: 100.0\n",
      "\tVal R2 1: 0.7288\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 2: 10.0\n",
      "\tVal R2 2: 0.7681\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 3: 0.01\n",
      "\tVal R2 3: 0.7817\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 4: 0.1\n",
      "\tVal R2 4: 0.7854\n",
      "\n",
      "1e-08 1e-07 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02 1e+03 1e+04 1e+05 1e+06 1e+07 1e+08 \n",
      "\tBest λ 5: 1e-05\n",
      "\tVal R2 5: 0.8554\n",
      "\n",
      "\n",
      "Finish:\n",
      "landsat-c2-l2_bands-r-g-b-nir-swir16-swir22_ZMB_20k-points_1024-features_yr-2009-2021_mn-1-12_lm-False_cm-True_wa-False_summary.feather\n",
      "sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather\n",
      "One-hot encoding: True\n",
      "Final Val R2:  0.8677 \n",
      "Final Test R2: 0.6797\n",
      "Total time: 806.75 minutes\n",
      "\n",
      "CPU times: total: 2d 4h 43min\n",
      "Wall time: 13h 26min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## TESTING  \n",
    "f1 = 'landsat-c2-l2_bands-r-g-b-nir-swir16-swir22_ZMB_20k-points_1024-features_yr-2009-2021_mn-1-12_lm-False_cm-True_wa-False_summary.feather'\n",
    "f2 = 'sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather'\n",
    "he = True\n",
    "n_splits = 5\n",
    "variable_groups = ['tmp', 'ndvi']\n",
    "\n",
    "#########################################     SET PARAMS    #########################################    \n",
    "\n",
    "satellite1, bands1, country_code, points1, yrs1, mns1,\\\n",
    "num_features1, limit_months1, crop_mask1, weighted_avg1 = split_fn(f1)\n",
    "\n",
    "satellite2, bands2, country_code, points2, yrs2, mns2,\\\n",
    "num_features2, limit_months2, crop_mask2, weighted_avg2 = split_fn(f2)\n",
    "\n",
    "print(f\"\\nBegin with paramters:\\n\\t{f1}\\n\\t{f2}\\n\\tOne-hot encoding: {he}\\n\", flush=True)\n",
    "\n",
    "#########################################     READ DATA    #########################################\n",
    "features_1 = pd.read_feather(here('data', 'random_features', 'summary', f1))\n",
    "features_2 = pd.read_feather(here('data', 'random_features', 'summary', f2))\n",
    "climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "\n",
    "#########################################     CLEAN DATA    #########################################  \n",
    "min_year = max(min(features_1.year), min(features_2.year))\n",
    "max_year = min(max(features_1.year), max(features_2.year))\n",
    "\n",
    "features_1 = features_1[features_1.year >= min_year]\n",
    "features_2 = features_2[features_2.year >= min_year]\n",
    "\n",
    "features_1 = features_1[features_1.year <= max_year]\n",
    "features_2 = features_2[features_2.year <= max_year]\n",
    "\n",
    "features_1.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "features_2.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "#########################################     JOIN FEATURES    #########################################  \n",
    "drop_cols = ['district', 'year', 'yield_mt']\n",
    "\n",
    "features_1 = features_1.set_index(drop_cols).add_prefix(\"f1_\")\n",
    "features_2 = features_2.set_index(drop_cols).add_prefix(\"f2_\")\n",
    "\n",
    "features = features_1.join(features_2).reset_index()\n",
    "features = features[~features.isna().any(axis = 1)]\n",
    "\n",
    "#########################################    JOIN CLIMATE VARS    #########################################\n",
    "keep_cols = []\n",
    "\n",
    "for var in variable_groups:\n",
    "    tmp = climate_df.columns[climate_df.columns.to_series().str.contains(var)].tolist()\n",
    "    keep_cols.append(tmp)\n",
    "\n",
    "keep_cols = [*drop_cols, *[col for cols in keep_cols for col in cols]]\n",
    "\n",
    "climate_df = climate_df.loc[:, keep_cols]\n",
    "\n",
    "features = (\n",
    "    features.set_index(drop_cols).join(climate_df.set_index(drop_cols)).reset_index()\n",
    ")\n",
    "features = features[features.year <= max(climate_df.year)]\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "features = features.set_index(drop_cols)\n",
    "features_scaled = StandardScaler().fit_transform(features.values)\n",
    "features = pd.DataFrame(features_scaled, index=features.index).reset_index()\n",
    "features.columns = features.columns.astype(str)\n",
    "\n",
    "#########################################     CLEAN AND COPY    #########################################\n",
    "yrs = f\"{min(features.year)}-{max(features.year)}\"\n",
    "n_fts_1 = features_1.shape[1]\n",
    "n_fts_2 = features_2.shape[1]\n",
    "n_districts = len(features.district.unique())\n",
    "n_climate_cols = climate_df.shape[1] - len(drop_cols)\n",
    "\n",
    "i = 0\n",
    "n_climate_groups = []\n",
    "for cols in range(n_climate_cols):\n",
    "    if cols % 12 == 0:\n",
    "        i += 1\n",
    "        n_climate_groups.append(i)\n",
    "n_climate_groups\n",
    "\n",
    "crop_yield = features.copy().loc[:, tuple(drop_cols)]\n",
    "crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "\n",
    "del features_1, features_2\n",
    "gc.collect()\n",
    "\n",
    "#########################################    HOT ENCODE    #########################################\n",
    "if he:\n",
    "    drop_cols.remove(\"district\")\n",
    "    features = pd.get_dummies(features, columns=[\"district\"], drop_first=False)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "\n",
    "#########################################     K-FOLD SPLIT    #########################################\n",
    "x_all = features.drop(drop_cols, axis = 1) \n",
    "y_all = np.log10(features.yield_mt.to_numpy() + 1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_all, y_all, test_size=0.2, random_state=0)\n",
    "\n",
    "# del features; gc.collect()\n",
    "\n",
    "#########################################     K-FOLD CV    ###########################################\n",
    "### SETUP\n",
    "tic = time.time()\n",
    "kfold  = KFold(n_splits=n_splits)\n",
    "alphas = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "\n",
    "### LAMBDA INDICIES\n",
    "start = [0, n_fts_1, n_fts_1+n_fts_2]\n",
    "end   = [n_fts_1, n_fts_1+n_fts_2, x_train.shape[1]] \n",
    "\n",
    "for n in n_climate_groups:\n",
    "    x = n * 12\n",
    "    y = n_fts_1+n_fts_2 + x\n",
    "    start.append(y)\n",
    "    end.append(y)\n",
    "\n",
    "end.sort()\n",
    "    \n",
    "print(start, end, sep = \"\\n   \")\n",
    "\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER(S)\n",
    "best_lambdas, best_scores, best_model = kfold_rr_multi_lambda_tuning(\n",
    "    X=x_train,\n",
    "    y=y_train, \n",
    "    grid=alphas.get('alpha'), \n",
    "    n_splits=n_splits,\n",
    "    start=start,\n",
    "    end=end, \n",
    "    static_lam=1,\n",
    "    verbose=2,\n",
    "    show_linalg_warning=False,\n",
    "    fit_model_after_tuning=True\n",
    ")\n",
    "### PREDICT WITH BEST HYPERPARAMETER(S)\n",
    "val_predictions   = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)\n",
    "print(f\"\"\"\n",
    "Finish:\n",
    "{f1}\n",
    "{f2}\n",
    "One-hot encoding: {he}\n",
    "Final Val R2:  {r2_score(y_train, val_predictions):0.4f} \n",
    "Final Test R2: {r2_score(y_test, test_predictions):0.4f}\n",
    "Total time: {(time.time()-tic)/60:0.2f} minutes\n",
    "\"\"\", flush=True)\n",
    "\n",
    "#########################################     DE-MEAN R2    #########################################    \n",
    "crop_yield[\"prediction\"] = np.maximum(best_model.predict(x_all), 0)\n",
    "\n",
    "train_split = pd.DataFrame(np.repeat('train', len(x_train)), columns = ['split'], index = x_train.index)\n",
    "train_split = train_split.join(crop_yield.copy()[crop_yield.index.isin(x_train.index)])\n",
    "train_split['cv_prediction'] = np.maximum(val_predictions, 0)\n",
    "train_split[\"demean_cv_yield\"] = train_split[\"log_yield\"]-train_split.groupby('district')['log_yield'].transform('mean')\n",
    "train_split[\"demean_cv_prediction\"] = train_split[\"cv_prediction\"]-train_split.groupby('district')['cv_prediction'].transform('mean')\n",
    "\n",
    "test_split = pd.DataFrame(np.repeat('test', len(x_test)), columns = ['split'], index = x_test.index)\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split['cv_prediction'] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_yield\"] = np.repeat(np.nan, len(x_test))\n",
    "test_split[\"demean_cv_prediction\"] = np.repeat(np.nan, len(x_test))\n",
    "\n",
    "#########################################     SAVE RESULTS    #########################################\n",
    "d = {\n",
    "    'country': country_code[0],\n",
    "    'year_range': yrs,\n",
    "\n",
    "    'satellite_1'   : satellite1[0],\n",
    "    'bands_1'       : bands1,\n",
    "    'num_features_1': num_features1,\n",
    "    'points_1'      : points1, \n",
    "    'month_range_1' : mns1,\n",
    "    'limit_months_1': limit_months1,\n",
    "    'crop_mask_1'   : crop_mask1,\n",
    "    'weighted_avg_1': weighted_avg1,\n",
    "\n",
    "    'satellite_2'   : satellite2[0],\n",
    "    'bands_2'       : bands2,\n",
    "    'num_features_2': num_features2,\n",
    "    'points_2'      : points2, \n",
    "    'month_range_2' : mns2,\n",
    "    'limit_months_2': limit_months2,\n",
    "    'crop_mask_2'   : crop_mask2,\n",
    "    'weighted_avg_2': weighted_avg2,\n",
    "\n",
    "    'hot_encode': he,\n",
    "\n",
    "    'total_n': len(x_all),\n",
    "    'train_n': len(x_train),\n",
    "    'test_n' : len(x_test),\n",
    "\n",
    "    'best_reg_param': [best_lambdas],\n",
    "    'mean_of_val_R2': [best_scores],\n",
    "    'val_R2': r2_score(y_train, val_predictions),\n",
    "    'val_r' : pearsonr(val_predictions, y_train)[0],\n",
    "    'val_r2': pearsonr(val_predictions, y_train)[0] ** 2,\n",
    "\n",
    "    'train_R2': r2_score(y_train, train_predictions),\n",
    "    'train_r' : pearsonr(train_predictions, y_train)[0],\n",
    "    'train_r2': pearsonr(train_predictions, y_train)[0] ** 2,\n",
    "\n",
    "    'test_R2': r2_score(y_test, test_predictions),\n",
    "    'test_r' : pearsonr(test_predictions, y_test)[0],\n",
    "    'test_r2': pearsonr(test_predictions, y_test)[0] ** 2,\n",
    "\n",
    "    'demean_cv_R2': r2_score(train_split.demean_cv_yield, train_split.demean_cv_prediction),\n",
    "    'demean_cv_r':  pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0],\n",
    "    'demean_cv_r2': pearsonr(train_split.demean_cv_yield, train_split.demean_cv_prediction)[0] ** 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d25759f-6837-4e2a-8e73-63af8dbe8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "file_name = f\"2_sensor_ndvi_tmp_{today}.csv\"\n",
    "data = pd.DataFrame(d)\n",
    "data.to_csv(here(\"data\", \"results\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1ba7001-3d2b-487a-84d5-8438e962171a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>district</th>\n",
       "      <th>year</th>\n",
       "      <th>yield_mt</th>\n",
       "      <th>log_yield</th>\n",
       "      <th>prediction</th>\n",
       "      <th>cv_prediction</th>\n",
       "      <th>demean_cv_yield</th>\n",
       "      <th>demean_cv_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>train</td>\n",
       "      <td>Mufumbwe</td>\n",
       "      <td>2018</td>\n",
       "      <td>2.433973</td>\n",
       "      <td>0.535797</td>\n",
       "      <td>0.510238</td>\n",
       "      <td>0.482459</td>\n",
       "      <td>0.015688</td>\n",
       "      <td>-0.043545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>train</td>\n",
       "      <td>Chibombo</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.865338</td>\n",
       "      <td>0.457176</td>\n",
       "      <td>0.428593</td>\n",
       "      <td>0.425167</td>\n",
       "      <td>-0.029443</td>\n",
       "      <td>-0.073926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>train</td>\n",
       "      <td>Mufumbwe</td>\n",
       "      <td>2019</td>\n",
       "      <td>2.066056</td>\n",
       "      <td>0.486580</td>\n",
       "      <td>0.480003</td>\n",
       "      <td>0.455626</td>\n",
       "      <td>-0.033528</td>\n",
       "      <td>-0.070378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>train</td>\n",
       "      <td>Kasama</td>\n",
       "      <td>2020</td>\n",
       "      <td>2.970522</td>\n",
       "      <td>0.598848</td>\n",
       "      <td>0.600974</td>\n",
       "      <td>0.616541</td>\n",
       "      <td>-0.008883</td>\n",
       "      <td>0.005669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train</td>\n",
       "      <td>Chavuma</td>\n",
       "      <td>2018</td>\n",
       "      <td>2.003123</td>\n",
       "      <td>0.477573</td>\n",
       "      <td>0.493034</td>\n",
       "      <td>0.531171</td>\n",
       "      <td>-0.061884</td>\n",
       "      <td>-0.032250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>test</td>\n",
       "      <td>Mumbwa</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.580626</td>\n",
       "      <td>0.411725</td>\n",
       "      <td>0.355326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>test</td>\n",
       "      <td>Kafue</td>\n",
       "      <td>2020</td>\n",
       "      <td>2.012612</td>\n",
       "      <td>0.478943</td>\n",
       "      <td>0.494897</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test</td>\n",
       "      <td>Chama</td>\n",
       "      <td>2017</td>\n",
       "      <td>2.308008</td>\n",
       "      <td>0.519567</td>\n",
       "      <td>0.464183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>test</td>\n",
       "      <td>Chibombo</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.693657</td>\n",
       "      <td>0.430342</td>\n",
       "      <td>0.421939</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>test</td>\n",
       "      <td>Gwembe</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.809107</td>\n",
       "      <td>0.257464</td>\n",
       "      <td>0.305039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>414 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     split  district  year  yield_mt  log_yield  prediction  cv_prediction  \\\n",
       "302  train  Mufumbwe  2018  2.433973   0.535797    0.510238       0.482459   \n",
       "20   train  Chibombo  2018  1.865338   0.457176    0.428593       0.425167   \n",
       "303  train  Mufumbwe  2019  2.066056   0.486580    0.480003       0.455626   \n",
       "142  train    Kasama  2020  2.970522   0.598848    0.600974       0.616541   \n",
       "14   train   Chavuma  2018  2.003123   0.477573    0.493034       0.531171   \n",
       "..     ...       ...   ...       ...        ...         ...            ...   \n",
       "308   test    Mumbwa  2018  1.580626   0.411725    0.355326            NaN   \n",
       "100   test     Kafue  2020  2.012612   0.478943    0.494897            NaN   \n",
       "7     test     Chama  2017  2.308008   0.519567    0.464183            NaN   \n",
       "22    test  Chibombo  2020  1.693657   0.430342    0.421939            NaN   \n",
       "68    test    Gwembe  2018  0.809107   0.257464    0.305039            NaN   \n",
       "\n",
       "     demean_cv_yield  demean_cv_prediction  \n",
       "302         0.015688             -0.043545  \n",
       "20         -0.029443             -0.073926  \n",
       "303        -0.033528             -0.070378  \n",
       "142        -0.008883              0.005669  \n",
       "14         -0.061884             -0.032250  \n",
       "..               ...                   ...  \n",
       "308              NaN                   NaN  \n",
       "100              NaN                   NaN  \n",
       "7                NaN                   NaN  \n",
       "22               NaN                   NaN  \n",
       "68               NaN                   NaN  \n",
       "\n",
       "[414 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df = pd.concat([train_split, test_split])\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0797eb-27c5-4759-8a47-ceb122523417",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "pd_file_name = f'2_sensor_ndvi_tmp_predictions_{today}.csv'\n",
    "# prediction_df = pd.DataFrame(d)\n",
    "prediction_df.to_csv(here('data', 'results', pd_file_name), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "fec78c4aa066b70eb4890a66d67729df88bd6fefdc0eec39af542817ac9abede"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
