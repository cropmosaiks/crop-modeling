{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f576830d-9afa-47d8-9fac-8e43292ee090",
   "metadata": {},
   "source": [
    "# Modeling Crop Yield: Landsat + Sentinel\n",
    "## Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44646286-2094-4bd0-8609-ad5efc857abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import warnings\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import glob\n",
    "from pyhere import here\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas\n",
    "import pickle\n",
    "\n",
    "import pyarrow\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import p_tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneGroupOut, cross_val_score, GridSearchCV, cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr,  pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09d78fb-ccb1-4da5-901f-ced9c633366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fn(file_name):\n",
    "    f            = file_name.split(sep=\"_\")\n",
    "    satellite    = f[0],\n",
    "    bands        = f[1].replace(\"bands-\", \"\")\n",
    "    country_code = f[2],\n",
    "    points       = f[3].replace(\"k-points\", \"\")\n",
    "    num_features = f[4].replace(\"-features\", \"\")\n",
    "    yrs          = f[5].replace(\"yr-\", \"\")\n",
    "    mns          = f[6].replace(\"mn-\", \"\")\n",
    "    limit_months = f[7].replace(\"lm-\", \"\")\n",
    "    crop_mask    = f[8].replace(\"cm-\", \"\")\n",
    "    weighted_avg = f[9].replace(\"wa-\", \"\")\n",
    "    \n",
    "    return satellite, bands, country_code, points, yrs, mns, num_features, limit_months, crop_mask, weighted_avg\n",
    "\n",
    "def merge(x, bases = (tuple, list)):\n",
    "    for e in x:\n",
    "        if type(e) in bases:\n",
    "            for e in merge(e, bases):\n",
    "                yield e\n",
    "        else:\n",
    "            yield e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d12eb13-13d9-4ac5-9128-495bcc51e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(here(\"data\", \"random_features\", 'summary'))\n",
    "files = [f for f in files if f not in ('.gitkeep', '.ipynb_checkpoints')]\n",
    "# files = files[0:8]\n",
    "paramlist = list(itertools.product(files, files))\n",
    "paramlist = [tuple(set(paramlist[i])) for i in range(len(paramlist))]\n",
    "paramlist = [x for x in paramlist if len(x) > 1] \n",
    "paramlist = list(itertools.product(paramlist, [True, False]))\n",
    "for i in range(len(paramlist)):\n",
    "    paramlist[i] = tuple(merge(paramlist[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee8f39bf-3508-4e63-a77f-64ce44c0598d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3784"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_pattern = re.compile(\"20k-points\")\n",
    "wa_pattern = re.compile(\"cm-False\")\n",
    "\n",
    "paramlist = [t for t in paramlist if not (bool(point_pattern.search(t[0])) & bool(wa_pattern.search(t[0])))]\n",
    "paramlist = [t for t in paramlist if not (bool(point_pattern.search(t[1])) & bool(wa_pattern.search(t[1])))]\n",
    "len(paramlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f946597a-20ae-4fb8-a9e1-35dce7e8e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### best-k-fold-2-sensor-params\n",
    "# f1 = 'landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_15k-points_1000-features_yr-2013-2021_mn-4-9_lm-True_cm-False_wa-False_summary.feather'\n",
    "# f2 = 'sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather'\n",
    "# paramlist = [(f1, f2, True)]\n",
    "\n",
    "### best-demean-2-sensor-params\n",
    "# f1 = 'landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_15k-points_1000-features_yr-2014-2021_mn-1-12_lm-False_cm-False_wa-True_summary.feather'\n",
    "# f2 = 'sentinel-2-l2a_bands-2-3-4_ZMB_15k-points_1000-features_yr-2016-2022_mn-4-9_lm-True_cm-False_wa-False_summary.feather'\n",
    "# paramlist = [(f1, f2, True)]\n",
    "\n",
    "### best-avg-2-2-sensor-params\n",
    "# f1 = 'landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_15k-points_1000-features_yr-2013-2021_mn-4-9_lm-True_cm-False_wa-False_summary.feather'\n",
    "# f2 = 'sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather'\n",
    "# paramlist = [(f1, f2, True)]\n",
    "\n",
    "### best-avg-3-2-sensor-params\n",
    "# f1 = 'landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_20k-points_1000-features_yr-2014-2021_mn-1-12_lm-False_cm-True_wa-True_summary.feather'\n",
    "# f2 = 'sentinel-2-l2a_bands-2-3-4_ZMB_20k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather'\n",
    "# paramlist = [(f1, f2, True)]\n",
    "\n",
    "### demean LOGO 2-sensor-params\n",
    "# f1 = 'sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-True_summary.feather'\n",
    "# f2 = 'landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_20k-points_1000-features_yr-2013-2021_mn-4-9_lm-True_cm-True_wa-True_summary.feather'\n",
    "# paramlist = [(f1, f2, True)]\n",
    "\n",
    "# paramlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ae5e44c6-6764-431c-a369-3ea976fc1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for params in paramlist[0:1]:\n",
    "def model_2_sensors(params):\n",
    "    #########################################     SET PARAMS    #########################################    \n",
    "    f1         = params[0]\n",
    "    f2         = params[1]\n",
    "    hot_encode = params[2]\n",
    "\n",
    "    satellite1, bands1, country_code, points1, yrs1, mns1,\\\n",
    "    num_features1, limit_months1, crop_mask1, weighted_avg1 = split_fn(f1)\n",
    "    \n",
    "    satellite2, bands2, country_code, points2, yrs2, mns2,\\\n",
    "    num_features2, limit_months2, crop_mask2, weighted_avg2 = split_fn(f2)\n",
    "    \n",
    "    #########################################     READ DATA    #########################################\n",
    "    features_1 = pd.read_feather(here('data', 'random_features', 'summary', f1))\n",
    "    features_2 = pd.read_feather(here('data', 'random_features', 'summary', f2))\n",
    "    \n",
    "    #########################################     CLEAN DATA    #########################################  \n",
    "    min_year = max(min(features_1.year), min(features_2.year))\n",
    "    max_year = min(max(features_1.year), max(features_2.year))\n",
    "    \n",
    "    features_1 = features_1[features_1.year >= min_year]\n",
    "    features_2 = features_2[features_2.year >= min_year]\n",
    "    \n",
    "    features_1 = features_1[features_1.year <= max_year]\n",
    "    features_2 = features_2[features_2.year <= max_year]\n",
    "    \n",
    "    features_1.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "    features_2.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "    \n",
    "    index_cols = ['district', 'year', 'yield_mt']\n",
    "    \n",
    "    features_1 = features_1.set_index(index_cols).add_prefix(\"f1_\")\n",
    "    features_2 = features_2.set_index(index_cols).add_prefix(\"f2_\")\n",
    "    \n",
    "    #########################################     JOIN DATA    #########################################  \n",
    "    features = features_1.join(features_2).reset_index()\n",
    "    \n",
    "    features = features[~features.isna().any(axis = 1)]\n",
    "    \n",
    "    crop_yield = features.copy().loc[:, tuple(index_cols)]\n",
    "    \n",
    "    if hot_encode:\n",
    "        index_cols.remove('district')\n",
    "        features = pd.get_dummies(features, columns = [\"district\"], drop_first = False)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    #########################################     SPLIT DATA    #########################################\n",
    "    x_all = features.drop(index_cols, axis=1)\n",
    "    x_all = StandardScaler().fit_transform(x_all)\n",
    "    x_all = pd.DataFrame(x_all)\n",
    "    y_all = np.log10(features.yield_mt.to_numpy() + 1)\n",
    "    g_all = features.year.ravel()\n",
    "    \n",
    "    x_train, x_test,\\\n",
    "    y_train, y_test,\\\n",
    "    g_train, g_test = train_test_split(x_all, y_all, g_all, test_size = 0.2, random_state = 0)\n",
    "\n",
    "    #########################################     K-FOLD CV    ###########################################\n",
    "    # ridge_kfold_cv = RidgeCV(cv = 5, alphas = np.logspace(-8, 8, base = 10, num = 17))\n",
    "    # ridge_kfold_cv.fit(x_train, y_train)\n",
    "    kfold = KFold()\n",
    "    ridge = Ridge()\n",
    "    parameters = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "    ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "    ridge_kfold_reg = GridSearchCV(ridge, parameters, scoring = 'r2', cv = kfold)\n",
    "    ridge_kfold_reg.fit(x_train, y_train)\n",
    "    best_kfold_model = ridge_kfold_reg.best_estimator_\n",
    "    ### CV PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "    kfold_val_predictions = cross_val_predict(best_kfold_model, X = x_train, y = y_train, cv = kfold)     \n",
    "    kfold_val_predictions = np.maximum(kfold_val_predictions, 0) \n",
    "    ### TRAIN BEST MODEL AND PREDICT\n",
    "    best_kfold_model.fit(x_train, y_train)\n",
    "    y_pred_train = np.maximum(best_kfold_model.predict(x_train), 0)\n",
    "    y_pred_test  = np.maximum(best_kfold_model.predict(x_test), 0)\n",
    "\n",
    "    #########################################     LOGO CV    ###########################################\n",
    "    logo = LeaveOneGroupOut()\n",
    "    ridge = Ridge()\n",
    "    parameters = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "    ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "    ridge_logo_reg = GridSearchCV(ridge, parameters, scoring = 'r2', cv = logo)\n",
    "    ridge_logo_reg.fit(x_all, y_all, groups = g_all)\n",
    "    best_logo_model = ridge_logo_reg.best_estimator_\n",
    "    ### CV PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "    logo_val_predictions = cross_val_predict(best_logo_model, X = x_all, y = y_all, groups = g_all,  cv = logo)      \n",
    "    logo_val_predictions = np.maximum(logo_val_predictions, 0)\n",
    "    \n",
    "    #########################################     DE-MEAN R2    #########################################    \n",
    "    crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "    crop_yield[\"district_yield_mean\"] = crop_yield.groupby('district')['log_yield'].transform('mean')\n",
    "    crop_yield[\"demean_yield\"] = crop_yield[\"log_yield\"] - crop_yield[\"district_yield_mean\"]\n",
    "    \n",
    "    crop_yield[\"k-fold_prediction\"] = np.maximum(best_kfold_model.predict(x_all), 0)\n",
    "    crop_yield[\"k-fold_district_prediction_mean\"] = crop_yield.groupby('district')['k-fold_prediction'].transform('mean')\n",
    "    crop_yield[\"k-fold_demean_prediction\"] = crop_yield[\"k-fold_prediction\"] - crop_yield[\"k-fold_district_prediction_mean\"]\n",
    "    \n",
    "    crop_yield[\"logo_prediction\"] = np.maximum(best_logo_model.predict(x_all), 0)\n",
    "    crop_yield[\"logo_district_prediction_mean\"] = crop_yield.groupby('district')['logo_prediction'].transform('mean')\n",
    "    crop_yield[\"logo_demean_prediction\"] = crop_yield[\"logo_prediction\"] - crop_yield[\"logo_district_prediction_mean\"]\n",
    "    \n",
    "    train = pd.DataFrame(x_train)\n",
    "    test = pd.DataFrame(x_test)\n",
    "    train['k-fold_cv_predictions'] = kfold_val_predictions\n",
    "    train['split'], test['split'] = 'train', 'test'\n",
    "    train_test = pd.concat([train, test])[['split','k-fold_cv_predictions']]\n",
    "\n",
    "    crop_yield = crop_yield.join(train_test)\n",
    "    crop_yield[\"logo_cv_prediction\"] = logo_val_predictions\n",
    "    \n",
    "    #########################################     SAVE MODELS   #########################################  \n",
    "    fn_1 = f'{satellite1[0]}_{bands1}_{points1}_{limit_months1}_{crop_mask1}_{weighted_avg1}'\n",
    "    fn_2 = f'{satellite2[0]}_{bands2}_{points2}_{limit_months2}_{crop_mask2}_{weighted_avg2}'\n",
    "    # fn_1 = f1.replace('_summary.feather', '')\n",
    "    # fn_2 = f2.replace('_summary.feather', '')\n",
    "    model_fn_suffix = f'fn-1_{fn_1}_fn-2_{fn_2}'\n",
    "    # model_fn_suffix = 'best-avg-3-2-sensor-params'\n",
    "    k_model_fn = f'k-fold-cv_rr-model_{model_fn_suffix}_he-{hot_encode}.pkl'\n",
    "    logo_model_fn = f'logo-cv_rr-model_{model_fn_suffix}_he-{hot_encode}.pkl'\n",
    "    \n",
    "    with open(here('models', k_model_fn),'wb') as f:\n",
    "        pickle.dump(best_kfold_model, f)\n",
    "        \n",
    "    with open(here('models', logo_model_fn),'wb') as f:\n",
    "        pickle.dump(best_logo_model, f)\n",
    "        \n",
    "    #########################################     SAVE RESULTS    #########################################\n",
    "    d = {\n",
    "        'country': country_code,\n",
    "        \n",
    "        'satellite_1': satellite1[0],\n",
    "        'bands_1': bands1,\n",
    "        'num_features_1': num_features1,\n",
    "        'points_1': points1, \n",
    "        'month_range_1': mns1,\n",
    "        'limit_months_1': limit_months1,\n",
    "        'crop_mask_1': crop_mask1,\n",
    "        'weighted_avg_1': weighted_avg1,\n",
    "        \n",
    "        'satellite_2': satellite2[0],\n",
    "        'bands_2': bands2,\n",
    "        'num_features_2': num_features2,\n",
    "        'points_2': points2, \n",
    "        'month_range_2': mns2,\n",
    "        'limit_months_2': limit_months2,\n",
    "        'crop_mask_2': crop_mask2,\n",
    "        'weighted_avg_2': weighted_avg2,\n",
    "\n",
    "        'hot_encode': hot_encode,\n",
    "        \n",
    "        'total_n': len(x_all),\n",
    "        'train_n': len(x_train),\n",
    "        'test_n': len(x_test),\n",
    "        \n",
    "        'kfold_best_reg_param': list(ridge_kfold_reg.best_params_.values())[0],\n",
    "        'kfold_mean_of_val_R2s': ridge_kfold_reg.best_score_,\n",
    "        'kfold_val_R2': r2_score(y_train, kfold_val_predictions),\n",
    "        'kfold_val_r' : pearsonr(kfold_val_predictions, y_train)[0],\n",
    "        'kfold_val_r2' : pearsonr(kfold_val_predictions, y_train)[0] ** 2,\n",
    "        \n",
    "        'kfold_train_R2': r2_score(y_train, y_pred_train),\n",
    "        'kfold_train_r': pearsonr(y_pred_train, y_train)[0],\n",
    "        'kfold_train_r2': pearsonr(y_pred_train, y_train)[0] ** 2,\n",
    "        \n",
    "        'kfold_test_R2': r2_score(y_test, y_pred_test),\n",
    "        'kfold_test_r': pearsonr(y_pred_test, y_test)[0],\n",
    "        'kfold_test_r2': pearsonr(y_pred_test, y_test)[0] ** 2,\n",
    "        \n",
    "        'logo_best_reg_param': list(ridge_logo_reg.best_params_.values())[0],      \n",
    "        'logo_mean_of_val_R2s' : ridge_logo_reg.best_score_,\n",
    "        'logo_val_R2' : r2_score(y_all, logo_val_predictions),\n",
    "        'logo_val_r' : pearsonr(logo_val_predictions, y_all)[0],\n",
    "        'logo_val_r2' : pearsonr(logo_val_predictions, y_all)[0] ** 2,\n",
    "        \n",
    "        'kfold_demean_R2': r2_score(crop_yield[\"demean_yield\"], crop_yield[\"k-fold_demean_prediction\"]),\n",
    "        'kfold_demean_r':  pearsonr(crop_yield[\"demean_yield\"], crop_yield[\"k-fold_demean_prediction\"])[0],\n",
    "        'kfold_demean_r2': pearsonr(crop_yield[\"demean_yield\"], crop_yield[\"k-fold_demean_prediction\"])[0] ** 2,\n",
    "        \n",
    "        'logo_demean_R2': r2_score(crop_yield[\"demean_yield\"], crop_yield[\"logo_demean_prediction\"]),\n",
    "        'logo_demean_r':  pearsonr(crop_yield[\"demean_yield\"], crop_yield[\"logo_demean_prediction\"])[0],\n",
    "        'logo_demean_r2': pearsonr(crop_yield[\"demean_yield\"], crop_yield[\"logo_demean_prediction\"])[0] ** 2,\n",
    "    }\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # return df\n",
    "    return crop_yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f752cfdc-7cca-4ce2-9841-ac6118f0bbb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02834292d61b4ee9b195b9a0bd8e7ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results as: 2_sensor_results_2022-11-12.csv\n",
      "\n",
      "\n",
      "CPU times: user 63.2 ms, sys: 25.5 ms, total: 88.7 ms\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time     \n",
    "##### With progress bar\n",
    "workers = os.cpu_count()\n",
    "if __name__ == \"__main__\":\n",
    "    output = []\n",
    "    for result in p_tqdm.p_map(model_2_sensors, paramlist):\n",
    "        output.append(result)\n",
    "    results = pd.concat(output).reset_index(drop=True)\n",
    "    today = date.today().strftime(\"%Y-%m-%d\")\n",
    "    file_name = f'2_sensor_results_{today}.csv'\n",
    "    print(f\"Saving results as: {file_name}\\n\\n\")           \n",
    "    results.to_csv(here(\"data\",\"results\", file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "72f16726-04d2-447b-a105-2a6c14742925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district</th>\n",
       "      <th>year</th>\n",
       "      <th>yield_mt</th>\n",
       "      <th>log_yield</th>\n",
       "      <th>district_yield_mean</th>\n",
       "      <th>demean_yield</th>\n",
       "      <th>k-fold_prediction</th>\n",
       "      <th>k-fold_district_prediction_mean</th>\n",
       "      <th>k-fold_demean_prediction</th>\n",
       "      <th>logo_prediction</th>\n",
       "      <th>logo_district_prediction_mean</th>\n",
       "      <th>logo_demean_prediction</th>\n",
       "      <th>split</th>\n",
       "      <th>k-fold_cv_predictions</th>\n",
       "      <th>logo_cv_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chadiza</td>\n",
       "      <td>2016</td>\n",
       "      <td>1.876427</td>\n",
       "      <td>0.458853</td>\n",
       "      <td>0.503862</td>\n",
       "      <td>-0.045009</td>\n",
       "      <td>0.455952</td>\n",
       "      <td>0.469358</td>\n",
       "      <td>-0.013405</td>\n",
       "      <td>0.458853</td>\n",
       "      <td>0.503862</td>\n",
       "      <td>-0.045009</td>\n",
       "      <td>train</td>\n",
       "      <td>0.423890</td>\n",
       "      <td>0.350585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chadiza</td>\n",
       "      <td>2017</td>\n",
       "      <td>2.882947</td>\n",
       "      <td>0.589161</td>\n",
       "      <td>0.503862</td>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.421962</td>\n",
       "      <td>0.469358</td>\n",
       "      <td>-0.047396</td>\n",
       "      <td>0.589161</td>\n",
       "      <td>0.503862</td>\n",
       "      <td>0.085299</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.436503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chadiza</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.299279</td>\n",
       "      <td>0.361592</td>\n",
       "      <td>0.503862</td>\n",
       "      <td>-0.142270</td>\n",
       "      <td>0.410220</td>\n",
       "      <td>0.469358</td>\n",
       "      <td>-0.059138</td>\n",
       "      <td>0.361592</td>\n",
       "      <td>0.503862</td>\n",
       "      <td>-0.142270</td>\n",
       "      <td>train</td>\n",
       "      <td>0.424618</td>\n",
       "      <td>0.442388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chadiza</td>\n",
       "      <td>2019</td>\n",
       "      <td>2.131008</td>\n",
       "      <td>0.495684</td>\n",
       "      <td>0.503862</td>\n",
       "      <td>-0.008178</td>\n",
       "      <td>0.441056</td>\n",
       "      <td>0.469358</td>\n",
       "      <td>-0.028302</td>\n",
       "      <td>0.495684</td>\n",
       "      <td>0.503862</td>\n",
       "      <td>-0.008178</td>\n",
       "      <td>train</td>\n",
       "      <td>0.394740</td>\n",
       "      <td>0.509221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chadiza</td>\n",
       "      <td>2020</td>\n",
       "      <td>2.626538</td>\n",
       "      <td>0.559492</td>\n",
       "      <td>0.503862</td>\n",
       "      <td>0.055630</td>\n",
       "      <td>0.537070</td>\n",
       "      <td>0.469358</td>\n",
       "      <td>0.067712</td>\n",
       "      <td>0.559492</td>\n",
       "      <td>0.503862</td>\n",
       "      <td>0.055630</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.692717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>Zambezi</td>\n",
       "      <td>2017</td>\n",
       "      <td>1.490456</td>\n",
       "      <td>0.396279</td>\n",
       "      <td>0.436595</td>\n",
       "      <td>-0.040316</td>\n",
       "      <td>0.417738</td>\n",
       "      <td>0.430644</td>\n",
       "      <td>-0.012906</td>\n",
       "      <td>0.396279</td>\n",
       "      <td>0.436595</td>\n",
       "      <td>-0.040316</td>\n",
       "      <td>train</td>\n",
       "      <td>0.467941</td>\n",
       "      <td>0.511513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>Zambezi</td>\n",
       "      <td>2018</td>\n",
       "      <td>1.622273</td>\n",
       "      <td>0.418678</td>\n",
       "      <td>0.436595</td>\n",
       "      <td>-0.017917</td>\n",
       "      <td>0.428774</td>\n",
       "      <td>0.430644</td>\n",
       "      <td>-0.001870</td>\n",
       "      <td>0.418678</td>\n",
       "      <td>0.436595</td>\n",
       "      <td>-0.017917</td>\n",
       "      <td>train</td>\n",
       "      <td>0.487973</td>\n",
       "      <td>0.486318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>Zambezi</td>\n",
       "      <td>2019</td>\n",
       "      <td>1.184923</td>\n",
       "      <td>0.339436</td>\n",
       "      <td>0.436595</td>\n",
       "      <td>-0.097159</td>\n",
       "      <td>0.388848</td>\n",
       "      <td>0.430644</td>\n",
       "      <td>-0.041796</td>\n",
       "      <td>0.339436</td>\n",
       "      <td>0.436595</td>\n",
       "      <td>-0.097159</td>\n",
       "      <td>train</td>\n",
       "      <td>0.440824</td>\n",
       "      <td>0.483137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>Zambezi</td>\n",
       "      <td>2020</td>\n",
       "      <td>1.689628</td>\n",
       "      <td>0.429692</td>\n",
       "      <td>0.436595</td>\n",
       "      <td>-0.006903</td>\n",
       "      <td>0.435576</td>\n",
       "      <td>0.430644</td>\n",
       "      <td>0.004932</td>\n",
       "      <td>0.429692</td>\n",
       "      <td>0.436595</td>\n",
       "      <td>-0.006903</td>\n",
       "      <td>train</td>\n",
       "      <td>0.451039</td>\n",
       "      <td>0.589586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>Zambezi</td>\n",
       "      <td>2021</td>\n",
       "      <td>2.981741</td>\n",
       "      <td>0.600073</td>\n",
       "      <td>0.436595</td>\n",
       "      <td>0.163478</td>\n",
       "      <td>0.464451</td>\n",
       "      <td>0.430644</td>\n",
       "      <td>0.033807</td>\n",
       "      <td>0.600073</td>\n",
       "      <td>0.436595</td>\n",
       "      <td>0.163478</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.475019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    district  year  yield_mt  log_yield  district_yield_mean  demean_yield  \\\n",
       "0    Chadiza  2016  1.876427   0.458853             0.503862     -0.045009   \n",
       "1    Chadiza  2017  2.882947   0.589161             0.503862      0.085299   \n",
       "2    Chadiza  2018  1.299279   0.361592             0.503862     -0.142270   \n",
       "3    Chadiza  2019  2.131008   0.495684             0.503862     -0.008178   \n",
       "4    Chadiza  2020  2.626538   0.559492             0.503862      0.055630   \n",
       "..       ...   ...       ...        ...                  ...           ...   \n",
       "415  Zambezi  2017  1.490456   0.396279             0.436595     -0.040316   \n",
       "416  Zambezi  2018  1.622273   0.418678             0.436595     -0.017917   \n",
       "417  Zambezi  2019  1.184923   0.339436             0.436595     -0.097159   \n",
       "418  Zambezi  2020  1.689628   0.429692             0.436595     -0.006903   \n",
       "419  Zambezi  2021  2.981741   0.600073             0.436595      0.163478   \n",
       "\n",
       "     k-fold_prediction  k-fold_district_prediction_mean  \\\n",
       "0             0.455952                         0.469358   \n",
       "1             0.421962                         0.469358   \n",
       "2             0.410220                         0.469358   \n",
       "3             0.441056                         0.469358   \n",
       "4             0.537070                         0.469358   \n",
       "..                 ...                              ...   \n",
       "415           0.417738                         0.430644   \n",
       "416           0.428774                         0.430644   \n",
       "417           0.388848                         0.430644   \n",
       "418           0.435576                         0.430644   \n",
       "419           0.464451                         0.430644   \n",
       "\n",
       "     k-fold_demean_prediction  logo_prediction  logo_district_prediction_mean  \\\n",
       "0                   -0.013405         0.458853                       0.503862   \n",
       "1                   -0.047396         0.589161                       0.503862   \n",
       "2                   -0.059138         0.361592                       0.503862   \n",
       "3                   -0.028302         0.495684                       0.503862   \n",
       "4                    0.067712         0.559492                       0.503862   \n",
       "..                        ...              ...                            ...   \n",
       "415                 -0.012906         0.396279                       0.436595   \n",
       "416                 -0.001870         0.418678                       0.436595   \n",
       "417                 -0.041796         0.339436                       0.436595   \n",
       "418                  0.004932         0.429692                       0.436595   \n",
       "419                  0.033807         0.600073                       0.436595   \n",
       "\n",
       "     logo_demean_prediction  split  k-fold_cv_predictions  logo_cv_prediction  \n",
       "0                 -0.045009  train               0.423890            0.350585  \n",
       "1                  0.085299   test                    NaN            0.436503  \n",
       "2                 -0.142270  train               0.424618            0.442388  \n",
       "3                 -0.008178  train               0.394740            0.509221  \n",
       "4                  0.055630   test                    NaN            0.692717  \n",
       "..                      ...    ...                    ...                 ...  \n",
       "415               -0.040316  train               0.467941            0.511513  \n",
       "416               -0.017917  train               0.487973            0.486318  \n",
       "417               -0.097159  train               0.440824            0.483137  \n",
       "418               -0.006903  train               0.451039            0.589586  \n",
       "419                0.163478   test                    NaN            0.475019  \n",
       "\n",
       "[420 rows x 15 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b5ef0e-b240-4e6a-a38d-5fb1a2b5e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time     \n",
    "##### No progress bar\n",
    "# workers = os.cpu_count()\n",
    "# if __name__ == \"__main__\":\n",
    "#     with multiprocessing.Pool(processes=workers) as pool:\n",
    "#         output = []\n",
    "#         for result in pool.imap_unordered(model_2_sensors, paramlist):\n",
    "#             output.append(result)\n",
    "#     results = pd.concat(output).reset_index(drop=True)\n",
    "#     today = date.today().strftime(\"%Y-%m-%d\")\n",
    "#     file_name = f'2_sensor_results_{today}.csv'\n",
    "#     print(f\"Saving results as: {file_name}\\n\\n\")           \n",
    "#     results.to_csv(here(\"data\",\"results\", file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fd857619-7fd7-4fa8-a12b-f9263d64a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = 'sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-True_summary.feather'\n",
    "# f2 = 'landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_20k-points_1000-features_yr-2013-2021_mn-4-9_lm-True_cm-True_wa-True_summary.feather'\n",
    "# hot_encode = True\n",
    "\n",
    "# satellite1, bands1, country_code, points1, yrs1, mns1,\\\n",
    "# num_features1, limit_months1, crop_mask1, weighted_avg1 = split_fn(f1)\n",
    "\n",
    "# satellite2, bands2, country_code, points2, yrs2, mns2,\\\n",
    "# num_features2, limit_months2, crop_mask2, weighted_avg2 = split_fn(f2)\n",
    "\n",
    "# #########################################     READ DATA    #########################################\n",
    "# features_1 = pd.read_feather(here('data', 'random_features', 'summary', f1))\n",
    "# features_2 = pd.read_feather(here('data', 'random_features', 'summary', f2))\n",
    "\n",
    "# #########################################     CLEAN DATA    #########################################  \n",
    "# min_year = max(min(features_1.year), min(features_2.year))\n",
    "# max_year = min(max(features_1.year), max(features_2.year))\n",
    "\n",
    "# features_1 = features_1[features_1.year >= min_year]\n",
    "# features_2 = features_2[features_2.year >= min_year]\n",
    "\n",
    "# features_1 = features_1[features_1.year <= max_year]\n",
    "# features_2 = features_2[features_2.year <= max_year]\n",
    "\n",
    "# features_1.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "# features_2.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "# index_cols = ['district', 'year', 'yield_mt']\n",
    "\n",
    "# features_1 = features_1.set_index(index_cols).add_prefix(\"f1_\")\n",
    "# features_2 = features_2.set_index(index_cols).add_prefix(\"f2_\")\n",
    "\n",
    "# #########################################     JOIN DATA    #########################################  \n",
    "# features = features_1.join(features_2).reset_index()\n",
    "\n",
    "# features = features[~features.isna().any(axis = 1)]\n",
    "\n",
    "# crop_yield = features.copy().loc[:, tuple(index_cols)]\n",
    "\n",
    "# if hot_encode:\n",
    "#     index_cols.remove('district')\n",
    "#     features = pd.get_dummies(features, columns = [\"district\"], drop_first = False)\n",
    "# else:\n",
    "#     pass\n",
    "\n",
    "# #########################################     SPLIT DATA    #########################################\n",
    "# x_all = features.drop(index_cols, axis=1)\n",
    "# x_all = StandardScaler().fit_transform(x_all)\n",
    "# x_all = pd.DataFrame(x_all)\n",
    "# y_all = np.log10(features.yield_mt.to_numpy() + 1)\n",
    "# g_all = features.year.ravel()\n",
    "\n",
    "# x_train, x_test,\\\n",
    "# y_train, y_test,\\\n",
    "# g_train, g_test = train_test_split(x_all, y_all, g_all, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# #########################################     K-FOLD CV    ###########################################\n",
    "# # ridge_kfold_cv = RidgeCV(cv = 5, alphas = np.logspace(-8, 8, base = 10, num = 17))\n",
    "# # ridge_kfold_cv.fit(x_train, y_train)\n",
    "# kfold = KFold()\n",
    "# ridge = Ridge()\n",
    "# parameters = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "# ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "# ridge_kfold_reg = GridSearchCV(ridge, parameters, scoring = 'r2', cv = kfold)\n",
    "# ridge_kfold_reg.fit(x_train, y_train)\n",
    "# best_kfold_model = ridge_kfold_reg.best_estimator_\n",
    "# ### CV PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "# kfold_val_predictions = cross_val_predict(best_kfold_model, X = x_train, y = y_train, cv = kfold)   \n",
    "# ### TRAIN BEST MODEL AND PREDICT\n",
    "# best_kfold_model.fit(x_train, y_train)\n",
    "# y_pred_train  = np.maximum(best_kfold_model.predict(x_train), 0)\n",
    "# y_pred_test   = np.maximum(best_kfold_model.predict(x_test), 0)\n",
    "\n",
    "# #########################################     LOGO CV    ###########################################\n",
    "# logo = LeaveOneGroupOut()\n",
    "# ridge = Ridge()\n",
    "# parameters = {'alpha': np.logspace(-8, 8, base = 10, num = 17)}\n",
    "# ### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER\n",
    "# ridge_logo_reg = GridSearchCV(ridge, parameters, scoring = 'r2', cv = logo)\n",
    "# ridge_logo_reg.fit(x_all, y_all, groups = g_all)\n",
    "# best_logo_model = ridge_logo_reg.best_estimator_\n",
    "# ### CV PREDICT - PREDICTING WITH BEST HYPERPARAMETER\n",
    "# logo_val_predictions = cross_val_predict(best_logo_model, X = x_all, y = y_all, groups = g_all,  cv = logo)   \n",
    "\n",
    "# #########################################     DE-MEAN R2    #########################################    \n",
    "# crop_yield[\"log_yield\"] = np.log10(crop_yield.yield_mt.to_numpy() + 1)\n",
    "# crop_yield[\"district_yield_mean\"] = crop_yield.groupby('district')['log_yield'].transform('mean')\n",
    "# crop_yield[\"demean_yield\"] = crop_yield[\"log_yield\"] - crop_yield[\"district_yield_mean\"]\n",
    "\n",
    "# crop_yield[\"k-fold_prediction\"] = np.maximum(best_kfold_model.predict(x_all), 0)\n",
    "# crop_yield[\"k-fold_district_prediction_mean\"] = crop_yield.groupby('district')['k-fold_prediction'].transform('mean')\n",
    "# crop_yield[\"k-fold_demean_prediction\"] = crop_yield[\"k-fold_prediction\"] - crop_yield[\"k-fold_district_prediction_mean\"]\n",
    "\n",
    "# crop_yield[\"logo_prediction\"] = np.maximum(best_logo_model.predict(x_all), 0)\n",
    "# crop_yield[\"logo_district_prediction_mean\"] = crop_yield.groupby('district')['logo_prediction'].transform('mean')\n",
    "# crop_yield[\"logo_demean_prediction\"] = crop_yield[\"logo_prediction\"] - crop_yield[\"logo_district_prediction_mean\"]\n",
    "\n",
    "# train = pd.DataFrame(x_train)\n",
    "# test = pd.DataFrame(x_test)\n",
    "# train['k-fold_cv_predictions'] = kfold_val_predictions\n",
    "# train['split'], test['split'] = 'train', 'test'\n",
    "# train_test = pd.concat([train, test])[['split','k-fold_cv_predictions']]\n",
    "\n",
    "# crop_yield = crop_yield.join(train_test)\n",
    "# crop_yield[\"logo_cv_prediction\"] = logo_val_predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
