{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f576830d-9afa-47d8-9fac-8e43292ee090",
   "metadata": {},
   "source": [
    "# Modeling Crop Yield: Landsat + Sentinel\n",
    "## Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "425d2ec4-79b2-4fd1-a800-14db43e42ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44646286-2094-4bd0-8609-ad5efc857abc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "Failed to create a completion queue (CQ):\n",
      "\n",
      "Hostname: braid2\n",
      "Requested CQE: 16384\n",
      "Error:    Cannot allocate memory\n",
      "\n",
      "Check the CQE attribute.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "Open MPI has detected that there are UD-capable Verbs devices on your\n",
      "system, but none of them were able to be setup properly.  This may\n",
      "indicate a problem on this system.\n",
      "\n",
      "You job will continue, but Open MPI will ignore the \"ud\" oob component\n",
      "in this run.\n",
      "\n",
      "Hostname: braid2\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "No OpenFabrics connection schemes reported that they were able to be\n",
      "used on a specific port.  As such, the openib BTL (OpenFabrics\n",
      "support) will be disabled for this port.\n",
      "\n",
      "  Local host:           braid2\n",
      "  Local device:         mlx4_0\n",
      "  Local port:           1\n",
      "  CPCs attempted:       rdmacm, udcm\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pyhere import here\n",
    "import random\n",
    "\n",
    "import itertools\n",
    "\n",
    "from task_modeling_utils import *\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25b353dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 'landsat-c2-l2_bands-r-g-b-nir-swir16-swir22_ZMB_20k-points_1024-features_yr-2009-2021_mn-4-9_lm-True_cm-True_wa-False_summary.feather',\n",
       " 'f2': 'sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather',\n",
       " 'he': True,\n",
       " 'anomaly': False,\n",
       " 'split': 0,\n",
       " 'random_state': 670487,\n",
       " 'include_climate': False,\n",
       " 'variable_groups': None,\n",
       " 'n_splits': 5,\n",
       " 'return_oos_predictions': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = \"landsat-c2-l2_bands-r-g-b-nir-swir16-swir22_ZMB_20k-points_1024-features_yr-2009-2021_mn-4-9_lm-True_cm-True_wa-False_summary.feather\"\n",
    "f2 = \"sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather\"\n",
    "\n",
    "# f1 = \"landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_15k-points_1000-features_yr-2014-2021_mn-1-12_lm-False_cm-True_wa-False_summary.feather\"\n",
    "# f2 = \"sentinel-2-l2a_bands-2-3-4_ZMB_4k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather\"\n",
    "\n",
    "kwargs = {\n",
    "    \"f1\": f1,\n",
    "    \"f2\": f2,\n",
    "    \"he\": True,\n",
    "    \"anomaly\": False,\n",
    "    \"split\": 0,\n",
    "    \"random_state\": 670487,\n",
    "    \"include_climate\": False,\n",
    "    \"variable_groups\": None,\n",
    "    \"n_splits\": 5,\n",
    "    \"return_oos_predictions\": False,\n",
    "}\n",
    "kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fe4a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# he = True\n",
    "# anomaly = False\n",
    "# split = 0\n",
    "# random_state = 670487\n",
    "# include_climate = False\n",
    "# variable_groups = None\n",
    "# n_splits = 5\n",
    "# return_oos_predictions = False\n",
    "\n",
    "# (\n",
    "#     satellite1,\n",
    "#     bands1,\n",
    "#     country_code,\n",
    "#     points1,\n",
    "#     yrs1,\n",
    "#     mns1,\n",
    "#     num_features1,\n",
    "#     limit_months1,\n",
    "#     crop_mask1,\n",
    "#     weighted_avg1,\n",
    "# ) = split_fn(f1)\n",
    "\n",
    "# (\n",
    "#     satellite2,\n",
    "#     bands2,\n",
    "#     country_code,\n",
    "#     points2,\n",
    "#     yrs2,\n",
    "#     mns2,\n",
    "#     num_features2,\n",
    "#     limit_months2,\n",
    "#     crop_mask2,\n",
    "#     weighted_avg2,\n",
    "# ) = split_fn(f2)\n",
    "\n",
    "# if variable_groups is None:\n",
    "#     variable_groups_str = \"rcf\"\n",
    "# else:\n",
    "#     variable_groups_str = \"_\".join(variable_groups)\n",
    "#     variable_groups_str = \"rcf_\" + variable_groups_str\n",
    "\n",
    "\n",
    "# #########################################     READ DATA    #########################################\n",
    "# features_1 = pd.read_feather(here(\"data\", \"random_features\", \"summary\", f1))\n",
    "# features_2 = pd.read_feather(here(\"data\", \"random_features\", \"summary\", f2))\n",
    "# if include_climate:\n",
    "#     climate_df = pd.read_csv(here(\"data\", \"climate\", \"climate_summary.csv\"))\n",
    "\n",
    "# #########################################     CLEAN DATA    #########################################\n",
    "# min_year = max(min(features_1.year), min(features_2.year))\n",
    "# max_year = min(max(features_1.year), max(features_2.year))\n",
    "\n",
    "# features_1 = features_1[features_1.year >= min_year]\n",
    "# features_2 = features_2[features_2.year >= min_year]\n",
    "\n",
    "# features_1 = features_1[features_1.year <= max_year]\n",
    "# features_2 = features_2[features_2.year <= max_year]\n",
    "\n",
    "# features_1.drop([\"crop_perc\"], axis=1, errors=\"ignore\", inplace=True)\n",
    "# features_2.drop([\"crop_perc\"], axis=1, errors=\"ignore\", inplace=True)\n",
    "\n",
    "# #########################################     JOIN FEATURES    #########################################\n",
    "# drop_cols = [\"district\", \"year\", \"yield_mt\"]\n",
    "\n",
    "# features_1 = features_1.set_index(drop_cols).add_prefix(\"f1_\")\n",
    "# features_2 = features_2.set_index(drop_cols).add_prefix(\"f2_\")\n",
    "\n",
    "# features = features_1.join(features_2).reset_index()\n",
    "# features = features[~features.isna().any(axis=1)]\n",
    "\n",
    "# features[\"log_yield\"] = np.log10(features[\"yield_mt\"] + 1)\n",
    "\n",
    "# features[\"demean_log_yield\"] = features.log_yield - features.groupby(\n",
    "#     \"district\"\n",
    "# ).log_yield.transform(\"mean\")\n",
    "\n",
    "# #########################################    JOIN CLIMATE VARS    #########################################\n",
    "# if include_climate:\n",
    "#     keep_cols = []\n",
    "\n",
    "#     for var in variable_groups:\n",
    "#         tmp = climate_df.columns[\n",
    "#             climate_df.columns.to_series().str.contains(var)\n",
    "#         ].tolist()\n",
    "#         keep_cols.append(tmp)\n",
    "\n",
    "#     keep_cols = [*drop_cols, *[col for cols in keep_cols for col in cols]]\n",
    "\n",
    "#     climate_df = climate_df.loc[:, keep_cols]\n",
    "\n",
    "#     features = (\n",
    "#         features.set_index(drop_cols)\n",
    "#         .join(climate_df.set_index(drop_cols))\n",
    "#         .reset_index()\n",
    "#     )\n",
    "#     features = features[features.year <= max(climate_df.year)]\n",
    "\n",
    "# drop_cols.append(\"log_yield\")\n",
    "# drop_cols.append(\"demean_log_yield\")\n",
    "\n",
    "# #########################################     CALCULATE ANOMALY   #########################################\n",
    "# if anomaly:\n",
    "#     features.set_index([\"year\", \"district\"], inplace=True)\n",
    "#     var_cols = features.columns\n",
    "#     features = features[var_cols] - features.groupby([\"district\"], as_index=True)[\n",
    "#         var_cols\n",
    "#     ].transform(\"mean\")\n",
    "#     features.reset_index(drop=False, inplace=True)\n",
    "# else:\n",
    "#     pass\n",
    "\n",
    "# #########################################     CLEAN AND COPY    #########################################\n",
    "# yrs = f\"{min(features.year)}-{max(features.year)}\"\n",
    "# n_fts_1 = features_1.shape[1]\n",
    "# n_fts_2 = features_2.shape[1]\n",
    "# n_districts = len(features.district.unique())\n",
    "\n",
    "# if include_climate:\n",
    "#     n_climate_cols = climate_df.shape[1] - len(drop_cols)\n",
    "\n",
    "#     i = 0\n",
    "#     n_climate_groups = []\n",
    "#     for cols in range(n_climate_cols):\n",
    "#         if cols % 12 == 0:\n",
    "#             i += 1\n",
    "#             n_climate_groups.append(i)\n",
    "#     n_climate_groups\n",
    "\n",
    "# crop_yield = features.copy().loc[:, tuple(drop_cols)]\n",
    "\n",
    "# #########################################    HOT ENCODE    #########################################\n",
    "# if he:\n",
    "#     drop_cols.remove(\"district\")\n",
    "#     features = pd.get_dummies(\n",
    "#         features, columns=[\"district\"], drop_first=False, dtype=float\n",
    "#     )\n",
    "# else:\n",
    "#     pass\n",
    "\n",
    "# #########################################     TRAIN/TEST SPLIT    #########################################\n",
    "# x_all = features.drop(drop_cols, axis=1)\n",
    "# if anomaly:\n",
    "#     y_all = features.demean_log_yield\n",
    "# else:\n",
    "#     y_all = features.log_yield\n",
    "# x_train, x_test, y_train, y_test = train_test_split(\n",
    "#     x_all, y_all, test_size=0.2, random_state=random_state\n",
    "# )\n",
    "\n",
    "# features = features.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f606c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.columns.tolist()[23900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad18a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin with paramters:\n",
      "    F1: landsat-c2-l2_bands-r-g-b-nir-swir16-swir22_ZMB_20k-points_1024-features_yr-2009-2021_mn-4-9_lm-True_cm-True_wa-False_summary.feather\n",
      "    F2: sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather\n",
      "    One-hot encoding: True\n",
      "    Anomaly: False\n",
      "    Split: 0\n",
      "    Random state: 670487\n",
      "    N-splits: 5\n",
      "    Include climate: False\n",
      "    Climate vars: rcf\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[braid2.cnsi.ucsb.edu:2562621] 1 more process has sent help message help-oob-ud.txt / create-cq-failed\n",
      "[braid2.cnsi.ucsb.edu:2562621] Set MCA parameter \"orte_base_help_aggregate\" to 0 to see all help / error messages\n",
      "[braid2.cnsi.ucsb.edu:2562621] 1 more process has sent help message help-oob-ud.txt / no-ports-usable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finish:\n",
      "    F1: landsat-c2-l2_bands-r-g-b-nir-swir16-swir22_ZMB_20k-points_1024-features_yr-2009-2021_mn-4-9_lm-True_cm-True_wa-False_summary.feather\n",
      "    F2: sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather\n",
      "    One-hot encoding: True\n",
      "    Anomaly: False\n",
      "    Split: 0\n",
      "    Random state: 670487\n",
      "    N-splits: 5\n",
      "    Include climate: False\n",
      "    Climate vars: rcf\n",
      "    Final Val R2:  0.7928\n",
      "    Final Val r2:  0.7937\n",
      "    Final Test R2: 0.8773\n",
      "    Final Test r2: 0.8878\n",
      "    Demean Val R2:  0.1460\n",
      "    Demean Val r2:  0.2164\n",
      "    Demean Test R2: 0.4936\n",
      "    Demean Test r2: 0.5112\n",
      "    Total time: 28.43 minutes\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "results, model = model_2_sensor(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8748729d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/cmolitor/crop-modeling/code/3_task_modeling/../../models/ridge_model_overall.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model to a file\n",
    "joblib.dump(model, here(\"models\", \"ridge_model_overall.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42575795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later, load the model from the file\n",
    "model_weights = joblib.load(here(\"models\", \"ridge_model_overall.pkl\")).coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a594bcc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18214"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a57a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76258725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26973d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 1\n",
    "random.seed(42)\n",
    "random_seeds = [random.randint(0, 1_000_000) for _ in range(n_splits)]\n",
    "\n",
    "directory = here(\"data\", \"random_features\", \"summary\")\n",
    "files = [\n",
    "    f for f in os.listdir(directory) if f not in (\".gitkeep\", \".ipynb_checkpoints\")\n",
    "]\n",
    "\n",
    "anom = False\n",
    "\n",
    "combinations = list(itertools.combinations(files, 2))\n",
    "kwarg_list = [\n",
    "    {\n",
    "        \"f1\": f1,\n",
    "        \"f2\": f2,\n",
    "        \"he\": he,\n",
    "        \"anomaly\": anom,\n",
    "        \"split\": split,\n",
    "        \"random_state\": random_state,\n",
    "        \"include_climate\": False,\n",
    "        \"variable_groups\": None,\n",
    "        \"n_splits\": 5,\n",
    "        \"return_oos_predictions\": False,\n",
    "    }\n",
    "    for he in [True, False]\n",
    "    for f1, f2 in combinations\n",
    "    for split, random_state in enumerate(random_seeds)\n",
    "]\n",
    "kwarg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "random.seed(42)\n",
    "random_seeds = [random.randint(0, 1_000_000) for _ in range(n_splits)]\n",
    "\n",
    "directory = here(\"data\", \"random_features\", \"summary\")\n",
    "files = [\n",
    "    f for f in os.listdir(directory) if f not in (\".gitkeep\", \".ipynb_checkpoints\")\n",
    "]\n",
    "# files = [f for f in files if not (f.startswith(\"landsat-8\") and \"lm-False\" in f)]\n",
    "# files = [f for f in files if not (f.startswith(\"sentinel\") and \"lm-True\" in f)]\n",
    "files = [f for f in files if \"cm-True\" in f]\n",
    "files = [f for f in files if \"wa-False\" in f]\n",
    "\n",
    "combinations = list(itertools.combinations(files, 2))\n",
    "combinations = [\n",
    "    t for t in combinations if not (\"landsat-c2\" in t[0] and \"landsat-c2\" in t[1])\n",
    "]\n",
    "\n",
    "kwarg_list = [\n",
    "    {\n",
    "        \"f1\": f1,\n",
    "        \"f2\": f2,\n",
    "        \"he\": True,\n",
    "        \"anomaly\": False,\n",
    "        \"split\": split,\n",
    "        \"random_state\": random_state,\n",
    "        \"include_climate\": False,\n",
    "        \"variable_groups\": None,\n",
    "        \"n_splits\": 5,\n",
    "        \"return_oos_predictions\": False,\n",
    "    }\n",
    "    for f1, f2 in combinations\n",
    "    for split, random_state in enumerate(random_seeds)\n",
    "]\n",
    "len(kwarg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843084db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, oos_preds = model_2_sensor(\n",
    "    f1=\"landsat-c2-l2_bands-r-g-b-nir-swir16-swir22_ZMB_20k-points_1024-features_yr-2009-2021_mn-4-9_lm-True_cm-True_wa-False_summary.feather\",\n",
    "    f2=\"sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather\",\n",
    "    he=True,\n",
    "    anomaly=True,\n",
    "    split=0,\n",
    "    random_state=116739,\n",
    "    include_climate=True,\n",
    "    variable_groups=[\"ndvi\"],\n",
    "    n_splits=5,\n",
    "    return_oos_predictions=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c5e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "oos = pd.read_csv(\n",
    "    here(\n",
    "        \"data\",\n",
    "        \"results\",\n",
    "        \"2_sensor_oos_predictions_n-splits-10_2023-05-22_rcf_climate.csv\",\n",
    "    )\n",
    ")\n",
    "oos = oos.loc[\n",
    "    (oos.variables == \"ndvi\") & (oos.split == 0) & (oos.random_state == 670487)\n",
    "]\n",
    "oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = [\"district\"]\n",
    "# group = [\"district\", \"data_fold\"]\n",
    "# group = [\"district\", \"val_fold\"]\n",
    "# group = [\"district\", \"val_fold\", \"split\", \"random_state\"]\n",
    "oos[\"demean_log_yield\"] = oos[\"log_yield\"] - oos.groupby(group)[\"log_yield\"].transform(\n",
    "    \"mean\"\n",
    ")\n",
    "# group.append(\"data_fold\")\n",
    "oos[\"demean_oos_prediction\"] = oos[\"oos_prediction\"] - oos.groupby(group)[\n",
    "    \"log_yield\"\n",
    "].transform(\"mean\")\n",
    "oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = oos.copy()\n",
    "test = test[test.data_fold == \"test\"]\n",
    "train = oos.copy()\n",
    "train = train[train.data_fold == \"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7902ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(train.demean_log_yield, train.demean_oos_prediction), r2_score(\n",
    "    test.demean_log_yield, test.demean_oos_prediction\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c76ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c28c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(test.log_yield, test.oos_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c7adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(\n",
    "    here(\"data\", \"results\", \"2_sensor_top-mod_n-splits-10_2023-05-22_rcf_climate.csv\")\n",
    ")\n",
    "results = results[results.variables == \"ndvi\"]\n",
    "np.mean(results.val_R2)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450fb07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = str(here(\"data\", \"results\", \"2_sensor_top-mod_n-splits-10_*_*.csv\"))\n",
    "files = glob.glob(pathname=file_pattern)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcd8160",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pattern = str(here(\"data\", \"results\", \"2_sensor_top-mod_n-splits-10_*_*.csv\"))\n",
    "files = glob.glob(pathname=file_pattern)\n",
    "results = merge_files(files)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a413ec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(results.val_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b0afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_cols = [\n",
    "    \"country\",\n",
    "    \"year_range\",\n",
    "    \"satellite_1\",\n",
    "    \"bands_1\",\n",
    "    \"num_features_1\",\n",
    "    \"points_1\",\n",
    "    \"month_range_1\",\n",
    "    \"limit_months_1\",\n",
    "    \"crop_mask_1\",\n",
    "    \"weighted_avg_1\",\n",
    "    \"satellite_2\",\n",
    "    \"bands_2\",\n",
    "    \"num_features_2\",\n",
    "    \"points_2\",\n",
    "    \"month_range_2\",\n",
    "    \"limit_months_2\",\n",
    "    \"crop_mask_2\",\n",
    "    \"weighted_avg_2\",\n",
    "    \"hot_encode\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad4aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_summary = results.groupby(groupby_cols, as_index=False).agg(\n",
    "    {\n",
    "        \"val_R2\": \"mean\",\n",
    "        \"test_R2\": \"mean\",\n",
    "        \"demean_cv_R2\": \"mean\",\n",
    "        \"demean_cv_r2\": \"mean\",\n",
    "        \"demean_test_R2\": \"mean\",\n",
    "    }\n",
    ")\n",
    "results_summary = results_summary.sort_values(\"val_R2\", ascending=False)  # .head(20)\n",
    "results_summary.iloc[0:1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94636d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = \"landsat-c2-l2_bands-r-g-b-nir-swir16-swir22_ZMB_20k-points_1024-features_yr-2009-2021_mn-4-9_lm-True_cm-True_wa-False_summary.feather\"\n",
    "f2 = \"sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6689b07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = \"landsat-c2-l2_bands-r-g-b-nir-swir16-swir22_ZMB_20k-points_1024-features_yr-2009-2021_mn-4-9_lm-True_cm-True_wa-True_summary.feather\"\n",
    "pd.read_feather(here(\"data\", \"random_features\", \"summary\", f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c5162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n random seeds\n",
    "n_splits = 10\n",
    "random_seeds = [random.randint(0, 1_000_000) for _ in range(n_splits)]\n",
    "\n",
    "directory = here(\"data\", \"random_features\", \"summary\")\n",
    "files = [\n",
    "    f for f in os.listdir(directory) if f not in (\".gitkeep\", \".ipynb_checkpoints\")\n",
    "]\n",
    "files = [f for f in files if not (f.startswith(\"landsat-8\") and \"lm-False\" in f)]\n",
    "files = [f for f in files if not (f.startswith(\"sentinel\") and \"lm-True\" in f)]\n",
    "files = [f for f in files if \"cm-True\" in f]\n",
    "# files = [f for f in files if \"wa-False\" in f]\n",
    "\n",
    "combinations = list(itertools.combinations(files, 2))\n",
    "combinations = [\n",
    "    t for t in combinations if not (\"landsat-c2\" in t[0] and \"landsat-c2\" in t[1])\n",
    "]\n",
    "\n",
    "kwarg_list = [\n",
    "    {\n",
    "        \"f1\": f1,\n",
    "        \"f2\": f2,\n",
    "        \"he\": False,\n",
    "        \"split\": split,\n",
    "        \"random_state\": random_state,\n",
    "        \"include_climate\": False,\n",
    "        \"variable_groups\": None,\n",
    "        \"n_splits\": 5,\n",
    "    }\n",
    "    for f1, f2 in combinations\n",
    "    for split, random_state in enumerate(random_seeds)\n",
    "]\n",
    "\n",
    "chunked_kwarg_list = list(chunks(kwarg_list, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0843bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunked_kwarg_list[9:10]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_kwarg_list[12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a054b69-93f3-4442-8013-266b4dba2063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_pattern = str(here(\"data\", \"results\", \"2_sensor_results_*_*.csv\"))\n",
    "files = glob.glob(pathname=file_pattern)\n",
    "results = merge_files(files)\n",
    "sorted(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85213ee8-c687-412c-b373-52acd6262357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# top = results.test_R2.sort_values().index[-1]\n",
    "# results.iloc[top:top+1, 1:20]\n",
    "\n",
    "top = results.val_R2.sort_values().index[-1]\n",
    "results.iloc[top : top + 1, 10:]\n",
    "# results.iloc[top : top + 1, 1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95303d9-ab3d-433c-aff7-def47d525597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1 = \"landsat-c2-l2_bands-r-g-b-nir-swir16-swir22_ZMB_20k-points_1024-features_yr-2009-2021_mn-1-12_lm-False_cm-True_wa-False_summary.feather\"\n",
    "f2 = \"sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3bd68e-7e0d-443c-8f8d-4a5579384ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "# Define the number of stratified random splits to perform\n",
    "n_splits = 100  # Generate n random seeds\n",
    "random_seeds = [random.randint(0, 1_000_000) for _ in range(n_splits)]\n",
    "\n",
    "paramlist = [\n",
    "    (\n",
    "        f1,\n",
    "        f2,\n",
    "        \"True\",\n",
    "        split,\n",
    "        random_state,\n",
    "    )\n",
    "    for split, random_state in enumerate(random_seeds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd8b55b-03b9-4140-b02b-91a05345aaf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## TESTING  \n",
    "f1 = \"landsat-c2-l2_bands-r-g-b-nir-swir16-swir22_ZMB_20k-points_1024-features_yr-2009-2021_mn-4-9_lm-True_cm-True_wa-False_summary.feather\"\n",
    "f2 = \"sentinel-2-l2a_bands-2-3-4-8_ZMB_15k-points_1000-features_yr-2016-2022_mn-1-12_lm-False_cm-True_wa-False_summary.feather\"\n",
    "he = True\n",
    "anomaly=True\n",
    "split=0\n",
    "random_state=670487\n",
    "n_splits = 5\n",
    "include_climate = False\n",
    "# variable_groups = ['tmp', 'ndvi']\n",
    "# variable_groups = ['ndvi']\n",
    "variable_groups = None \n",
    "return_oos_predictions = True\n",
    "  \n",
    "  \n",
    "  \n",
    "#########################################     SET PARAMS    #########################################    \n",
    "satellite1, bands1, country_code, points1, yrs1, mns1,\\\n",
    "num_features1, limit_months1, crop_mask1, weighted_avg1 = split_fn(f1)\n",
    "\n",
    "satellite2, bands2, country_code, points2, yrs2, mns2,\\\n",
    "num_features2, limit_months2, crop_mask2, weighted_avg2 = split_fn(f2)\n",
    "\n",
    "if variable_groups is None:\n",
    "    variable_groups_str = \"rcf\"\n",
    "else:\n",
    "    variable_groups_str = \"_\".join(variable_groups)\n",
    "\n",
    "print(f\"\"\"\n",
    "Begin with paramters:\n",
    "    F1: {f1}\n",
    "    F2: {f2}\n",
    "    One-hot encoding: {he}\n",
    "    Anomaly: {anomaly}\n",
    "    Split: {split}\n",
    "    Random state: {random_state}\n",
    "    N-splits: {n_splits}\n",
    "    Include climate: {include_climate}\n",
    "    Climate vars: {variable_groups_str}\n",
    "\"\"\", flush=True)\n",
    "\n",
    "#########################################     READ DATA    #########################################\n",
    "features_1 = pd.read_feather(here('data', 'random_features', 'summary', f1))\n",
    "features_2 = pd.read_feather(here('data', 'random_features', 'summary', f2))\n",
    "if include_climate:\n",
    "    climate_df = pd.read_csv(here('data', 'climate', 'climate_summary.csv'))\n",
    "\n",
    "#########################################     CLEAN DATA    #########################################  \n",
    "min_year = max(min(features_1.year), min(features_2.year))\n",
    "max_year = min(max(features_1.year), max(features_2.year))\n",
    "\n",
    "features_1 = features_1[features_1.year >= min_year]\n",
    "features_2 = features_2[features_2.year >= min_year]\n",
    "\n",
    "features_1 = features_1[features_1.year <= max_year]\n",
    "features_2 = features_2[features_2.year <= max_year]\n",
    "\n",
    "features_1.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "features_2.drop(['crop_perc'], axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "#########################################     JOIN FEATURES    #########################################  \n",
    "drop_cols = ['district', 'year', 'yield_mt']\n",
    "\n",
    "features_1 = features_1.set_index(drop_cols).add_prefix(\"f1_\")\n",
    "features_2 = features_2.set_index(drop_cols).add_prefix(\"f2_\")\n",
    "\n",
    "features = features_1.join(features_2).reset_index()\n",
    "features = features[~features.isna().any(axis = 1)]\n",
    "\n",
    "features['log_yield'] = np.log10(features['yield_mt'] + 1)\n",
    "\n",
    "#########################################    JOIN CLIMATE VARS    #########################################\n",
    "if include_climate:\n",
    "    keep_cols = []\n",
    "\n",
    "    for var in variable_groups:\n",
    "        tmp = climate_df.columns[climate_df.columns.to_series().str.contains(var)].tolist()\n",
    "        keep_cols.append(tmp)\n",
    "\n",
    "    keep_cols = [*drop_cols, *[col for cols in keep_cols for col in cols]]\n",
    "\n",
    "    climate_df = climate_df.loc[:, keep_cols]\n",
    "\n",
    "    features = (\n",
    "        features.set_index(drop_cols).join(climate_df.set_index(drop_cols)).reset_index()\n",
    "    )\n",
    "    features = features[features.year <= max(climate_df.year)]\n",
    "\n",
    "drop_cols.append('log_yield')\n",
    "\n",
    "#########################################     CALCULATE ANOMALY   #########################################\n",
    "if anomaly:\n",
    "    features.set_index(['year', 'district'], inplace=True)\n",
    "    var_cols = features.columns\n",
    "    features = features[var_cols] - features.groupby(['district'], as_index=True)[var_cols].transform('mean')\n",
    "    features.reset_index(drop=False, inplace=True)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################     CLEAN AND COPY    #########################################\n",
    "yrs = f\"{min(features.year)}-{max(features.year)}\"\n",
    "n_fts_1 = features_1.shape[1]\n",
    "n_fts_2 = features_2.shape[1]\n",
    "n_districts = len(features.district.unique())\n",
    "\n",
    "if include_climate:\n",
    "    n_climate_cols = climate_df.shape[1] - len(drop_cols)\n",
    "\n",
    "    i = 0\n",
    "    n_climate_groups = []\n",
    "    for cols in range(n_climate_cols):\n",
    "        if cols % 12 == 0:\n",
    "            i += 1\n",
    "            n_climate_groups.append(i)\n",
    "    n_climate_groups\n",
    "\n",
    "crop_yield = features.copy().loc[:, tuple(drop_cols)]\n",
    "\n",
    "del features_1, features_2\n",
    "gc.collect()\n",
    "\n",
    "#########################################    HOT ENCODE    #########################################\n",
    "if he:\n",
    "    drop_cols.remove(\"district\")\n",
    "    features = pd.get_dummies(\n",
    "        features, columns=[\"district\"], drop_first=False, dtype=float\n",
    "    )\n",
    "else:\n",
    "    pass\n",
    "\n",
    "#########################################     TRAIN/TEST SPLIT    #########################################\n",
    "x_all = features.drop(drop_cols, axis=1)\n",
    "y_all = features.log_yield\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_all, y_all, test_size=0.2, random_state=random_state\n",
    ")\n",
    "\n",
    "del features\n",
    "gc.collect()\n",
    "\n",
    "#########################################    STANDARDIZE FEATURES    #########################################\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = pd.DataFrame(scaler.transform(x_train), columns=x_train.columns, index=x_train.index)\n",
    "x_test = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns, index=x_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce48e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#########################################     K-FOLD CV    ###########################################\n",
    "### SETUP\n",
    "tic = time.time()\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "alphas = {\"alpha\": np.logspace(-1, 1, base=10, num=3)}\n",
    "\n",
    "### LAMBDA INDICIES\n",
    "start = [0, n_fts_1]\n",
    "end = [n_fts_1, x_train.shape[1]]\n",
    "\n",
    "if include_climate:\n",
    "    start.append(n_fts_1 + n_fts_2)  \n",
    "    end.append(n_fts_1 + n_fts_2)  \n",
    "\n",
    "    for n in n_climate_groups:\n",
    "        x = n * 12\n",
    "        y = n_fts_1 + n_fts_2 + x\n",
    "        start.append(y)\n",
    "        end.append(y)\n",
    "\n",
    "if not include_climate and he:\n",
    "    start.append(x_train.shape[1] - n_districts)\n",
    "    end.append(x_train.shape[1] - n_districts)\n",
    "\n",
    "end.sort()\n",
    "\n",
    "print(f'Group indicies {start}\\n\\t\\t  {end}', end='\\n\\n')\n",
    "\n",
    "### GRID SEARCH - FINDING BEST REGULARIZATION PARAMETER(S)\n",
    "best_lambdas, best_scores, best_model = kfold_rr_multi_lambda_tuning(\n",
    "    X=x_train,\n",
    "    y=y_train, \n",
    "    grid=alphas.get('alpha'), \n",
    "    n_splits=n_splits,\n",
    "    start=start,\n",
    "    end=end, \n",
    "    static_lam=1,\n",
    "    verbose=2,\n",
    "    show_linalg_warning=False,\n",
    "    fit_model_after_tuning=True\n",
    ")\n",
    "### PREDICT WITH BEST HYPERPARAMETER(S)\n",
    "val_predictions   = cross_val_predict(best_model, X=x_train, y=y_train, cv=kfold)   \n",
    "train_predictions = best_model.predict(x_train)\n",
    "test_predictions  = best_model.predict(x_test)\n",
    "\n",
    "if anomaly:\n",
    "    pass\n",
    "else:\n",
    "    val_predictions   = np.maximum(val_predictions, 0)\n",
    "    train_predictions = np.maximum(train_predictions, 0)\n",
    "    test_predictions  = np.maximum(test_predictions, 0)\n",
    "    \n",
    "print(f\"\"\"\n",
    "Finish:\n",
    "    F1: {f1}\n",
    "    F2: {f2}\n",
    "    One-hot encoding: {he}\n",
    "    Anomaly: {anomaly}\n",
    "    Split: {split}\n",
    "    Random state: {random_state}\n",
    "    N-splits: {n_splits}\n",
    "    Include climate: {include_climate}\n",
    "    Climate vars: {variable_groups_str}\n",
    "    Final Val R2:  {r2_score(y_train, val_predictions):0.4f} \n",
    "    Final Test R2: {r2_score(y_test, test_predictions):0.4f}\n",
    "    Total time: {(time.time()-tic)/60:0.2f} minutes\n",
    "\"\"\", flush=True)\n",
    "\n",
    "#########################################     DE-MEAN TRAIN R2    #########################################\n",
    "fold_list = []\n",
    "for i in range(n_splits):\n",
    "    idx = len(list(kfold.split(y_train))[i][1])\n",
    "    fold = np.repeat(i + 1, idx).tolist()\n",
    "    fold_list.append(fold)\n",
    "fold_list = [item for sublist in fold_list for item in sublist]\n",
    "\n",
    "train_split = pd.DataFrame(\n",
    "    np.repeat(\"train\", len(x_train)), columns=[\"data_fold\"], index=x_train.index\n",
    ")\n",
    "train_split = train_split.join(\n",
    "    crop_yield.copy()[crop_yield.index.isin(x_train.index)]\n",
    ")\n",
    "train_split[\"oos_prediction\"] = val_predictions\n",
    "train_split[\"val_fold\"] = fold_list\n",
    "train_split = demean_by_group(train_split, predicted=\"oos_prediction\", group=[\"district\"])\n",
    "\n",
    "#########################################     DE-MEAN TEST R2    #########################################\n",
    "test_split = pd.DataFrame({\"data_fold\": np.repeat(\"test\", len(x_test))}, index=x_test.index)\n",
    "test_split = test_split.join(crop_yield.copy()[crop_yield.index.isin(x_test.index)])\n",
    "test_split[\"oos_prediction\"] = test_predictions\n",
    "test_split[\"val_fold\"] = n_splits + 1\n",
    "test_split = demean_by_group(test_split, predicted=\"oos_prediction\", group=[\"district\"])\n",
    "\n",
    "#########################################     OUT OF SAMPLE PREDICTIONS    #########################################\n",
    "oos_preds = pd.concat([train_split, test_split])\n",
    "oos_preds[[\"split\", \"random_state\"]] = split, random_state\n",
    "oos_preds[\"variables\"] = variable_groups_str\n",
    "oos_preds[\"anomaly\"] = anomaly\n",
    "\n",
    "#########################################     SCORES    #########################################\n",
    "val_R2 = r2_score(y_train, val_predictions)\n",
    "val_r = pearsonr(val_predictions, y_train)[0]\n",
    "train_R2 = r2_score(y_train, train_predictions)\n",
    "train_r = pearsonr(train_predictions, y_train)[0]\n",
    "test_R2 = r2_score(y_test, test_predictions)\n",
    "test_r = pearsonr(test_predictions, y_test)[0]\n",
    "\n",
    "if anomaly:\n",
    "    demean_cv_R2   = np.nan\n",
    "    demean_cv_r    = np.nan\n",
    "    demean_test_R2 = np.nan\n",
    "    demean_test_r  = np.nan\n",
    "else:\n",
    "    demean_cv_R2 = r2_score(train_split.demean_log_yield, train_split.demean_oos_prediction)\n",
    "    demean_cv_r  = pearsonr(train_split.demean_log_yield, train_split.demean_oos_prediction)[0]\n",
    "    demean_test_R2 = r2_score(test_split.demean_log_yield, test_split.demean_oos_prediction)\n",
    "    demean_test_r  = pearsonr(test_split.demean_log_yield, test_split.demean_oos_prediction)[0]\n",
    "\n",
    "#########################################     SAVE RESULTS    #########################################\n",
    "d = {\n",
    "    \"split\": split,\n",
    "    \"random_state\": random_state,\n",
    "    \"variables\": variable_groups_str,\n",
    "    \"anomaly\": anomaly,\n",
    "    \"country\": country_code[0],\n",
    "    \"year_range\": yrs,\n",
    "    \"satellite_1\": satellite1[0],\n",
    "    \"bands_1\": bands1,\n",
    "    \"num_features_1\": num_features1,\n",
    "    \"points_1\": points1,\n",
    "    \"month_range_1\": mns1,\n",
    "    \"limit_months_1\": limit_months1,\n",
    "    \"crop_mask_1\": crop_mask1,\n",
    "    \"weighted_avg_1\": weighted_avg1,\n",
    "    \"satellite_2\": satellite2[0],\n",
    "    \"bands_2\": bands2,\n",
    "    \"num_features_2\": num_features2,\n",
    "    \"points_2\": points2,\n",
    "    \"month_range_2\": mns2,\n",
    "    \"limit_months_2\": limit_months2,\n",
    "    \"crop_mask_2\": crop_mask2,\n",
    "    \"weighted_avg_2\": weighted_avg2,\n",
    "    \"hot_encode\": he,\n",
    "    \"total_n\": len(x_all),\n",
    "    \"train_n\": len(x_train),\n",
    "    \"test_n\": len(x_test),\n",
    "    \"best_reg_param\": [best_lambdas],\n",
    "    \"mean_of_val_R2\": [best_scores],\n",
    "    \"val_R2\": val_R2,\n",
    "    \"val_r\": val_r,\n",
    "    \"val_r2\": val_r ** 2,\n",
    "    \"train_R2\": train_R2,\n",
    "    \"train_r\": train_r,\n",
    "    \"train_r2\": train_r ** 2,\n",
    "    \"test_R2\": test_R2,\n",
    "    \"test_r\": test_r,\n",
    "    \"test_r2\": test_r ** 2,\n",
    "    \"demean_cv_R2\": demean_cv_R2,\n",
    "    \"demean_cv_r\": demean_cv_r,\n",
    "    \"demean_cv_r2\": demean_cv_r ** 2,\n",
    "    \"demean_test_R2\": demean_test_R2,\n",
    "    \"demean_test_r\": demean_test_r,\n",
    "    \"demean_test_r2\": demean_test_r ** 2,\n",
    "}\n",
    "# if return_oos_predictions:\n",
    "#     return d, oos_preds\n",
    "# else:\n",
    "#     return d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "fec78c4aa066b70eb4890a66d67729df88bd6fefdc0eec39af542817ac9abede"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
