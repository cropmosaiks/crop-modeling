---
title: CropMOSAIKS Model Results
author: 
  - name: Cullen Molitor
    orcid: 0000-0001-8309-8821
    url: https://github.com/cullen-molitor
    affiliations:
      - ref: bren
  - name: Grace Lewin
    orcid: 0000-0001-7106-0201
    url: https://github.com/gracelewin
    affiliations:
      - ref: eemb
  - name: Juliet Cohen
    orcid: 0000-0001-8217-4028
    url: https://github.com/julietcohen
    affiliations:
      - ref: nceas
  - name: Steven Cognac
    orcid: 0000-0001-8803-3074
    url: https://github.com/cognack
    affiliations:
      - ref: bren
  - name: Jonathan Proctor
    orcid: 0000-0001-8053-8828
    url: https://www.jonathanproctor.org/
    affiliations:
      - ref: harvard
  - name: Tamma Carleton
    orcid: 0000-0002-5518-0550
    url: https://www.tammacarleton.com/
    affiliations:
      - ref: bren
affiliations:
  - id: bren
    name: Bren School of Environmental Science & Management, UC Santa Barbara
    city: Santa Barbara
    state: CA
  - id: eemb
    name: Ecology, Evolution, and Marine Biology, UC Santa Barbara
    city: Santa Barbara
    state: CA
  - id: nceas
    name: National Center for Ecological Analysis and Synthesis
    city: Santa Barbara
    state: CA
  - id: harvard
    name: Center for the Environment and Data Science Initiative, Harvard University
    city: Cambridge
    state: MA
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    toc: true
    toc-title: Contents
    toc-depth: 4
    number-sections: false
fig-cap-location: bottom
---

# Setup

```{r message=F, warning=F}
####################### R ENVIRONMENT #######################
knitr::opts_chunk$set(message = FALSE, warning = FALSE)

source(here::here('code', '4_explore_results', 'utility.R'))
```

# Summary

This script produces graphics to explore the results of the statistics gathered as a result of iteratively modeling crop yield on random convolutional features created from satellite imagery. Each model iteration represents a combination of unique inputs, described in detail below.   

## TODO: 

1. Fill in results with explanatory text

# Data

**Zambian crop yield**: Maize crop forecast survey (CFS) data summarized to the district level for 2009 to 2022. The Zambian Maize growing season spans the calendar year with planting typically done in November and harvest in June through August of the following year. A crop year of 2009 therefore spans 2008-2009. Crop forecast surveys are conducted in May of the harvest year to aid in national level post harvest planning. Data is provided by the Baylis lab of University of California, Santa Barbara and University of Chicago. [Link](https://baylislab.ace.illinois.edu/).

**sentinel-2-l2a (S2)**: Imagery from the Sentinel constellation of satellites including Sentinel-2A and Sentinel-2B. [Link](https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a).

**landsat-8-c2-l2 (LS8)**: Imagery from Landsat 8. [Link](https://www.usgs.gov/landsat-missions/landsat-8).

**landsat-c2-l2 (LSC)**: Imagery from the Landsat constellation of satellites including Landsat 5, Landsat 7, and Landsat 8 (if we go into 2022, then also Landsat 9). [Link](https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2).

Landsat 7 suffered a [scan line correction failure](https://www.usgs.gov/landsat-missions/landsat-7?qt-science_support_page_related_con=0#qt-science_support_page_related_con) in 2003. All imagery thereafter contains striping with missing data, which is typically worse at the eastern and western edges of the image. There is a center strip from north to south that is unaffected. We use patches of a full  satellite scene, and patches with striping are unsuitable for convolution directly. Therefore, all Landsat 7 imagery used has been corrected with simple imputation through a nearest neighbor approach to approximate the missing pixel values. We keep track of the amount of percent of missing pixels. 

The nearest neighbor is a simple imputation method and was selected due to Landsat pass-over rate of 16 days. This means many months there is only a single image over a given point. We use monthly imagery rather than aggregate across months, thus nearest neighbor is the simplest method of filling in missing pixel values. 

**Time range**:

-   Crop yield - 2009 to 2022
-   S2 - June 2015 to Current
-   LS8 - April 2013 to Current
-   LSC - October 2008 to Current (Begins in 1982, limited to useful range)

**Resolution**:  

-   Crop yield - Administrative boundary level 2, Zambian districts    
-   S2 - 10 m ground sample distance (GSD)  
-   LS8 - 30 m GSD  
-   LSC - 30 m GSD    

**Data access**: Satellite imagery is all publicly available. All imagery was accessed and processed through the [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/) in this project. Crop forecast survey data is not yet publicly available, but will be made available at the time of publication. 

# Methods

The core modeling of this project is done in other notebooks. A brief description of the modeling process is provided below. The rest of this notebook explores the results of iterating over model inputs used in the 'task modeling' process.

## Random convolution feature modeling

Fill in later with summarized description of RCF.

## Task modeling

All models use ridge regression on data summarized to the district level (both yield and features) and split into a training set (80%) and a test set (20%). Each combinations of model inputs are used to train 2 separate models with different cross validation (CV) strategies. One uses "k-fold" CV, and one the other uses "leave one group out" CV. The two strategies and their implications are described below. Additionally, each model uses a grid search method to identify and select the "best" regularization parameter by iterating the CV process over an array of values, and selecting the value which provides the best performance. In our case we search over an array of 17 values between 0 and 100,000,000:
`1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1,`
`1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06, 1.e+07, 1.e+08`. 

### K-fold CV 

The primary model in each iteration uses 5-fold cross validation. This means the training data is split into five folds, each with 20% of the available data. The model is trained using 4 of the folds as training data, and is validated on the remaining fold. This process is repeated 5 times, using a different fold for validation on each iteration. The performance measures are calculated by taking the predicted and observed values of the validation data in each of the 5 iterations. Our primary metric is the coefficient of determination calculated on the observed and predicted values of the validation data.      

![Figure 1. Diagram representing k-fold cross validation data splitting strategy](../../figures/kfold_cv_split.png)

We chose 5-fold CV for its ability to predict over space as well as over time... more on why later.

## Model inputs

The following variables were either included or excluded to test their affect on model performance.

1) **Sensor platform**: the sensor platform which imagery was taken from. Options are:

    + The Landsat satellite constellation including 5, 7, and 8  
    + Landsat 8 only (this avoids the SLC failure)
    + The Sentinel 2 satellite constellation including 2A and 2B

1) **Spectral bandpass**: the combination of distinct spectral bandpass's collected by the electro-optical sensor and utilized in the convolutional neural network model.

1) **Spatial resolution**: the number of 0.01 by 0.01 degree grid cells we targeted for featurization. These are represented as points in longitude and latitude, where the coordinates are the center of the grid cell. Exact area covered by each grid cell varies with latitude and elevation. 

1) **One hot encoding**: when true, the Zambian districts were used as dummy variables. Each district becomes a column with either inside district (1) or outside district (0). This is equivalent to a spatial fixed effects model. When false, district is dropped altogether. 

1) **Crop mask**: when true, only points with cropland are retained. In some iterations, this becomes redundant as only points with cropland were featurized, rather than including a random selection of points (targeted vs non-targeted approach). When false, all points are retained.   

1) **Weighted average**: when true, a weighted average was used in summarizing the features to the district level. Weights are the calculated proportion of cropland over the area where the features were calculated. When false, a simple mean is used in summarizing the features to the district level.  

1) **Limiting month range**: whether the months were limited to the dry season (from April to October, or using the full year range. Limiting month ranges to the dry season results in more high quality imagery (i.e., low cloud cover). 

1) **Available year range**: features are limited in overall time range by the launch date of the individual sensor platforms. 

# Results 

Here we break the results into two sections based on the statistic we are interested in evaluating. We collect the statistics from running each model with varied inputs described in the methods section. The distribution of these statistics are used to visualize the outcome of manipulating those model inputs. There are `r length(one_sensor_results$kfold_val_R2)` unique outcomes from the combinations of these model inputs from 1 sensor and `r length(two_sensor_results$kfold_val_R2)` from combining 2 sensors. 

## Validation score (R<sup>2</sup>)

The first statistic we evaluate is the coefficient of determination (R<sup>2</sup>) calculated from the predicted values and the observed values from each of the 5 validation sets. Here we see the distribution of all validation scores. 

**Note**: The scales are forced to be the same between K-Fold and LOGO. The idea was to evaluate the relative difference in performance, but it makes some distributions harder to interpret. Would it help to make them independent?

```{r echo = FALSE}
min_lim <- min(
  min(one_sensor_results$kfold_val_R2), 
  min(one_sensor_results$logo_val_R2)
)
max_lim <- max(
  max(one_sensor_results$kfold_val_R2), 
  max(one_sensor_results$logo_val_R2)
)
limits <- c(min_lim, max_lim)

min_lim_two <- min(
  min(two_sensor_results$kfold_val_R2), 
  min(two_sensor_results$logo_val_R2)
)
max_lim_two <- max(
  max(two_sensor_results$kfold_val_R2), 
  max(two_sensor_results$logo_val_R2)
)
limits_two <- c(min_lim_two, max_lim_two)
```

### Overall

::: {.panel-tabset}

#### One sensor

```{r}
hist_plot(one_sensor_results, kfold_val_R2)
```

#### Two sensor

```{r}
hist_plot(two_sensor_results, kfold_val_R2)
```

:::

### Hot encoding

::: {.panel-tabset}

#### One sensor

```{r}
dist_plot(
  data = one_sensor_results, 
  x = hot_encode, 
  y = kfold_val_R2,
  clr = month_range,
  y_lims = limits
)
```

#### Two sensor

```{r fig.height=8, fig.width=8}
dist_plot(
  data = two_sensor_results, 
  x = hot_encode, 
  y = kfold_val_R2,
  clr = month_range,
  y_lims = limits_two
)
```

:::

### Crop mask

::: {.panel-tabset}

#### One sensor

```{r}
dist_plot(
  data = one_sensor_results, 
  x = crop_mask, 
  y = kfold_val_R2,
  clr = month_range,
  y_lims = limits
)
```

#### Two sensor

```{r fig.height=8, fig.width=8}
dist_plot(
  data = two_sensor_results, 
  x = crop_mask, 
  y = kfold_val_R2,
  clr = month_range,
  y_lims = limits_two
)
```

:::

### Weighted average

::: {.panel-tabset}

#### One sensor

```{r }
dist_plot(
  data = one_sensor_results, 
  x = weighted_avg, 
  y = kfold_val_R2,
  clr = month_range,
  y_lims = limits
)
```

#### Two sensor

```{r fig.height=8, fig.width=8}
dist_plot(
  data = two_sensor_results, 
  x = weighted_avg, 
  y = kfold_val_R2,
  clr = month_range,
  y_lims = limits_two
)
```

:::

## De-meaned score (R<sup>2</sup>)

To get an idea of the each models ability to predict crop yield over time, we remove spatial variability by subtracting the district means from both observed and predicted yields. These de-meaned values are then evaluated by calculating the coefficient of determination. A high score indicates the model is able to predict well over time, while a low score indicates it performed this task poorly. Here we see the distribution of all de-meaned scores.

```{r}
min_lim <- min(
  min(one_sensor_results$kfold_demean_R2), 
  min(one_sensor_results$logo_demean_R2)
)
max_lim <- max(
  max(one_sensor_results$kfold_demean_R2), 
  max(one_sensor_results$logo_demean_R2)
)
limits <- c(min_lim, max_lim + 1)

min_lim_two <- min(
  min(two_sensor_results$kfold_demean_R2), 
  min(two_sensor_results$logo_demean_R2)
)
max_lim_two <- max(
  max(two_sensor_results$kfold_demean_R2), 
  max(two_sensor_results$logo_demean_R2)
)
limits_two <- c(min_lim_two, max_lim_two)
```

### Overall

::: {.panel-tabset}

#### One sensor

```{r}
hist_plot(one_sensor_results, kfold_demean_R2)
```

#### Two sensor

```{r}
hist_plot(two_sensor_results, kfold_demean_R2)
```

:::

### Hot encoding

::: {.panel-tabset}

#### One sensor

```{r}
dist_plot(
  data = one_sensor_results, 
  x = hot_encode, 
  y = kfold_demean_R2,
  clr = month_range,
  y_lims = limits
)
```

#### Two sensor

```{r fig.height=8, fig.width=8}
dist_plot(
  data = two_sensor_results, 
  x = hot_encode, 
  y = kfold_demean_R2,
  clr = month_range,
  y_lims = limits_two
)
```

:::

### Crop mask

::: {.panel-tabset}

#### One sensor

```{r}
dist_plot(
  data = one_sensor_results, 
  x = crop_mask, 
  y = kfold_demean_R2,
  clr = month_range,
  y_lims = limits
)
```

#### Two sensor

```{r fig.height=8, fig.width=8}
dist_plot(
  data = two_sensor_results, 
  x = crop_mask, 
  y = kfold_demean_R2,
  clr = month_range,
  y_lims = limits_two
)
```

:::

### Weighted average

::: {.panel-tabset}

#### One sensor

```{r}
dist_plot(
  data = one_sensor_results, 
  x = weighted_avg, 
  y = kfold_demean_R2,
  clr = month_range,
  y_lims = limits
)
```

#### Two sensor

```{r fig.height=8, fig.width=8}
dist_plot(
  data = two_sensor_results, 
  x = weighted_avg, 
  y = kfold_demean_R2,
  clr = month_range,
  y_lims = limits_two
)
```

:::

## Anomaly validation score (R<sup>2</sup>)

Finally, we evaluate the coefficient of determination (R<sup>2</sup>) calculated from the predicted values and the observed values from each of the 5 validation sets of the models trained on feature anomalies. Here we see the distribution of all validation scores.

**Note**: The scales are forced to be the same between K-Fold and LOGO. The idea was to evaluate the relative difference in performance, but it makes some distributions harder to interpret. Would it help to make them independent?

```{r echo = FALSE}
min_lim <- min(
  min(one_anomaly_results$kfold_val_R2), 
  min(one_anomaly_results$logo_val_R2)
)
max_lim <- max(
  max(one_anomaly_results$kfold_val_R2), 
  max(one_anomaly_results$logo_val_R2)
)
limits <- c(min_lim, max_lim)
```

### Overall

::: {.panel-tabset}

#### One sensor

```{r}
hist_plot(one_anomaly_results, kfold_val_R2 )
```

#### Two sensor

```{r}
hist_plot(two_anomaly_results, kfold_val_R2)
```

:::

### Crop mask

::: {.panel-tabset}

#### One sensor

```{r}
dist_plot(
  data = one_anomaly_results, 
  x = crop_mask, 
  y = kfold_val_R2,
  clr = month_range,
  y_lims = limits
)
```

#### Two sensor

```{r fig.height=8, fig.width=8}
dist_plot(
  data = two_anomaly_results, 
  x = crop_mask, 
  y = kfold_val_R2,
  clr = month_range,
  y_lims = limits
)
```

:::

### Weighted average

::: {.panel-tabset}

#### One sensor

```{r}
dist_plot(
  data = one_anomaly_results, 
  x = weighted_avg, 
  y = kfold_val_R2,
  clr = month_range,
  y_lims = limits
)
```

#### Two sensor

```{r fig.height=8, fig.width=8}
dist_plot(
  data = two_anomaly_results, 
  x = weighted_avg, 
  y = kfold_val_R2,
  clr = month_range,
  y_lims = limits
)
```

:::

## Top models

### Best general model

The top performing model used ridge regression with 5-fold cross validation, repeated for 17 regularization parameters, with the top performing regularization parameter used for model fitting. 

Top performing model:

1) **Sensor platform**: 

    + Landsat 8 (LS8) 
    + Sentinel 2 (S2)  

1) **Spectral bandpass**:  

    + LS8 bands: AOT, B, G, R, NIR, SWIR16, SWIR22  
    + S2 bands: B, G, R, NIR 

1) **Spatial resolution**: 

    + LS8: 15,000 0.01 by 0.01 degree grid cells, sampled form an evenly spaced grid.   
    + S2: 15,000 0.01 by 0.01 degree grid cells, sampled form an evenly spaced grid.    

1) **Number of features**: 18,000 total (1,000 for each satellite, for each month).  

    + LS8: 6,000 (April to September)
    + S2: 12,000 (January to December)   

1) **Limiting month range**: 

    + LS8: true, the months were limited to the dry season (April to September)  
    + S2: false, all months were used (January to December).     
    
1) **Crop mask**:  

    + LS8: false, all points are retained. 
    + S2: true, only points with cropland are retained.  
    
1) **Weighted average**: 

    + LS8: false, a simple average was used in summarizing the features to the district level.  
    + S2: false, a simple average was used in summarizing the features to the district level.

1) **One hot encoding**: true, the Zambian districts were used as dummy variables. 

1) **Available year range**: 2016 to 2021, the overlap of available satellite data and crop yield data. 

**Notes**: 

1) I cap the predictions at roughly the level of the yield levels. This is largely for plotting purpose, need to discuss in next meeting!

```{r}
train_pred <- filter(summary_predictions, split == "train")
test_pred  <- filter(summary_predictions, split == "test")

train_n <- length(train_pred$prediction)
test_n  <- length(test_pred$prediction)

val_R2   <- r2_general(train_pred$log_yield, train_pred$kfold_cv_predictions)
train_R2 <- r2_general(train_pred$log_yield, train_pred$prediction)
test_R2  <- r2_general(test_pred$log_yield, test_pred$prediction)

val_label   <- plot_label(val_R2,   train_n)
train_label <- plot_label(train_R2, train_n)
test_label  <- plot_label(test_R2,  test_n)
```

#### Observed vs predicted 

::: {.panel-tabset}

##### Validation

```{r warning = FALSE, fig.width=5, fig.height=5}
pred_plot(train_pred, x = log_yield, y = kfold_cv_predictions, val_label)
```

##### Train

```{r warning = FALSE, fig.width=5, fig.height=5}
pred_plot(train_pred, x = log_yield, prediction, train_label)
```

##### Test

```{r warning = FALSE, fig.width=5, fig.height=5}
pred_plot(test_pred, x = log_yield, prediction, test_label)
```

:::

#### Crop Yield

::: {.panel-tabset}

##### Maps

::: {.panel-tabset}

###### Crop Land

```{r fig.height=10, fig.width=10, warning=FALSE, message=FALSE}
ggplot() +
  geom_sf(data = country_shp, color = 'black', fill = "#414487FF") +
  tidyterra::geom_spatraster(data = crop_land) +
  scale_fill_gradientn(
    colours = '#5ec962', na.value = NA, labels = NULL,
    guide = guide_colorbar(barheight = unit(.5, 'cm'), ticks = F)) +
  scale_x_continuous(expand = expansion(c(0.01,0.01))) +
  scale_y_continuous(expand = expansion(c(0.01,0.01))) +
  labs(fill = "Cropland")
```


###### Log yield

```{r fig.height=10, fig.width=10}
ggplot() +
  geom_sf(data = yield_sf, aes(fill = log_yield), color = 'black') +
  facet_wrap(~year) +
  scale_fill_viridis_c()
```

###### Predicted yield

```{r fig.height=10, fig.width=10}
ggplot() +
  geom_sf(data = yield_sf, aes(fill = prediction), color = 'black') +
  facet_wrap(~year) +
  scale_fill_viridis_c()
```

###### Residual

```{r fig.height=10, fig.width=10}
ggplot() +
  geom_sf(data = yield_sf, aes(fill = residuals), color = 'black') +
  facet_wrap(~year) +
  scale_fill_distiller(palette = "BrBG")
  # scico::scale_fill_scico(palette = 'vik')
```

$predicted - observed$ so positive values indicates over-prediction, and negative values indicates under-prediction.

###### High res. predictions

```{r fig.height=10, fig.width=10}
ggplot() +
  geom_point(data = high_res_predictions, size = .1,
             aes(x = lon, y = lat, color = prediction)) +
  geom_sf(data = country_shp, fill = NA, color = 'black') +
  facet_wrap(~year) +
  scale_color_viridis_c()
```

:::

##### Histograms

::: {.panel-tabset}

###### Log yield

```{r }
ggplot(data = yield_sf, aes(x = log_yield)) +
  geom_histogram(bins = 30, fill = 'dodgerblue', color = 'black') 
```

###### Predicted yield

```{r }
ggplot(data = yield_sf, aes(x = prediction)) +
  geom_histogram(bins = 30, fill = 'dodgerblue', color = 'black') 
```

###### Residual

```{r }
ggplot(data = yield_sf, aes(x = residuals)) +
  geom_histogram(bins = 30, fill = 'dodgerblue', color = 'black') 
```

$predicted - observed$ so positive values indicates over-prediction, and negative values indicates under-prediction.

:::

:::

### Best over-time model

The top performing model wen evaluating the ability to predict over time as selected from the   

Top performing model:

1) **Sensor platform**: 

    + Landsat Collection (LSC) 
    + Sentinel 2 (S2)  

1) **Spectral bandpass**:  

    + LSC bands: B, G, R, NIR, SWIR16, SWIR22  
    + S2 bands: B, G, R 

1) **Spatial resolution**: 

    + LSC: 20,000 0.01 by 0.01 degree grid cells, targeted cropland sampling.   
    + S2: 4,000 0.01 by 0.01 degree grid cells, sampled form an evenly spaced grid.    

1) **Number of features**: 18,000 total (1,000 for each satellite, for each month).  

    + LSC: 6,000 (April to September)
    + S2: 12,000 (January to December)   

1) **Limiting month range**: 

    + LSC: true, the months were limited to the dry season (April to September)  
    + S2: false, all months were used (January to December).     
    
1) **Crop mask**:  

    + LSC: true, by default from targeted cropland sampling approach. 
    + S2: true, only points with cropland are retained.  
    
1) **Weighted average**: 

    + LSC: false, a simple average was used in summarizing the features to the district level.  
    + S2: false, a simple average was used in summarizing the features to the district level.
    
1) **One hot encoding**: NA, the  features and yield values are demeaned prior to modeling. 

1) **Available year range**: 2016 to 2021, the overlap of available satellite data and crop yield data.

#### Observed vs predicted 

::: {.panel-tabset}

##### Validation

```{r warning = FALSE, fig.width=5, fig.height=5}
train_pred <- filter(best_anom_train_pred, split == "train")
val_n      <- length(best_anom_train_pred$kfold_cv_predictions)
val_R2     <- r2_general(best_anom_train_pred$yield_anom,
                        best_anom_train_pred$kfold_cv_predictions)
val_label  <- plot_label(val_R2, val_n)

min_x = min(best_anom_train_pred$yield_anom)
max_x = max(best_anom_train_pred$yield_anom)

min_y = min(best_anom_train_pred$kfold_cv_predictions)
max_y = max(best_anom_train_pred$kfold_cv_predictions)

pred_plot(
  .data = best_anom_train_pred, 
  x = yield_anom, 
  y = kfold_cv_predictions, 
  label = val_label,
  y_lims = c(min_y, max_y), 
  x_lims = c(min_x, max_x), 
  label_pos = c(min_x+.05, max_y-.05)
)
```

##### Train

```{r warning = FALSE, fig.width=5, fig.height=5}
train_n      <- length(best_anom_train_pred$prediction)
train_R2     <- r2_general(best_anom_train_pred$yield_anom,
                        best_anom_train_pred$prediction)
train_label  <- plot_label(train_R2, train_n)

min_x = min(best_anom_train_pred$yield_anom)
max_x = max(best_anom_train_pred$yield_anom)

min_y = min(best_anom_train_pred$prediction)
max_y = max(best_anom_train_pred$prediction)

pred_plot(
  .data = best_anom_train_pred, 
  x = yield_anom, 
  y = prediction, 
  label = train_label,
  y_lims = c(min_y, max_y), 
  x_lims = c(min_x, max_x), 
  label_pos = c(min_x+.05, max_y-.05)
)
```

##### Test

```{r warning = FALSE, fig.width=5, fig.height=5}
test_pred <- filter(best_anom_test_pred, split == "test")
test_n      <- length(best_anom_test_pred$prediction)
test_R2     <- r2_general(best_anom_test_pred$yield_anom,
                        best_anom_test_pred$prediction)
test_label  <- plot_label(test_R2, test_n)

# min_x = min(best_anom_test_pred$yield_anom)
# max_x = max(best_anom_test_pred$yield_anom)
# 
# min_y = min(best_anom_test_pred$prediction)
# max_y = max(best_anom_test_pred$prediction)

pred_plot(
  .data = best_anom_test_pred, 
  x = yield_anom, 
  y = prediction, 
  label = test_label,
  y_lims = c(min_y, max_y), 
  x_lims = c(min_x, max_x), 
  label_pos = c(min_x+.05, max_y-.05)
)
```

:::
