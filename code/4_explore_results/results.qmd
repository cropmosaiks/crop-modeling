---
title: CropMOSAIKS Model Results
author: 
  - name: Cullen Molitor
    orcid: 0000-0001-8309-8821
    url: https://github.com/cullen-molitor
    affiliations:
      - ref: ucsb
  - name: Grace Lewin
    url: https://github.com/gracelewin
    affiliations:
      - ref: ucsb
  - name: Juliet Cohen
    url: https://github.com/julietcohen
    affiliations:
      - ref: nceas
affiliations:
  - id: ucsb
    name: University of California, Santa Barbara
    city: Santa Barbara
    state: CA
  - id: nceas
    name: National Center for Ecological Analysis and Synthesis
    city: Santa Barbara
    state: CA
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    toc: true
    toc-title: Contents
    toc-depth: 4
    number-sections: false
fig-cap-location: bottom
---

```{r echo=F,message=F, warning=F}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)

if (!require(librarian,quietly = T)){
  install.packages('librarian')
}

librarian::shelf(
  plotly,
  tidyverse,
  here,
  rlang,
  latex2exp,
  arrow,
  quiet = T
)

one_sensor_date <- "2022-10-26"
two_sensor_date <- "2022-10-28"
anomaly_date    <- "2022-11-02"

one_sensor_fn <- paste0("results_", one_sensor_date, ".csv")
two_sensor_fn <- paste0("2_sensor_results_", two_sensor_date, ".csv")
anomaly_fn    <- paste0("anomaly_results_", anomaly_date, ".csv")
pred_suffix   <- 'landsat-8-c2-l2_bands-1-2-3-4-5-6-7_ZMB_20k-points_1000-features_yr-2013-2021_mn-4-9_lm-True_cm-True_wa-True_he-True'
high_res_fn   <- paste0('high-res-pred_k-fold-cv_', pred_suffix, '.feather')
summary_fn    <- paste0('summary-pred_k-fold-cv_', pred_suffix, '.csv')

one_sensor_results <- here::here("data", "results", one_sensor_fn) %>% 
  read_csv(show_col_types = FALSE) %>% 
  filter(points != 20 | crop_mask == T) %>% 
  mutate(avg_res = (kfold_val_R2 + kfold_demean_R2) / 2)

two_sensor_results <- here::here("data", "results", two_sensor_fn) %>%
  read_csv(show_col_types = FALSE) %>% 
  distinct(across(18:41), .keep_all = TRUE) %>%
  mutate(
    satellite = paste0(satellite_1,' with ',satellite_2),
    month_range = case_when(
      month_range_1 == '4-9' & month_range_2 == '4-9' ~ '4-9',
      month_range_1 == '1-12' & month_range_2 == '1-12' ~ '1-12',
      T ~ 'mixed'),
    crop_mask = case_when(
      crop_mask_1 == T & crop_mask_2 == T ~ 'TRUE',
      crop_mask_1 == F & crop_mask_2 == F ~ 'FALSE',
      T ~ 'mixed'),
    weighted_avg = case_when(
      weighted_avg_1 == T & weighted_avg_2 == T ~ 'TRUE',
      weighted_avg_1 == F & weighted_avg_2 == F ~ 'FALSE',
      T ~ 'mixed')) %>% 
  mutate(avg_res = (kfold_val_R2 + kfold_demean_R2) / 2)

anomaly_results <- here::here("data", "results", anomaly_fn) %>% 
  read_csv(show_col_types = FALSE) %>% 
  filter(points != 20 | crop_mask == T)

high_res_predictions <- here::here('data', 'results', high_res_fn) %>% 
  arrow::read_feather()

summary_predictions <- here::here('data', 'results', summary_fn) %>% 
  readr::read_csv() %>% 
  mutate(split = factor(split, levels = c('train', 'test')))

country_shp <- here::here('data', 'geo_boundaries', 'gadm36_ZMB_2.shp') %>% 
  sf::read_sf()
```

```{r echo=F,message=F, warning=F}
colors <- c("TRUE" = 'dodgerblue', "FALSE" = "firebrick",
            'mixed' = 'darkorchid',
            "4-9" = 'dodgerblue', "1-12" = "firebrick")

expand_fld <- function(fld) {
  switch(
    rlang::as_name(fld),
    'kfold_val_R2'    = 'K-Fold Validation Score (R$^2$)',
    'logo_val_R2'     = 'LOGO Validation Score (R$^2$)',
    'kfold_demean_R2' = 'De-meaned Score (R$^2$)',
    'month_range'     = 'Month\nRange',
    'hot_encode'      = 'One Hot Encoding',
    'crop_mask'       = 'Crop Mask',
    'weighted_avg'    = 'Weighted Average',
    rlang::as_name(fld)
  ) %>% TeX()
}

theme_set(theme_bw())
theme_update()

custom_plot <- function(data, x, y, clr, p_type = "scatter", y_lims = NULL) {
  x     <- enquo(x)
  y     <- enquo(y)
  clr   <- enquo(clr)
  x_lab <- expand_fld(x)
  y_lab <- expand_fld(y)
  c_lab <- expand_fld(clr)
  
  base  <- ggplot(data) +
    aes(!!x, !!y, color = !!clr) +
    scale_color_manual(values = colors, limits = force) +
    labs(x = x_lab, y = y_lab, color = c_lab) + 
    facet_wrap(~satellite) +
    scale_y_continuous(limits = y_lims, expand = expansion(mult = c(.01, .01)))
  
  if (p_type == "box") {
    base + geom_boxplot() 
  } else {
    jd  <- position_jitterdodge(.2)
    base + geom_point(position = jd)
  }
}
```

# Summary

This script produces graphics to explore the results of the statistics gathered as a result of iteratively modeling crop yield on random convolutional features created from satellite imagery. Each model iteration represents a combination of unique inputs, described in detail below.   

## TODO: 

1. Fill in results with explanatory text

# Data

**Zambian crop yield**: Maize crop forecast survey (CFS) data summarized to the district level for 2009 to 2022. The Zambian Maize growing season spans the calendar year with planting typically done in November and harvest in June through August of the following year. A crop year of 2009 therefore spans 2008-2009. Crop forecast surveys are conducted in May of the harvest year to aid in national level post harvest planning. Data is provided by the Baylis lab of University of California, Santa Barbara and University of Chicago. [Link](https://baylislab.ace.illinois.edu/).

**sentinel-2-l2a (S2)**: Imagery from the Sentinel constellation of satellites including Sentinel-2A and Sentinel-2B. [Link](https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a).

**landsat-8-c2-l2 (LS8)**: Imagery from Landsat 8. [Link](https://www.usgs.gov/landsat-missions/landsat-8).

**landsat-c2-l2 (LSC)**: Imagery from the Landsat constellation of satellites including Landsat 5, Landsat 7, and Landsat 8 (if we go into 2022, then also Landsat 9). [Link](https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2).

Landsat 7 suffered a [scan line correction failure](https://www.usgs.gov/landsat-missions/landsat-7?qt-science_support_page_related_con=0#qt-science_support_page_related_con) in 2003. All imagery thereafter contains striping with missing data, which is typically worse at the eastern and western edges of the image. There is a center strip from north to south that is unaffected. We use patches of a full  satellite scene, and patches with striping are unsuitable for convolution directly. Therefore, all Landsat 7 imagery used has been corrected with simple imputation through a nearest neighbor approach to approximate the missing pixel values. We keep track of the amount of percent of missing pixels. 

The nearest neighbor is a simple imputation method and was selected due to Landsat pass-over rate of 16 days. This means many months there is only a single image over a given point. We use monthly imagery rather than aggregate across months, thus nearest neighbor is the simplest method. 

**Time range**:

-   Crop yield - 2009 to 2022
-   S2 - June 2015 to Current
-   LS8 - April 2013 to Current
-   LSC - October 2008 to Current (Begins in 1982, limited to useful range)

**Resolution**:  

-   Crop yield - Administrative boundary level 2, Zambian districts    
-   S2 - 10 m ground sample distance (GSD)  
-   LS8 - 30 m GSD  
-   LSC - 30 m GSD    

**Data access**: Satellite imagery is all publicly available. All imagery was accessed and processed through the [Microsoft Planetary Computer](https://planetarycomputer.microsoft.com/) in this project. Crop forecast survey data is not yet publicly available, but will be made available at the time of publication. 

# Methods

The core modeling of this project is done in other notebooks. A brief description of the modeling process is provided below. The rest of this notebook explores the results of iterating over model inputs used in the 'task modeling' process.

## Random convolution feature modeling

Fill in later with summarized description of RCF.

## Task modeling

All models use ridge regression on data summarized to the district level (both yield and features) and split into a training set (80%) and a test set (20%). Each combinations of model inputs are used to train 2 separate models with different cross validation (CV) strategies. One uses "k-fold" CV, and one the other uses "leave one group out" CV. The two strategies and their implications are described below. Additionally, each model uses a grid search method to identify and select the "best" regularization parameter by iterating the CV process over an array of values, and selecting the value which provides the best performance. In our case we search over an array of 17 values between 0 and 100,000,000:
`1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1,`
`1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06, 1.e+07, 1.e+08`. 

### K-fold CV 

The primary model in each iteration uses k-fold cross validation using 5 folds of data. This means the training data is split into five folds, each with 20% of the available data. The model is trained using 4 of the folds as training data, and is validated on the remaining fold. This process is repeated 5 times, using a different fold for validation on each iteration. The performance measures are calculated by taking the predicted and observed values of the validation data in each of the 5 iterations. Our primary metric is the coefficient of determination calculated on the observed and predicted values of the validation data. We chose 5-fold CV for its ability to predict over space as well as over  time... more on why later.     

![Figure 1. Diagram representing k-fold cross validation data splitting strategy](../../figures/kfold_cv_split.png)

### Leave one group out (LOGO) CV 

As a secondary test, we also train a model in each iteration uses leave one group out CV using year as the grouping variable. This means the training data is split into $n$ groups, where $n$ is the number of years in the data, with each group containing $1/n^{th}$ of the available data. Each iteration of the model is trained using $n-1$ groups as training data, and is validated on the remaining group. This process is repeated $n$ times, using a different group for validation on each iteration. The performance measures are calculated by taking the predicted and observed values of the validation data in each of the $n$ iterations. Our primary metric is the coefficient of determination calculated on the observed and predicted values of the validation data. We use LOGO CV as a secondary test for it ability to evaluate how well these models are able to predict over time... More on why later..

![Figure 2. Diagram representing leave one group out cross validation data splitting strategy](../../figures/logo_cv_split.png)

**Note**: Currently we are not splitting into train and test sets for LOGO CV. This is something that should be decided and finalized. 

## Model inputs

The following variables were either included or excluded to test their affect on model performance. 

1) **Sensor platform**: the sensor platform which imagery was taken from. Options are:

    + The Landsat satellite constellation including 5, 7, and 8  
    + Landsat 8 only (this avoids the SLC failure)
    + The Sentinel 2 satellite constellation including 2A and 2B

1) **Spectral bandpass**: the combination of distinct spectral bandpass's collected by the electro-optical sensor and utilized in the convolutional neural network model.

1) **Spatial resolution**: the number of 0.01 by 0.01 degree grid cells we targeted for featurization. These are represented as points in longitude and latitude, where the coordinates are the center of the grid cell. Exact area covered by each grid cell varies with latitude and elevation. 

1) **One hot encoding**: when true, the Zambian districts were used as dummy variables. Each district becomes a column with either inside district (1) or outside district (0). This is equivalent to a spatial fixed effects model. When false, district is dropped altogether. 

1) **Crop mask**: when true, only points with cropland are retained. In some iterations, this becomes redundant as only points with cropland were featurized, rather than including a random selection of points (targeted vs non-targeted approach). When false, all points are retained.   

1) **Weighted average**: when true, a weighted average was used in summarizing the features to the district level. Weights are the calculated proportion of cropland over the area where the features were calculated. When false, a simple mean is used in summarizing the features to the district level.  

1) **Limiting month range**: whether the months were limited to the dry season (from April to October, or using the full year range. Limiting month ranges to the dry season results in more high quality imagery (i.e., low cloud cover). 

1) **Available year range**: features are limited in overall time range by the launch date of the individual sensor platforms. 

# Results 

Here we break the results into two sections based on the statistic we are interested in evaluating. We collect the statistics from running each model with varied inputs described in the methods section. The distribution of these statistics are used to visualize the outcome of manipulating those model inputs. There are `r length(one_sensor_results$kfold_val_R2)` unique outcomes from the combinations of these model inputs from 1 sensor and `r length(two_sensor_results$kfold_val_R2)` from combining 2 sensors. 

## Validation score (R<sup>2</sup>)

The first statistic we evaluate is the coefficient of determination (R<sup>2</sup>) calculated from the predicted values and the observed values from each of the 5 validation sets. Here we see the distribution of all validation scores. 

**Note**: The scales are forced to be the same between K-Fold and LOGO. The idea was to evaluate the relative difference in performance, but it makes some distributions harder to interpret. Would it help to make them independent?

```{r echo = FALSE}
min_lim <- min(
  min(one_sensor_results$kfold_val_R2), 
  min(one_sensor_results$logo_val_R2)
)
max_lim <- max(
  max(one_sensor_results$kfold_val_R2), 
  max(one_sensor_results$logo_val_R2)
)
limits <- c(min_lim, max_lim)

min_lim_two <- min(
  min(two_sensor_results$kfold_val_R2), 
  min(two_sensor_results$logo_val_R2)
)
max_lim_two <- max(
  max(two_sensor_results$kfold_val_R2), 
  max(two_sensor_results$logo_val_R2)
)
limits_two <- c(min_lim_two, max_lim_two)
```

### Overall

::: {.panel-tabset}

#### One sensor

::: {.panel-tabset}

##### K-Fold

```{r}
ggplot(data = one_sensor_results) +
  aes(x = kfold_val_R2) +
  geom_histogram(bins = 30, fill = 'dodgerblue', color = 'black') +
  labs(x = expand_fld('kfold_val_R2')) +
  scale_x_continuous(limits = limits, expand = expansion(mult = c(.01, .01)))
```

##### LOGO

```{r}
ggplot(data = one_sensor_results) +
  aes(x = logo_val_R2) +
  geom_histogram(bins = 30, fill = 'dodgerblue', color = 'black') +
  labs(x = expand_fld('logo_val_R2')) +
  scale_x_continuous(limits = limits, expand = expansion(mult = c(.01, .01)))
```

:::

#### Two sensor

::: {.panel-tabset}

##### K-Fold

```{r}
ggplot(data = two_sensor_results) +
  aes(x = kfold_val_R2) +
  geom_histogram(bins = 60, fill = 'darkorchid', color = 'black') +
  labs(x = expand_fld('kfold_val_R2')) +
  scale_x_continuous(limits = limits_two, expand = expansion(mult = c(.01, .01)))
```

##### LOGO

```{r}
ggplot(data = two_sensor_results) +
  aes(x = logo_val_R2) +
  geom_histogram(bins = 60, fill = 'darkorchid', color = 'black') +
  labs(x = expand_fld('logo_val_R2')) +
  scale_x_continuous(limits = limits_two, expand = expansion(mult = c(.01, .01)))
```

:::

:::

### Hot encoding

::: {.panel-tabset}

#### One sensor

::: {.panel-tabset}

##### K-Fold

```{r}
custom_plot(
  data = one_sensor_results, 
  x = hot_encode, 
  y = kfold_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits
)
```

##### LOGO

```{r}
custom_plot(
  data = one_sensor_results, 
  x = hot_encode, 
  y = logo_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits
)
```

:::

#### Two sensor

::: {.panel-tabset}

##### K-Fold

```{r fig.height=8, fig.width=8}
custom_plot(
  data = two_sensor_results, 
  x = hot_encode, 
  y = kfold_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits_two
)
```

##### LOGO

```{r fig.height=8, fig.width=8}
custom_plot(
  data = two_sensor_results, 
  x = hot_encode, 
  y = logo_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits_two
)
```

:::

:::

### Crop mask

::: {.panel-tabset}

#### One sensor

::: {.panel-tabset}

##### K-Fold

```{r}
custom_plot(
  data = one_sensor_results, 
  x = crop_mask, 
  y = kfold_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits
)
```

##### LOGO

```{r}
custom_plot(
  data = one_sensor_results, 
  x = crop_mask, 
  y = logo_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits
)
```

:::

#### Two sensor

::: {.panel-tabset}

##### K-Fold

```{r fig.height=8, fig.width=8}
custom_plot(
  data = two_sensor_results, 
  x = crop_mask, 
  y = kfold_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits_two
)
```

##### LOGO

```{r fig.height=8, fig.width=8}
custom_plot(
  data = two_sensor_results, 
  x = crop_mask, 
  y = logo_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits_two
)
```

:::

:::

### Weighted average

::: {.panel-tabset}

#### One sensor

::: {.panel-tabset}

##### K-Fold

```{r }
custom_plot(
  data = one_sensor_results, 
  x = weighted_avg, 
  y = kfold_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits
)
```

##### LOGO

```{r }
custom_plot(
  data = one_sensor_results, 
  x = weighted_avg, 
  y = logo_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits
)
```

:::

#### Two sensor

::: {.panel-tabset}

##### K-Fold

```{r fig.height=8, fig.width=8}
custom_plot(
  data = two_sensor_results, 
  x = weighted_avg, 
  y = kfold_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits_two
)
```

##### LOGO

```{r fig.height=8, fig.width=8}
custom_plot(
  data = two_sensor_results, 
  x = weighted_avg, 
  y = logo_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits_two
)
```

:::

:::

## De-meaned score (R<sup>2</sup>)

To get an idea of the each models ability to predict crop yield over time, we remove spatial variability by subtracting the district means from both observed and predicted yields. These de-meaned values are then evaluated by calculating the coefficient of determination. A high score indicates the model is able to predict well over time, while a low score indicates it performed this task poorly. Here we see the distribution of all de-meaned scores.

### Overall

::: {.panel-tabset}

#### One sensor

```{r}
ggplot(data = one_sensor_results) +
  aes(x = kfold_demean_R2) +
  geom_histogram(bins = 30, fill = 'dodgerblue', color = 'black') +
  labs(x = expand_fld('kfold_demean_R2'))
```

#### Two sensor

```{r}
ggplot(data = two_sensor_results) +
  aes(x = kfold_demean_R2) +
  geom_histogram(bins = 60, fill = 'darkorchid', color = 'black') +
  labs(x = expand_fld('kfold_demean_R2'))
```

:::

### Hot encoding

::: {.panel-tabset}

#### One sensor

```{r}
custom_plot(
  data = one_sensor_results, 
  x = hot_encode, 
  y = kfold_demean_R2,
  clr = month_range,
  p_type = "box"
)
```

#### Two sensor

```{r fig.height=8, fig.width=8}
custom_plot(
  data = two_sensor_results, 
  x = hot_encode, 
  y = kfold_demean_R2,
  clr = month_range,
  p_type = "box"
)
```

:::

### Crop mask

::: {.panel-tabset}

#### One sensor

```{r}
custom_plot(
  data = one_sensor_results, 
  x = crop_mask, 
  y = kfold_demean_R2,
  clr = month_range,
  p_type = "box"
)
```

#### Two sensor

```{r fig.height=8, fig.width=8}
custom_plot(
  data = two_sensor_results, 
  x = crop_mask, 
  y = kfold_demean_R2,
  clr = month_range,
  p_type = "box"
)
```

:::

### Weighted average

::: {.panel-tabset}

#### One sensor

```{r}
custom_plot(
  data = one_sensor_results, 
  x = weighted_avg, 
  y = kfold_demean_R2,
  clr = month_range,
  p_type = "box"
)
```

#### Two sensor

```{r fig.height=8, fig.width=8}
custom_plot(
  data = two_sensor_results, 
  x = weighted_avg, 
  y = kfold_demean_R2,
  clr = month_range,
  p_type = "box"
)
```

:::

## Anomaly validation score (R<sup>2</sup>)

Finally, we evaluate the coefficient of determination (R<sup>2</sup>) calculated from the predicted values and the observed values from each of the 5 validation sets of the models trained on feature anomalies. Here we see the distribution of all validation scores.


**Note**: The scales are forced to be the same between K-Fold and LOGO. The idea was to evaluate the relative difference in performance, but it makes some distributions harder to interpret. Would it help to make them independent?

```{r echo = FALSE}
min_lim <- min(
  min(anomaly_results$kfold_val_R2), 
  min(anomaly_results$logo_val_R2)
)
max_lim <- max(
  max(anomaly_results$kfold_val_R2), 
  max(anomaly_results$logo_val_R2)
)
limits <- c(min_lim, max_lim)
```

### Overall

::: {.panel-tabset}

#### K-Fold

```{r}
ggplot(data = anomaly_results) +
  aes(x = kfold_val_R2) +
  geom_histogram(bins = 30, fill = 'dodgerblue', color = 'black') +
  labs(x = expand_fld('kfold_val_R2')) +
  scale_x_continuous(limits = limits, expand = expansion(mult = c(.01, .01)))
```

#### LOGO

```{r}
ggplot(data = anomaly_results) +
  aes(x = logo_val_R2) +
  geom_histogram(bins = 30, fill = 'dodgerblue', color = 'black') +
  labs(x = expand_fld('logo_val_R2')) +
  scale_x_continuous(limits = limits, expand = expansion(mult = c(.01, .01)))
```

:::

### Hot encoding

::: {.panel-tabset}

#### K-Fold

```{r}
custom_plot(
  data = anomaly_results, 
  x = hot_encode, 
  y = kfold_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits
)
```

#### LOGO

```{r}
custom_plot(
  data = anomaly_results, 
  x = hot_encode, 
  y = logo_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits
)
```

:::

### Crop mask

::: {.panel-tabset}

#### K-Fold

```{r}
custom_plot(
  data = anomaly_results, 
  x = crop_mask, 
  y = kfold_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits
)
```

#### LOGO

```{r}
custom_plot(
  data = anomaly_results, 
  x = crop_mask, 
  y = logo_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits
)
```

:::

### Weighted average

::: {.panel-tabset}

#### K-Fold

```{r }
custom_plot(
  data = anomaly_results, 
  x = weighted_avg, 
  y = kfold_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits
)
```

#### LOGO

```{r }
custom_plot(
  data = anomaly_results, 
  x = weighted_avg, 
  y = logo_val_R2,
  clr = month_range,
  p_type = "box",
  y_lims = limits
)
```

:::

## Predictions

Result from using the top performing model to make predictions on the high_reolution features (before summarizing to district level). 

Top performing model:

1) **Sensor platform**: Landsat 8 

1) **Spectral bandpass**: bands 1-7 (AOT, R, G, B, NIR, SWIR16, SWIR22)

1) **Spatial resolution**: 20,000 0.01 by 0.01 degree grid cells, taken from the top 10% of cropland area in each district.

1) **One hot encoding**: true, the Zambian districts were used as dummy variables. 

1) **Crop mask**: true, only points with cropland are retained. This is by default with the targeted method of selecting points described abovve.  

1) **Weighted average**: true, a weighted average was used in summarizing the features to the district level. 

1) **Limiting month range**: the months were limited to the dry season (from April to September).

1) **Available year range**: 2013 to 2021. 

The top performing model used ridge regression with 5-fold cross validation. Evaluated by taking the mean of the validation score and the de-meaned R2 score.

**Notes**: 

1) Need to discuss how to select the "best model". 

1) I cap the predictions at roughly the level of the yield levels. This is largely for plotting purpose, need to discuss in next meeting!

### Observed vs predicted 

::: {.panel-tabset}

#### Validation

```{r warning = FALSE}
train_pred <- filter(summary_predictions, split == "train")

r2_general <-function(preds,actual){ 
  r2 <- 1- sum((preds - actual) ^ 2)/sum((actual - mean(actual))^2)
  return(round(r2, 3))
}
# rsq <- function (x, y) cor(x, y) ^ 2

val_R2 <- r2_general(train_pred$log_yield, train_pred$kfold_cv_predictions)

val_R2_label <- latex2exp::TeX(
  "$R^2$ = \\r2", user_defined = list("\\r2"=val_R2))

ggplot() +
  geom_point(data = train_pred,
             aes(x = log_yield, y = kfold_cv_predictions, color = as.factor(year))) +
  geom_abline() +
  scale_color_viridis_d() +
  labs(color = 'Year', x = 'Observed', y = 'Predicted') +
  geom_text(data = NULL, aes(x = 0.05, y = .7), label = val_R2_label) +
  scale_x_continuous(limits = c(0, .82)) +
  scale_y_continuous(limits = c(-.15, .82))
```

#### Train

```{r warning = FALSE}
train_R2 <- r2_general(train_pred$log_yield, train_pred$prediction)

train_R2_label <- latex2exp::TeX(
  "$R^2$ = \\r2", user_defined = list("\\r2"=train_R2))

ggplot() +
  geom_point(data = train_pred,
             aes(x = log_yield, y = prediction, color = as.factor(year))) +
  geom_abline() +
  scale_color_viridis_d() +
  labs(color = 'Year', x = 'Observed', y = 'Predicted') +
  geom_text(data = NULL, aes(x = 0.05, y = .70), label = train_R2_label) +
  scale_x_continuous(limits = c(0, .82)) +
  scale_y_continuous(limits = c(-.15, .82))
```

#### Test

```{r warning = FALSE}
test_pred <- filter(summary_predictions, split == "test")
test_R2 <- r2_general(test_pred$log_yield, test_pred$prediction)

test_R2_label <- latex2exp::TeX(
  "$R^2$ = \\r2", user_defined = list("\\r2"=test_R2))

ggplot() +
  geom_point(data = test_pred,
             aes(x = log_yield, y = prediction, color = as.factor(year))) +
  geom_abline() +
  scale_color_viridis_d() +
  labs(color = 'Year', x = 'Observed', y = 'Predicted') +
  geom_text(data = NULL, aes(x = 0.05, y = .7), label = test_R2_label) +
  scale_x_continuous(limits = c(0, .82)) +
  scale_y_continuous(limits = c(-.15, .82))
```

:::

### Maps

::: {.panel-tabset}

#### Log yield

```{r fig.height=10, fig.width=10}
yield <- summary_predictions %>% 
  mutate(residuals = prediction - log_yield) %>% 
  left_join(country_shp) %>% 
  sf::st_as_sf()

ggplot() +
  geom_sf(data = yield, aes(fill = log_yield), color = 'black') +
  facet_wrap(~year) +
  scale_fill_viridis_c()
```

#### Predicted yield

```{r fig.height=10, fig.width=10}
ggplot() +
  geom_sf(data = yield, aes(fill = prediction), color = 'black') +
  facet_wrap(~year) +
  scale_fill_viridis_c()
```

#### Residual


```{r fig.height=10, fig.width=10}
ggplot() +
  geom_sf(data = yield, aes(fill = residuals), color = 'black') +
  facet_wrap(~year) +
  scale_fill_viridis_c()
```

$predicted - observed$ so positive values indicates over-prediction, and negative values indicates under-prediction.

#### High res. predictions

```{r fig.height=10, fig.width=10}
high_res_predictions <- high_res_predictions %>% 
  mutate(prediction = ifelse(prediction > 1.1, 1, prediction))

ggplot() +
  geom_point(data = high_res_predictions, size = .1,
             aes(x = lon, y = lat, color = prediction)) +
  geom_sf(data = country_shp, fill = NA, color = 'black') +
  facet_wrap(~year) +
  scale_color_viridis_c()
```

:::
