{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "440e592b-c09d-4e75-91fc-00e5d36d391d",
   "metadata": {},
   "source": [
    "## MOSAIKS meta data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522ab90b-af76-477d-a930-4d63c6847028",
   "metadata": {
    "gather": {
     "logged": 1650114371790
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/geopandas/dask-geopandas\n",
    "!pip install -q pyhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "281d0543-f6b0-4b68-a4ba-a99c547b00c8",
   "metadata": {
    "gather": {
     "logged": 1651174535306
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import calendar\n",
    "import re\n",
    "\n",
    "RASTERIO_BEST_PRACTICES = dict(  # See https://github.com/pangeo-data/cog-best-practices\n",
    "    CURL_CA_BUNDLE=\"/etc/ssl/certs/ca-certificates.crt\",\n",
    "    GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\",\n",
    "    AWS_NO_SIGN_REQUEST=\"YES\",\n",
    "    GDAL_MAX_RAW_BLOCK_CACHE_SIZE=\"200000000\",\n",
    "    GDAL_SWATH_SIZE=\"200000000\",\n",
    "    VSI_CURL_CACHE_SIZE=\"200000000\",\n",
    ")\n",
    "os.environ.update(RASTERIO_BEST_PRACTICES)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyhere import here\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scipy import ndimage as nd\n",
    "\n",
    "import rasterio\n",
    "import rasterio.warp\n",
    "import rasterio.mask\n",
    "import shapely.geometry\n",
    "import geopandas\n",
    "import dask_geopandas\n",
    "from dask.distributed import Client\n",
    "\n",
    "from pystac import Item\n",
    "import stackstac\n",
    "import pyproj\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=UserWarning, module=\"torch\")\n",
    "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(action=\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "import pystac_client\n",
    "import planetary_computer as pc\n",
    "\n",
    "\n",
    "# Disabling the benchmarking feature with torch.backends.cudnn.benchmark = False \n",
    "# causes cuDNN to deterministically select an algorithm, possibly at the cost of reduced performance.\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cefa598-0653-4fd4-b7ce-14fb7c4e59e2",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28b414c-1d02-491a-9518-1b777898950d",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1651174535433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "num_features = 1024\n",
    "country_code = 'ZMB'\n",
    "use_file = True\n",
    "# use_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce354d45-8f23-4630-ae55-95738e4a443d",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1651174535433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "satellite = \"landsat-c2-l2\"\n",
    "bands = [\"red\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb7126f-4aaf-4615-82da-efc06a2140bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if satellite == \"landsat-c2-l2\":\n",
    "    resolution = 30\n",
    "    min_image_edge = 6\n",
    "else:\n",
    "    resolution = 10\n",
    "    min_image_edge = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae9b0ec-c334-4751-91c4-91dabff6a44a",
   "metadata": {},
   "source": [
    "## Create grid and sample points to featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b09c0e05-9ba6-407e-9260-5f9f00decc18",
   "metadata": {
    "gather": {
     "logged": 1651174535812
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19598, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if use_file:\n",
    "    gdf = pd.read_feather(here('data', 'land_cover', 'ZMB_cropland_percentage_20k-points.feather'))\n",
    "    gdf = (\n",
    "        geopandas\n",
    "        .GeoDataFrame(\n",
    "            gdf, \n",
    "            geometry = geopandas.points_from_xy(x = gdf.lon, y = gdf.lat), \n",
    "            crs='EPSG:4326')\n",
    "    )\n",
    "else:\n",
    "    cell_size = 0.01  # Roughly 1 km\n",
    "    ### get country shape\n",
    "    country_file_name = f\"data/geo_boundaries/africa_adm0.geojson\"\n",
    "    africa = geopandas.read_file(country_file_name)\n",
    "    country = africa[africa.adm0_a3 == country_code]\n",
    "    #### This would be simpler, but throws an error down the line if used \n",
    "    # world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "    # country = world.query(f'iso_a3 == \"{country_code}\"')\n",
    "    ### Create grid of points\n",
    "    cell_size = .01  # Very roughly 1 km\n",
    "    xmin, ymin, xmax, ymax = country.total_bounds\n",
    "    xs = list(np.arange(xmin, xmax + cell_size, cell_size))\n",
    "    ys = list(np.arange(ymin, ymax + cell_size, cell_size))\n",
    "    def make_cell(x, y, cell_size):\n",
    "        ring = [\n",
    "            (x, y),\n",
    "            (x + cell_size, y),\n",
    "            (x + cell_size, y + cell_size),\n",
    "            (x, y + cell_size)\n",
    "        ]\n",
    "        cell = shapely.geometry.Polygon(ring).centroid\n",
    "        return cell\n",
    "    center_points = []\n",
    "    for x in xs:\n",
    "        for y in ys:\n",
    "            cell = make_cell(x, y, cell_size)\n",
    "            center_points.append(cell)\n",
    "    ### Put grid into a GeDataFrame for cropping to country shape\n",
    "    gdf = geopandas.GeoDataFrame({'geometry': center_points}, crs = 'EPSG:4326')\n",
    "    gdf['lon'], gdf['lat'] = gdf.geometry.x, gdf.geometry.y\n",
    "    ### Subset to country \n",
    "    ### This buffer ensures that no points are take at the border \n",
    "    ### which would lead to duplication with neighboring countries\n",
    "    gdf = gdf[gdf.within(country.unary_union.buffer(-0.005))]\n",
    "    gdf = gdf[['lon', 'lat', 'geometry']].reset_index(drop = True)\n",
    "    gdf = gdf.sample(frac = 0.1, random_state=42, ignore_index=False)\n",
    "    points = gdf[[\"lon\", \"lat\"]].to_numpy()\n",
    "pt_len = gdf.shape[0]\n",
    "gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebfd0c2e-78fe-45a9-99a4-13112f0da841",
   "metadata": {
    "gather": {
     "logged": 1651174537641
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NPARTITIONS = 250\n",
    "\n",
    "ddf = dask_geopandas.from_geopandas(gdf, npartitions=1)\n",
    "hd = ddf.hilbert_distance().compute()\n",
    "gdf[\"hd\"] = hd\n",
    "gdf = gdf.sort_values(\"hd\")\n",
    "\n",
    "dgdf = dask_geopandas.from_geopandas(gdf, npartitions=NPARTITIONS, sort=False)\n",
    "\n",
    "del ddf, hd, gdf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b07b3638-e152-4858-972a-5bef5486ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 0.005\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, points, items, buffer=buffer_size):\n",
    "        self.points = points\n",
    "        self.items = items\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.points.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        lon, lat = self.points[idx]\n",
    "        fn = self.items[idx]\n",
    "\n",
    "        if fn is None:\n",
    "            return None\n",
    "        else:\n",
    "            stack = stackstac.stack(fn, assets=bands, resolution=resolution)\n",
    "            x_min, y_min = pyproj.Proj(stack.crs)(lon-self.buffer, lat-self.buffer)\n",
    "            x_max, y_max = pyproj.Proj(stack.crs)(lon+self.buffer, lat+self.buffer)\n",
    "            aoi = stack.loc[..., y_max:y_min, x_min:x_max]\n",
    "            data = aoi.data.squeeze()\n",
    "            na_percentage = np.isnan(data).sum() / (data.shape[0] * data.shape[1])\n",
    "            return na_percentage\n",
    "        \n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, points, items, buffer=buffer_size):\n",
    "#         self.points = points\n",
    "#         self.items = items\n",
    "#         self.buffer = buffer\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.points.shape[0]\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "\n",
    "#         lon, lat = self.points[idx]\n",
    "#         fn = self.items[idx]\n",
    "\n",
    "#         if fn is None:\n",
    "#             return None\n",
    "#         else:\n",
    "#             stack = stackstac.stack(\n",
    "#                 fn,\n",
    "#                 assets=bands,\n",
    "#                 resolution=resolution,\n",
    "#             )\n",
    "#             x_min, y_min = pyproj.Proj(stack.crs)(lon-self.buffer, lat-self.buffer)\n",
    "#             x_max, y_max = pyproj.Proj(stack.crs)(lon+self.buffer, lat+self.buffer)\n",
    "#             aoi = stack.loc[..., y_max:y_min, x_min:x_max]\n",
    "#             data = aoi.compute(\n",
    "#                 scheduler=\"single-threaded\"\n",
    "#                 )\n",
    "#             out_image = data.data.squeeze()\n",
    "#             out_image = torch.from_numpy(out_image).float()\n",
    "#             return out_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fe0280-66ed-4491-9bec-d69e08f67840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:  \n",
      "    Satellite: landsat-c2-l2  \n",
      "    Pixel resolution: 30  \n",
      "    Grid resolution: 0.01 degree squared (WGS84) \n",
      "    Cloud limit: 20%  \n",
      "    Bands: ['red'] \n",
      "    Number of points: 19598 \n",
      "    Number of features: 1024 features \n",
      "    Year range: 2017 to 2022 \n",
      "\n",
      "Matching images to points for: 7-2017\n",
      "Found acceptable images for 19598/19598 points in 142.54 seconds\n",
      "Collecting metadata: 07-2017\n",
      "0/19598 -- 0.00% -- 1.38 seconds\n",
      "1000/19598 -- 5.10% -- 278.44 seconds\n",
      "2000/19598 -- 10.21% -- 282.43 seconds\n",
      "3000/19598 -- 15.31% -- 265.12 seconds\n",
      "4000/19598 -- 20.41% -- 260.30 seconds\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start_month = 7\n",
    "year_start = 2017\n",
    "year_end = 2022\n",
    "\n",
    "cloud_limit = 20\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "workers = os.cpu_count() \n",
    "\n",
    "print(\n",
    "f\"\"\"\n",
    "Parameters:  \n",
    "    Satellite: {satellite}  \n",
    "    Pixel resolution: {resolution}  \n",
    "    Grid resolution: {buffer_size * 2} degree squared (WGS84) \n",
    "    Cloud limit: {cloud_limit}%  \n",
    "    Bands: {bands} \n",
    "    Number of points: {pt_len} \n",
    "    Number of features: {num_features} features \n",
    "    Year range: {year_start} to {year_end} \n",
    "\"\"\"\n",
    ")\n",
    "for yr in range(year_start, year_end+1):\n",
    "    \n",
    "    # data = pd.DataFrame()\n",
    "    df = []\n",
    "\n",
    "    if (yr == year_start):\n",
    "        month_range = range(start_month, 13)\n",
    "    else:\n",
    "        month_range = range(1, 13) \n",
    "\n",
    "    for mn in month_range:\n",
    "\n",
    "        if mn < 10:\n",
    "            month = \"0\"+str(mn)\n",
    "        else:\n",
    "            month = mn\n",
    "\n",
    "        def query(points):\n",
    "            \"\"\"\n",
    "            Find a STAC item for points in the `points` DataFrame\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            points : geopandas.GeoDataFrame\n",
    "                A GeoDataFrame\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            geopandas.GeoDataFrame\n",
    "                A new geopandas.GeoDataFrame with a `stac_item` column containing the STAC\n",
    "                item that covers each point.\n",
    "            \"\"\"\n",
    "            intersects = shapely.geometry.mapping(points.unary_union.convex_hull)\n",
    "\n",
    "            catalog = pystac_client.Client.open(\n",
    "                \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
    "            )\n",
    "            # Define search date range for query\n",
    "            ending_day = calendar.monthrange(yr, int(mn))[1]\n",
    "            search_start = f\"{yr}-{month}-1\" \n",
    "            search_end = f\"{yr}-{month}-{ending_day}\" \n",
    "\n",
    "            # The time frame in which we search for non-cloudy imagery\n",
    "            search = catalog.search(\n",
    "                collections=[satellite],  \n",
    "                intersects=intersects,\n",
    "                datetime=[search_start, search_end],\n",
    "                query={\"eo:cloud_cover\": {\"lt\": cloud_limit}},\n",
    "                limit=500,\n",
    "            )\n",
    "            ic = search.get_all_items_as_dict()\n",
    "            features = ic[\"features\"]\n",
    "            features_d = {item[\"id\"]: item for item in features}\n",
    "            data = {\n",
    "                \"eo:cloud_cover\": [],\n",
    "                \"geometry\": [],\n",
    "            }\n",
    "            index = []\n",
    "            for item in features:\n",
    "                data[\"eo:cloud_cover\"].append(item[\"properties\"][\"eo:cloud_cover\"])\n",
    "                data[\"geometry\"].append(shapely.geometry.shape(item[\"geometry\"]))\n",
    "                index.append(item[\"id\"])\n",
    "            items = geopandas.GeoDataFrame(data, index=index, geometry=\"geometry\").sort_values(\n",
    "                \"eo:cloud_cover\"\n",
    "            )\n",
    "            point_list = points.geometry.tolist()\n",
    "            point_items = []\n",
    "            for point in point_list:\n",
    "                covered_by = items[items.covers(point)]\n",
    "                if len(covered_by):\n",
    "                    point_items.append(features_d[covered_by.index[0]])\n",
    "                else:\n",
    "                    # There weren't any scenes matching our conditions for this point (too cloudy)\n",
    "                    point_items.append(None)\n",
    "            return points.assign(stac_item=point_items)\n",
    "\n",
    "        tic = time.time()\n",
    "        print(\"Matching images to points for: \", mn, \"-\", yr, sep = \"\")\n",
    "\n",
    "        with Client(n_workers=16) as client:\n",
    "            meta = dgdf._meta.assign(stac_item=[])\n",
    "            df2 = dgdf.map_partitions(query, meta=meta).compute()\n",
    "            \n",
    "        df3 = df2.dropna(subset=[\"stac_item\"]).reset_index(drop = True)\n",
    "\n",
    "        matching_items = []\n",
    "        for item in df3.stac_item.tolist():\n",
    "            signed_item = pc.sign(Item.from_dict(item))\n",
    "            matching_items.append(signed_item)\n",
    "\n",
    "        points = df3[[\"lon\", \"lat\"]].to_numpy()\n",
    "\n",
    "        print(\"Found acceptable images for \", \n",
    "              points.shape[0], \"/\", pt_len,\n",
    "              \" points in \", \n",
    "              f\"{time.time()-tic:0.2f} seconds\", \n",
    "              sep = \"\")\n",
    "\n",
    "        \n",
    "#         print(\"Collecting metadata: \", month, \"-\", yr, sep = \"\") \n",
    "#         na_perc = np.zeros((points.shape[0], 1), dtype=float)\n",
    "#         tic = time.time()\n",
    "#         for i in range(0, len(points)):\n",
    "#             lon, lat = points[i]\n",
    "#             fn = matching_items[i]\n",
    "#             stack = stackstac.stack(fn, assets=bands,  resolution=resolution)\n",
    "#             x_min, y_min = pyproj.Proj(stack.crs)(lon-buffer_size, lat-buffer_size)\n",
    "#             x_max, y_max = pyproj.Proj(stack.crs)(lon+buffer_size, lat+buffer_size)\n",
    "#             aoi = stack.loc[..., y_max:y_min, x_min:x_max]\n",
    "#             out_image = aoi.data.squeeze()\n",
    "#             out_image = torch.from_numpy(out_image.compute()).float()\n",
    "#             na_perc[i] = ((out_image.isnan()).sum() / out_image.numel()).item()\n",
    "#             if i % 1000 == 0:\n",
    "#                 print(\n",
    "#                     f\"{i}/{points.shape[0]} -- {i / points.shape[0] * 100:0.2f}%\"\n",
    "#                     + f\" -- {time.time()-tic:0.2f} seconds\"\n",
    "#                 )\n",
    "#                 tic = time.time()\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "#         print(\"Collecting metadata: \", month, \"-\", yr, sep = \"\")   \n",
    "#         dataset = CustomDataset(points, matching_items)    \n",
    "#         na_perc = np.zeros((points.shape[0], 1), dtype=float)\n",
    "#         tic = time.time()\n",
    "#         i = 0\n",
    "#         for image in dataset:\n",
    "#             na_perc[i] = ((image.isnan()).sum() / image.numel()).item()\n",
    "#             if i % 1000 == 0:\n",
    "#                 print(\n",
    "#                     f\"{i}/{points.shape[0]} -- {i / points.shape[0] * 100:0.2f}%\"\n",
    "#                     + f\" -- {time.time()-tic:0.2f} seconds\"\n",
    "#                 )\n",
    "#                 tic = time.time()\n",
    "#             i += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        dataset = CustomDataset(points, matching_items)\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=workers,\n",
    "            collate_fn=lambda x: x,\n",
    "        )\n",
    "\n",
    "        print(\"Collecting metadata: \", month, \"-\", yr, sep = \"\")\n",
    "\n",
    "        na_perc = np.zeros((points.shape[0], 1), dtype=float)\n",
    "        tic = time.time()\n",
    "        i = 0\n",
    "        for images in dataloader:\n",
    "            for image in images:\n",
    "                \n",
    "                na_perc[i] = image\n",
    "\n",
    "                if i % 1000 == 0:\n",
    "                    print(\n",
    "                        f\"{i}/{points.shape[0]} -- {i / points.shape[0] * 100:0.2f}%\"\n",
    "                        + f\" -- {time.time()-tic:0.2f} seconds\"\n",
    "                    )\n",
    "                    tic = time.time()\n",
    "                i += 1\n",
    "\n",
    "        df3['stac_id'] = df3['stac_item'].apply(pd.Series)['id']\n",
    "        df3['platform'] = df3['stac_item'].apply(pd.Series)['properties'].apply(pd.Series)['platform']\n",
    "        df3['cloud_cover'] = df3['stac_item'].apply(pd.Series)['properties'].apply(pd.Series)['eo:cloud_cover']\n",
    "        df3[['na_percent', 'year', \"month\"]] = na_perc, yr, mn\n",
    "        df3.drop(['geometry', 'hd', 'stac_item'], axis = 1, inplace = True)\n",
    "        df3 = pd.DataFrame(df3)\n",
    "        \n",
    "        fn = f'{satellite}_{country_code}_{pt_len/1000:.0f}k-points_meta_{yr}_{mn}.csv'\n",
    "        file_name = here('data', 'feature_meta_data', fn)\n",
    "        print(\"Saving file as:\", file_name, \"\\n\")\n",
    "        df3.to_csv(file_name, index=False)\n",
    "        \n",
    "        # df.append(df3)\n",
    "        \n",
    "#     data = pd.concat(df).reset_index(drop = True)\n",
    "    \n",
    "#     fn = f'{satellite}_{country_code}_{pt_len/1000:.0f}k-points_meta.csv'\n",
    "#     file_name = here('data', 'feature_meta_data', satellite, fn)\n",
    "#     print(\"Saving file as:\", file_name)\n",
    "#     data.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e03fd0-8ddd-41f5-a9e1-4505c841ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
