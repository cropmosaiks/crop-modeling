{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5303c5",
   "metadata": {},
   "source": [
    "## MOSAIKS feature extraction\n",
    "\n",
    "This tutorial demonstrates the **MOSAIKS** method for extracting _feature vectors_ from satellite imagery patches for use in downstream modeling tasks. It will show:\n",
    "- How to extract 1km$^2$ patches of Sentinel 2 or Landsat multispectral imagery for a list of latitude, longitude points\n",
    "- How to extract summary features from each of these imagery patches\n",
    "- How to use the summary features in a linear model of the population density at each point\n",
    "\n",
    "### Background\n",
    "\n",
    "Consider the case where you have a dataset of latitude and longitude points assosciated with some dependent variable (for example: population density, weather, housing prices, biodiversity) and, potentially, other independent variables. You would like to model the dependent variable as a function of the independent variables, but instead of including latitude and longitude directly in this model, you would like to include some high dimensional representation of what the Earth looks like at that point (that hopefully explains some of the variance in the dependent variable!). From the computer vision literature, there are various [representation learning techniques](https://en.wikipedia.org/wiki/Feature_learning) that can be used to do this, i.e. extract _features vectors_ from imagery. This notebook gives an implementation of the technique described in [Rolf et al. 2021](https://www.nature.com/articles/s41467-021-24638-z), \"A generalizable and accessible approach to machine learning with global satellite imagery\" called Multi-task Observation using Satellite Imagery & Kitchen Sinks (**MOSAIKS**). For more information about **MOSAIKS** see the [project's webpage](http://www.globalpolicy.science/mosaiks).\n",
    "\n",
    "### Environment setup\n",
    "This notebook works with or without an API key, but you will be given more permissive access to the data with an API key.\n",
    "- If you're running this on the [Planetary Computer Hub](http://planetarycomputer.microsoft.com/compute), make sure to choose the **GPU - PyTorch** profile when presented with the form to choose your environment.\n",
    "- The Planetary Computer Hub is pre-configured to use your API key.\n",
    "- To use your API key locally, set the environment variable `PC_SDK_SUBSCRIPTION_KEY` or use `pc.settings.set_subscription_key(<YOUR API Key>)`.\n",
    "    \n",
    "**Notes**:\n",
    "- This example uses either\n",
    "    - [sentinel-2-l2a data](https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a)\n",
    "    - [landsat-c2-l2 data](https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2)\n",
    "- The techniques used here apply equally well to other remote-sensing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90f8b309",
   "metadata": {
    "gather": {
     "logged": 1650114371790
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/geopandas/dask-geopandas\n",
    "!pip install -q pyhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cea1e5e",
   "metadata": {
    "gather": {
     "logged": 1651174535306
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import calendar\n",
    "import re\n",
    "\n",
    "RASTERIO_BEST_PRACTICES = dict(  # See https://github.com/pangeo-data/cog-best-practices\n",
    "    CURL_CA_BUNDLE=\"/etc/ssl/certs/ca-certificates.crt\",\n",
    "    GDAL_DISABLE_READDIR_ON_OPEN=\"EMPTY_DIR\",\n",
    "    AWS_NO_SIGN_REQUEST=\"YES\",\n",
    "    GDAL_MAX_RAW_BLOCK_CACHE_SIZE=\"200000000\",\n",
    "    GDAL_SWATH_SIZE=\"200000000\",\n",
    "    VSI_CURL_CACHE_SIZE=\"200000000\",\n",
    ")\n",
    "os.environ.update(RASTERIO_BEST_PRACTICES)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyhere import here\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scipy import ndimage as nd\n",
    "\n",
    "import rasterio\n",
    "import rasterio.warp\n",
    "import rasterio.mask\n",
    "import shapely.geometry\n",
    "import geopandas\n",
    "import dask_geopandas\n",
    "from dask.distributed import Client\n",
    "\n",
    "from pystac import Item\n",
    "import stackstac\n",
    "import pyproj\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=UserWarning, module=\"torch\")\n",
    "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(action=\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "import pystac_client\n",
    "import planetary_computer as pc\n",
    "\n",
    "\n",
    "# Disabling the benchmarking feature with torch.backends.cudnn.benchmark = False \n",
    "# causes cuDNN to deterministically select an algorithm, possibly at the cost of reduced performance.\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb671b5a",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "645f46e7",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1651174535433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "num_features = 1000\n",
    "country_code = 'ZMB'\n",
    "# use_file = True\n",
    "use_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f428845",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1651174535433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "satellite = \"landsat-c2-l2\"\n",
    "bands = [\n",
    "    'coastal', \n",
    "    \"red\",\n",
    "    \"green\", \n",
    "    \"blue\",\n",
    "    \"nir08\",\n",
    "    \"swir16\",\n",
    "    \"swir22\"\n",
    "]\n",
    "channels = len(bands)\n",
    "# bands_short = \"r-g-b-nir-swir16-swir22\"\n",
    "bands_short = \"1-2-3-4-5-6-7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b68da94f",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1651174535433
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# satellite = \"sentinel-2-l2a\"\n",
    "# bands = [  # Sentinel-2 Bands\n",
    "#     \"B02\", # B02 (blue) 10 meter\n",
    "#     \"B03\", # B03 (green) 10 meter\n",
    "#     \"B04\", # B04 (red) 10 meter\n",
    "#     # \"B05\", # B05(Veg Red Edge 1) 20 meter\n",
    "#     # \"B06\", # B06(Veg Red Edge 2) 20 meter\n",
    "#     # \"B07\", # B07(Veg Red Edge 3) 20 meter\n",
    "#     \"B08\", # B08 (NIR) 10 meter\n",
    "#     # \"B11\", # B11 (SWIR (1.6)) 20 meter\n",
    "#     # \"B12\", # B12 (SWIR (2.2)) 20 meter\n",
    "# ]\n",
    "# channels = len(bands)\n",
    "# bands_short = \"2-3-4-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c2528fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if satellite == \"landsat-c2-l2\":\n",
    "    resolution = 30\n",
    "    min_image_edge = 6\n",
    "else:\n",
    "    resolution = 10\n",
    "    min_image_edge = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dc8a79b",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1651174535542
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# dat_re = re.compile(r'\\d+') \n",
    "# l = [str(int(dat_re.search(x).group())) for x in bands if dat_re.search(x)]\n",
    "# bands_short = '-'.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de1b7f5",
   "metadata": {
    "collapsed": false,
    "gather": {
     "logged": 1651174535542
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e260a344",
   "metadata": {},
   "source": [
    "First we define the pytorch model that we will use to extract the features and a helper method. The **MOSAIKS** methodology describes several ways to do this and we use the simplest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbcca53c",
   "metadata": {
    "gather": {
     "logged": 1651174535955
    }
   },
   "outputs": [],
   "source": [
    "# class RCF(nn.Module):\n",
    "#     \"\"\"A model for extracting Random Convolution Features (RCF) from input imagery.\"\"\"\n",
    "#     def __init__(self, num_features=16, kernel_size=3, num_input_channels=channels):\n",
    "#         super(RCF, self).__init__()\n",
    "#         # We create `num_features / 2` filters so require `num_features` to be divisible by 2\n",
    "#         assert num_features % 2 == 0, \"Please enter an even number of features.\"\n",
    "#         # Applies a 2D convolution over an input image composed of several input planes.\n",
    "#         self.conv1 = nn.Conv2d(\n",
    "#             num_input_channels,\n",
    "#             num_features // 2,\n",
    "#             kernel_size=kernel_size,\n",
    "#             stride=1,\n",
    "#             padding=0,\n",
    "#             dilation=1,\n",
    "#             bias=True,\n",
    "#         )\n",
    "#         # Fills the input Tensor 'conv1.weight' with values drawn from the normal distribution\n",
    "#         nn.init.normal_(self.conv1.weight, mean=0.0, std=1.0)\n",
    "#         # Fills the input Tensor 'conv1.bias' with the value 'val = -1'.\n",
    "#         nn.init.constant_(self.conv1.bias, -1.0)\n",
    "#     def forward(self, x):\n",
    "#         # The rectified linear activation function or ReLU for short is a piecewise linear function \n",
    "#         # that will output the input directly if it is positive, otherwise, it will output zero.\n",
    "#         x1a = F.relu(self.conv1(x), inplace=True)\n",
    "#         # The below step is where we take the inverse which is appended later\n",
    "#         x1b = F.relu(-self.conv1(x), inplace=True)\n",
    "#         # Applies a 2D adaptive average pooling over an input signal composed of several input planes.\n",
    "#         x1a = F.adaptive_avg_pool2d(x1a, (1, 1)).squeeze()\n",
    "#         x1b = F.adaptive_avg_pool2d(x1b, (1, 1)).squeeze()\n",
    "#         if len(x1a.shape) == 1:  # case where we passed a single input\n",
    "#             return torch.cat((x1a, x1b), dim=0)\n",
    "#         elif len(x1a.shape) == 2:  # case where we passed a batch of > 1 inputs\n",
    "#             return torch.cat((x1a, x1b), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e80a28d3-0be4-45a5-81c2-a27bc9b1c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RCF(num_features)\n",
    "# model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "# model_scripted.save(here(\"models\", f'rcf_model_{channels}_channel_{num_features}-features.pt')) # Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1ac46",
   "metadata": {},
   "source": [
    "Next, we initialize the model and pytorch components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48978367",
   "metadata": {
    "gather": {
     "logged": 1651174537496
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([500, 7, 3, 3])\n",
      "conv1.bias \t torch.Size([500])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = RCF(num_features).eval().to(device)\n",
    "\n",
    "model = torch.jit.load(here(\"models\", f'rcf_model_{channels}_channel_{num_features}-features.pt'))\n",
    "model = model.eval().to(device)\n",
    "\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e95ffd3",
   "metadata": {},
   "source": [
    "## Create grid and sample points to featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3d9cf5",
   "metadata": {
    "gather": {
     "logged": 1651174535812
    }
   },
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "data/geo_boundaries/africa_adm0.geojson: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mfiona/_shim.pyx:83\u001b[0m, in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/_err.pyx:291\u001b[0m, in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: data/geo_boundaries/africa_adm0.geojson: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m### get country shape\u001b[39;00m\n\u001b[1;32m     13\u001b[0m country_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/geo_boundaries/africa_adm0.geojson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 14\u001b[0m africa \u001b[38;5;241m=\u001b[39m \u001b[43mgeopandas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountry_file_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m country \u001b[38;5;241m=\u001b[39m africa[africa\u001b[38;5;241m.\u001b[39madm0_a3 \u001b[38;5;241m==\u001b[39m country_code]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#### This would be simpler, but throws an error down the line if used \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# country = world.query(f'iso_a3 == \"{country_code}\"')\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m### Create grid of points\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/geopandas/io/file.py:253\u001b[0m, in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, engine, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m     path_or_bytes \u001b[38;5;241m=\u001b[39m filename\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_file_fiona\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _read_file_pyogrio(\n\u001b[1;32m    258\u001b[0m         path_or_bytes, bbox\u001b[38;5;241m=\u001b[39mbbox, mask\u001b[38;5;241m=\u001b[39mmask, rows\u001b[38;5;241m=\u001b[39mrows, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    259\u001b[0m     )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/geopandas/io/file.py:294\u001b[0m, in \u001b[0;36m_read_file_fiona\u001b[0;34m(path_or_bytes, from_bytes, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     reader \u001b[38;5;241m=\u001b[39m fiona\u001b[38;5;241m.\u001b[39mopen\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fiona_env():\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mreader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m features:\n\u001b[1;32m    295\u001b[0m \n\u001b[1;32m    296\u001b[0m         \u001b[38;5;66;03m# In a future Fiona release the crs attribute of features will\u001b[39;00m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;66;03m# no longer be a dict, but will behave like a dict. So this should\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         \u001b[38;5;66;03m# be forwards compatible\u001b[39;00m\n\u001b[1;32m    299\u001b[0m         crs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    300\u001b[0m             features\u001b[38;5;241m.\u001b[39mcrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    301\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m features\u001b[38;5;241m.\u001b[39mcrs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mcrs\n\u001b[1;32m    302\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m features\u001b[38;5;241m.\u001b[39mcrs_wkt\n\u001b[1;32m    303\u001b[0m         )\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;66;03m# handle loading the bounding box\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fiona/env.py:408\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local\u001b[38;5;241m.\u001b[39m_env:\n\u001b[0;32m--> 408\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fiona/__init__.py:264\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     path \u001b[38;5;241m=\u001b[39m parse_path(fp)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 264\u001b[0m     c \u001b[38;5;241m=\u001b[39m \u001b[43mCollection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menabled_drivers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menabled_drivers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m schema:\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;66;03m# Make an ordered dict of schema properties.\u001b[39;00m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.10/site-packages/fiona/collection.py:162\u001b[0m, in \u001b[0;36mCollection.__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m Session()\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m WritingSession()\n",
      "File \u001b[0;32mfiona/ogrext.pyx:540\u001b[0m, in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mfiona/_shim.pyx:90\u001b[0m, in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: data/geo_boundaries/africa_adm0.geojson: No such file or directory"
     ]
    }
   ],
   "source": [
    "if use_file:\n",
    "    gdf = pd.read_feather(here('data', 'land_cover', 'ZMB_cropland_percentage_15k-points.feather'))\n",
    "    gdf = (\n",
    "        geopandas\n",
    "        .GeoDataFrame(\n",
    "            gdf, \n",
    "            geometry = geopandas.points_from_xy(x = gdf.lon, y = gdf.lat), \n",
    "            crs='EPSG:4326')\n",
    "    )\n",
    "else:\n",
    "    cell_size = 0.01  # Roughly 1 km\n",
    "    ### get country shape\n",
    "    country_file_name = f\"data/geo_boundaries/africa_adm0.geojson\"\n",
    "    africa = geopandas.read_file(country_file_name)\n",
    "    country = africa[africa.adm0_a3 == country_code]\n",
    "    #### This would be simpler, but throws an error down the line if used \n",
    "    # world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "    # country = world.query(f'iso_a3 == \"{country_code}\"')\n",
    "    ### Create grid of points\n",
    "    cell_size = .01  # Very roughly 1 km\n",
    "    xmin, ymin, xmax, ymax = country.total_bounds\n",
    "    xs = list(np.arange(xmin, xmax + cell_size, cell_size))\n",
    "    ys = list(np.arange(ymin, ymax + cell_size, cell_size))\n",
    "    def make_cell(x, y, cell_size):\n",
    "        ring = [\n",
    "            (x, y),\n",
    "            (x + cell_size, y),\n",
    "            (x + cell_size, y + cell_size),\n",
    "            (x, y + cell_size)\n",
    "        ]\n",
    "        cell = shapely.geometry.Polygon(ring).centroid\n",
    "        return cell\n",
    "    center_points = []\n",
    "    for x in xs:\n",
    "        for y in ys:\n",
    "            cell = make_cell(x, y, cell_size)\n",
    "            center_points.append(cell)\n",
    "    ### Put grid into a GeDataFrame for cropping to country shape\n",
    "    gdf = geopandas.GeoDataFrame({'geometry': center_points}, crs = 'EPSG:4326')\n",
    "    gdf['lon'], gdf['lat'] = gdf.geometry.x, gdf.geometry.y\n",
    "    ### Subset to country \n",
    "    ### This buffer ensures that no points are take at the border \n",
    "    ### which would lead to duplication with neighboring countries\n",
    "    gdf = gdf[gdf.within(country.unary_union.buffer(-0.005))]\n",
    "    gdf = gdf[['lon', 'lat', 'geometry']].reset_index(drop = True)\n",
    "    gdf = gdf.sample(frac = 0.1, random_state=42, ignore_index=False)\n",
    "    points = gdf[[\"lon\", \"lat\"]].to_numpy()\n",
    "pt_len = gdf.shape[0]\n",
    "gdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc1e0ea",
   "metadata": {},
   "source": [
    "### Extract features from the imagery around each point\n",
    "\n",
    "We need to find a suitable Sentinel 2 scene for each point. As usual, we'll use `pystac-client` to search for items matching some conditions, but we don't just want do make a `.search()` call for each of the 67,968 remaining points. Each HTTP request is relatively slow. Instead, we will *batch* or points and search *in parallel*.\n",
    "\n",
    "We need to be a bit careful with how we batch up our points though. Since a single Sentinel 2 scene will cover many points, we want to make sure that points which are spatially close together end up in the same batch. In short, we need to spatially partition the dataset. This is implemented in `dask-geopandas`.\n",
    "\n",
    "So the overall workflow will be\n",
    "\n",
    "1. Find an appropriate STAC item for each point (in parallel, using the spatially partitioned dataset)\n",
    "2. Feed the points and STAC items to a custom Dataset that can read imagery given a point and the URL of a overlapping S2 scene\n",
    "3. Use a custom Dataloader, which uses our Dataset, to feed our model imagery and save the corresponding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90382d1e",
   "metadata": {
    "gather": {
     "logged": 1651174537641
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NPARTITIONS = 250\n",
    "\n",
    "ddf = dask_geopandas.from_geopandas(gdf, npartitions=1)\n",
    "hd = ddf.hilbert_distance().compute()\n",
    "gdf[\"hd\"] = hd\n",
    "gdf = gdf.sort_values(\"hd\")\n",
    "\n",
    "dgdf = dask_geopandas.from_geopandas(gdf, npartitions=NPARTITIONS, sort=False)\n",
    "\n",
    "del ddf\n",
    "del hd\n",
    "del gdf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d430715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill(data, invalid=None):\n",
    "    \"\"\"\n",
    "    Replace the value of invalid 'data' cells (indicated by 'invalid') \n",
    "    by the value of the nearest valid data cell\n",
    "\n",
    "    Input:\n",
    "        data:    numpy array of any dimension\n",
    "        invalid: a binary array of same shape as 'data'. \n",
    "                 data value are replaced where invalid is True\n",
    "                 If None (default), use: invalid = np.isnan(data)\n",
    "\n",
    "    Output: \n",
    "        Return a filled array. \n",
    "    \"\"\"    \n",
    "    if invalid is None: invalid = np.isnan(data)\n",
    "\n",
    "    ind = nd.distance_transform_edt(\n",
    "        invalid, \n",
    "        return_distances=False, \n",
    "        return_indices=True\n",
    "    )\n",
    "    return data[tuple(ind)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2a56ad1-74d4-49da-9bb9-2a1ed0c3d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 0.005\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, points, items, buffer=buffer_size):\n",
    "        self.points = points\n",
    "        self.items = items\n",
    "        self.buffer = buffer\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.points.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        lon, lat = self.points[idx]\n",
    "        fn = self.items[idx]\n",
    "\n",
    "        if fn is None:\n",
    "            return None\n",
    "        else:\n",
    "            try:\n",
    "                stack = stackstac.stack(\n",
    "                    fn,\n",
    "                    assets=bands,\n",
    "                    resolution=resolution,\n",
    "                )\n",
    "                x_min, y_min = pyproj.Proj(stack.crs)(lon-self.buffer, lat-self.buffer)\n",
    "                x_max, y_max = pyproj.Proj(stack.crs)(lon+self.buffer, lat+self.buffer)\n",
    "                aoi = stack.loc[..., y_max:y_min, x_min:x_max]\n",
    "                data = aoi.compute(\n",
    "                    scheduler=\"single-threaded\"\n",
    "                    )\n",
    "                # out_image = data.data \n",
    "                out_image = fill(data.data)\n",
    "                out_image = ((out_image - out_image.min()) ) / (out_image.max() - out_image.min())\n",
    "            except ValueError:\n",
    "                pass\n",
    "            out_image = torch.from_numpy(out_image).float()\n",
    "            return out_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e23174e4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  \n",
      "   Satellite: landsat-c2-l2  \n",
      "   Pixel Resolution: 30  \n",
      "   Grid Resolution: 0.01 degree squared (WGS84) \n",
      "   Cloud Limit: less than 10%  \n",
      "   Bands: ['coastal', 'red', 'green', 'blue', 'nir08', 'swir16', 'swir22'] \n",
      "   Points: 15058 \n",
      "   Number Features: 1000 features \n",
      "   Year Range: 2022 to 2022 \n",
      "\n",
      "Matching images to points for: 5-2022\n",
      "Found acceptable images for 13862/15058 points in 57.05 seconds\n",
      "Featurizing: 05-2022\n",
      "0/13862 -- 0.00% -- 61.79 seconds\n",
      "1000/13862 -- 7.21% -- 498.41 seconds\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"rasterio/_base.pyx\", line 302, in rasterio._base.DatasetBase.__init__\n  File \"rasterio/_base.pyx\", line 213, in rasterio._base.open_dataset\n  File \"rasterio/_err.pyx\", line 217, in rasterio._err.exc_wrap_pointer\nrasterio._err.CPLE_OpenFailedError: '/vsicurl/https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/oli-tirs/2022/172/072/LC08_L2SP_172072_20220512_20220519_02_T1/LC08_L2SP_172072_20220512_20220519_02_T1_SR_B5.TIF?st=2022-11-09T21%3A52%3A03Z&se=2022-11-17T21%3A52%3A04Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-11-10T21%3A52%3A02Z&ske=2022-11-17T21%3A52%3A02Z&sks=b&skv=2021-06-08&sig=FuCH141deUsroWltadfTgYp8Bu/p7akzXcuMQa%2BX9Uk%3D' not recognized as a supported file format.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/stackstac/rio_reader.py\", line 326, in _open\n    ds = SelfCleaningDatasetReader(\n  File \"rasterio/_base.pyx\", line 304, in rasterio._base.DatasetBase.__init__\nrasterio.errors.RasterioIOError: '/vsicurl/https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/oli-tirs/2022/172/072/LC08_L2SP_172072_20220512_20220519_02_T1/LC08_L2SP_172072_20220512_20220519_02_T1_SR_B5.TIF?st=2022-11-09T21%3A52%3A03Z&se=2022-11-17T21%3A52%3A04Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-11-10T21%3A52%3A02Z&ske=2022-11-17T21%3A52%3A02Z&sks=b&skv=2021-06-08&sig=FuCH141deUsroWltadfTgYp8Bu/p7akzXcuMQa%2BX9Uk%3D' not recognized as a supported file format.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_3643/2343653854.py\", line 28, in __getitem__\n    data = aoi.compute(\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/dataarray.py\", line 993, in compute\n    return new.load(**kwargs)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/dataarray.py\", line 967, in load\n    ds = self._to_temp_dataset().load(**kwargs)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/dataset.py\", line 733, in load\n    evaluated_data = da.compute(*lazy_data.values(), **kwargs)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/base.py\", line 575, in compute\n    results = schedule(dsk, keys, **kwargs)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/local.py\", line 554, in get_sync\n    return get_async(\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/local.py\", line 497, in get_async\n    for key, res_info, failed in queue_get(queue).result():\n  File \"/srv/conda/envs/notebook/lib/python3.9/concurrent/futures/_base.py\", line 439, in result\n    return self.__get_result()\n  File \"/srv/conda/envs/notebook/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n    raise self._exception\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/local.py\", line 539, in submit\n    fut.set_result(fn(*args, **kwargs))\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/local.py\", line 235, in batch_execute_tasks\n    return [execute_task(*a) for a in it]\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/local.py\", line 235, in <listcomp>\n    return [execute_task(*a) for a in it]\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/local.py\", line 226, in execute_task\n    result = pack_exception(e, dumps)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/local.py\", line 221, in execute_task\n    result = _execute_task(task, data)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/core.py\", line 119, in _execute_task\n    return func(*(_execute_task(a, cache) for a in args))\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/core.py\", line 119, in <genexpr>\n    return func(*(_execute_task(a, cache) for a in args))\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/core.py\", line 119, in _execute_task\n    return func(*(_execute_task(a, cache) for a in args))\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/optimization.py\", line 990, in __call__\n    return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/core.py\", line 149, in get\n    result = _execute_task(task, cache)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/core.py\", line 119, in _execute_task\n    return func(*(_execute_task(a, cache) for a in args))\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/stackstac/to_dask.py\", line 185, in fetch_raster_window\n    data = reader.read(current_window)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/stackstac/rio_reader.py\", line 385, in read\n    reader = self.dataset\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/stackstac/rio_reader.py\", line 381, in dataset\n    self._dataset = self._open()\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/stackstac/rio_reader.py\", line 337, in _open\n    raise RuntimeError(msg) from e\nRuntimeError: Error opening 'https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/oli-tirs/2022/172/072/LC08_L2SP_172072_20220512_20220519_02_T1/LC08_L2SP_172072_20220512_20220519_02_T1_SR_B5.TIF?st=2022-11-09T21%3A52%3A03Z&se=2022-11-17T21%3A52%3A04Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-11-10T21%3A52%3A02Z&ske=2022-11-17T21%3A52%3A02Z&sks=b&skv=2021-06-08&sig=FuCH141deUsroWltadfTgYp8Bu/p7akzXcuMQa%2BX9Uk%3D': RasterioIOError(\"'/vsicurl/https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/oli-tirs/2022/172/072/LC08_L2SP_172072_20220512_20220519_02_T1/LC08_L2SP_172072_20220512_20220519_02_T1_SR_B5.TIF?st=2022-11-09T21%3A52%3A03Z&se=2022-11-17T21%3A52%3A04Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-11-10T21%3A52%3A02Z&ske=2022-11-17T21%3A52%3A02Z&sks=b&skv=2021-06-08&sig=FuCH141deUsroWltadfTgYp8Bu/p7akzXcuMQa%2BX9Uk%3D' not recognized as a supported file format.\")\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:138\u001b[0m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1356\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1355\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1359\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1402\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1402\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/_utils.py:461\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"rasterio/_base.pyx\", line 302, in rasterio._base.DatasetBase.__init__\n  File \"rasterio/_base.pyx\", line 213, in rasterio._base.open_dataset\n  File \"rasterio/_err.pyx\", line 217, in rasterio._err.exc_wrap_pointer\nrasterio._err.CPLE_OpenFailedError: '/vsicurl/https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/oli-tirs/2022/172/072/LC08_L2SP_172072_20220512_20220519_02_T1/LC08_L2SP_172072_20220512_20220519_02_T1_SR_B5.TIF?st=2022-11-09T21%3A52%3A03Z&se=2022-11-17T21%3A52%3A04Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-11-10T21%3A52%3A02Z&ske=2022-11-17T21%3A52%3A02Z&sks=b&skv=2021-06-08&sig=FuCH141deUsroWltadfTgYp8Bu/p7akzXcuMQa%2BX9Uk%3D' not recognized as a supported file format.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/stackstac/rio_reader.py\", line 326, in _open\n    ds = SelfCleaningDatasetReader(\n  File \"rasterio/_base.pyx\", line 304, in rasterio._base.DatasetBase.__init__\nrasterio.errors.RasterioIOError: '/vsicurl/https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/oli-tirs/2022/172/072/LC08_L2SP_172072_20220512_20220519_02_T1/LC08_L2SP_172072_20220512_20220519_02_T1_SR_B5.TIF?st=2022-11-09T21%3A52%3A03Z&se=2022-11-17T21%3A52%3A04Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-11-10T21%3A52%3A02Z&ske=2022-11-17T21%3A52%3A02Z&sks=b&skv=2021-06-08&sig=FuCH141deUsroWltadfTgYp8Bu/p7akzXcuMQa%2BX9Uk%3D' not recognized as a supported file format.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_3643/2343653854.py\", line 28, in __getitem__\n    data = aoi.compute(\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/dataarray.py\", line 993, in compute\n    return new.load(**kwargs)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/dataarray.py\", line 967, in load\n    ds = self._to_temp_dataset().load(**kwargs)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/dataset.py\", line 733, in load\n    evaluated_data = da.compute(*lazy_data.values(), **kwargs)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/base.py\", line 575, in compute\n    results = schedule(dsk, keys, **kwargs)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/local.py\", line 554, in get_sync\n    return get_async(\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/local.py\", line 497, in get_async\n    for key, res_info, failed in queue_get(queue).result():\n  File \"/srv/conda/envs/notebook/lib/python3.9/concurrent/futures/_base.py\", line 439, in result\n    return self.__get_result()\n  File \"/srv/conda/envs/notebook/lib/python3.9/concurrent/futures/_base.py\", line 391, in __get_result\n    raise self._exception\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/local.py\", line 539, in submit\n    fut.set_result(fn(*args, **kwargs))\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/local.py\", line 235, in batch_execute_tasks\n    return [execute_task(*a) for a in it]\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/local.py\", line 235, in <listcomp>\n    return [execute_task(*a) for a in it]\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/local.py\", line 226, in execute_task\n    result = pack_exception(e, dumps)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/local.py\", line 221, in execute_task\n    result = _execute_task(task, data)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/core.py\", line 119, in _execute_task\n    return func(*(_execute_task(a, cache) for a in args))\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/core.py\", line 119, in <genexpr>\n    return func(*(_execute_task(a, cache) for a in args))\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/core.py\", line 119, in _execute_task\n    return func(*(_execute_task(a, cache) for a in args))\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/optimization.py\", line 990, in __call__\n    return core.get(self.dsk, self.outkey, dict(zip(self.inkeys, args)))\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/core.py\", line 149, in get\n    result = _execute_task(task, cache)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/dask/core.py\", line 119, in _execute_task\n    return func(*(_execute_task(a, cache) for a in args))\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/stackstac/to_dask.py\", line 185, in fetch_raster_window\n    data = reader.read(current_window)\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/stackstac/rio_reader.py\", line 385, in read\n    reader = self.dataset\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/stackstac/rio_reader.py\", line 381, in dataset\n    self._dataset = self._open()\n  File \"/srv/conda/envs/notebook/lib/python3.9/site-packages/stackstac/rio_reader.py\", line 337, in _open\n    raise RuntimeError(msg) from e\nRuntimeError: Error opening 'https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/oli-tirs/2022/172/072/LC08_L2SP_172072_20220512_20220519_02_T1/LC08_L2SP_172072_20220512_20220519_02_T1_SR_B5.TIF?st=2022-11-09T21%3A52%3A03Z&se=2022-11-17T21%3A52%3A04Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-11-10T21%3A52%3A02Z&ske=2022-11-17T21%3A52%3A02Z&sks=b&skv=2021-06-08&sig=FuCH141deUsroWltadfTgYp8Bu/p7akzXcuMQa%2BX9Uk%3D': RasterioIOError(\"'/vsicurl/https://landsateuwest.blob.core.windows.net/landsat-c2/level-2/standard/oli-tirs/2022/172/072/LC08_L2SP_172072_20220512_20220519_02_T1/LC08_L2SP_172072_20220512_20220519_02_T1_SR_B5.TIF?st=2022-11-09T21%3A52%3A03Z&se=2022-11-17T21%3A52%3A04Z&sp=rl&sv=2021-06-08&sr=c&skoid=c85c15d6-d1ae-42d4-af60-e2ca0f81359b&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-11-10T21%3A52%3A02Z&ske=2022-11-17T21%3A52%3A02Z&sks=b&skv=2021-06-08&sig=FuCH141deUsroWltadfTgYp8Bu/p7akzXcuMQa%2BX9Uk%3D' not recognized as a supported file format.\")\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "start_month = 5\n",
    "end_month = 9\n",
    "year_start = 2022\n",
    "year_end = 2022\n",
    "\n",
    "cloud_limit = 10\n",
    "\n",
    "batch_size = 30\n",
    "workers = os.cpu_count() \n",
    "\n",
    "print(\"Using:  \\n\", \n",
    "      f\"  Satellite: {satellite}  \\n\",\n",
    "      f\"  Pixel Resolution: {resolution}  \\n\",\n",
    "      f\"  Grid Resolution: {buffer_size * 2} degree squared (WGS84) \\n\",\n",
    "      f\"  Cloud Limit: less than {cloud_limit}%  \\n\",\n",
    "      f\"  Bands: {bands} \\n\",\n",
    "      f\"  Points: {pt_len} \\n\",\n",
    "      f\"  Number Features: {num_features} features \\n\",\n",
    "      f\"  Year Range: {year_start} to {year_end} \\n\")\n",
    "\n",
    "for yr in range(year_start, year_end+1):\n",
    "    features = pd.DataFrame()\n",
    "    ft = []\n",
    "    \n",
    "    if (yr == year_start):\n",
    "        month_range = range(start_month, end_month + 1)\n",
    "    else:\n",
    "        month_range = range(1, 13) \n",
    "        \n",
    "    for mn in month_range:\n",
    "\n",
    "        if mn < 10:\n",
    "            month = \"0\"+str(mn)\n",
    "        else:\n",
    "            month = mn\n",
    "\n",
    "        def query(points):\n",
    "            \"\"\"\n",
    "            Find a STAC item for points in the `points` DataFrame\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            points : geopandas.GeoDataFrame\n",
    "                A GeoDataFrame\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            geopandas.GeoDataFrame\n",
    "                A new geopandas.GeoDataFrame with a `stac_item` column containing the STAC\n",
    "                item that covers each point.\n",
    "            \"\"\"\n",
    "            intersects = shapely.geometry.mapping(points.unary_union.convex_hull)\n",
    "\n",
    "            catalog = pystac_client.Client.open(\n",
    "                \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
    "            )\n",
    "            # Define search date range for query\n",
    "            ending_day = calendar.monthrange(yr, int(mn))[1]\n",
    "            search_start = f\"{yr}-{month}-1\" \n",
    "            search_end = f\"{yr}-{month}-{ending_day}\" \n",
    "            \n",
    "            # The time frame in which we search for non-cloudy imagery\n",
    "            search = catalog.search(\n",
    "                collections=[satellite],  \n",
    "                intersects=intersects,\n",
    "                datetime=[search_start, search_end],\n",
    "                query={\"eo:cloud_cover\": {\"lt\": cloud_limit},\"platform\": {\"in\": \"landsat-8\"}},\n",
    "                limit=500,\n",
    "            )\n",
    "            ic = search.get_all_items_as_dict()\n",
    "            features = ic[\"features\"]\n",
    "            features_d = {item[\"id\"]: item for item in features}\n",
    "            data = {\n",
    "                \"eo:cloud_cover\": [],\n",
    "                \"geometry\": [],\n",
    "            }\n",
    "            index = []\n",
    "            for item in features:\n",
    "                data[\"eo:cloud_cover\"].append(item[\"properties\"][\"eo:cloud_cover\"])\n",
    "                data[\"geometry\"].append(shapely.geometry.shape(item[\"geometry\"]))\n",
    "                index.append(item[\"id\"])\n",
    "            items = geopandas.GeoDataFrame(data, index=index, geometry=\"geometry\").sort_values(\n",
    "                \"eo:cloud_cover\"\n",
    "            )\n",
    "            point_list = points.geometry.tolist()\n",
    "            point_items = []\n",
    "            for point in point_list:\n",
    "                covered_by = items[items.covers(point)]\n",
    "                if len(covered_by):\n",
    "                    point_items.append(features_d[covered_by.index[0]])\n",
    "                else:\n",
    "                    # There weren't any scenes matching our conditions for this point (too cloudy)\n",
    "                    point_items.append(None)\n",
    "            return points.assign(stac_item=point_items)\n",
    "\n",
    "        tic = time.time()\n",
    "        print(\"Matching images to points for: \", mn, \"-\", yr, sep = \"\")\n",
    "\n",
    "        with Client(n_workers=16) as client:\n",
    "            meta = dgdf._meta.assign(stac_item=[])\n",
    "            df2 = dgdf.map_partitions(query, meta=meta).compute()\n",
    "        df3 = df2.dropna(subset=[\"stac_item\"]).reset_index(drop = True)\n",
    "\n",
    "        matching_items = []\n",
    "        for item in df3.stac_item.tolist():\n",
    "            signed_item = pc.sign(Item.from_dict(item))\n",
    "            matching_items.append(signed_item)\n",
    "\n",
    "\n",
    "        points = df3[[\"lon\", \"lat\"]].to_numpy()\n",
    "        \n",
    "        print(\"Found acceptable images for \", \n",
    "              points.shape[0], \"/\", pt_len,\n",
    "              \" points in \", \n",
    "              f\"{time.time()-tic:0.2f} seconds\", \n",
    "              sep = \"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        dataset = CustomDataset(points, matching_items)\n",
    "\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=workers,\n",
    "            collate_fn=lambda x: x,\n",
    "            pin_memory=False,\n",
    "            # persistent_workers=True,\n",
    "        )\n",
    "\n",
    "        x_all = np.zeros((points.shape[0], num_features), dtype=float)\n",
    "        tic = time.time()\n",
    "        toc = time.time()\n",
    "        i = 0\n",
    "        print(\"Featurizing: \", month, \"-\", yr, sep = \"\")\n",
    "        for images in dataloader:\n",
    "            for image in images:\n",
    "\n",
    "                if i % 1000 == 0:\n",
    "                    print(\n",
    "                        f\"{i}/{points.shape[0]} -- {i / points.shape[0] * 100:0.2f}%\"\n",
    "                        + f\" -- {time.time()-tic:0.2f} seconds\"\n",
    "                    )\n",
    "                    tic = time.time()\n",
    "\n",
    "                    # LS 8 scene size is 185 km x 180 km\n",
    "\n",
    "                if image is not None:\n",
    "                    # each image should have dim (time, bands, height, width) so len(image.shape) == 4\n",
    "                    # with only 1 timestamp (image.shape[0] == 1)\n",
    "                    # Ideally an image.shape will be (1, 7, 33, 34)\n",
    "                    assert len(image.shape) == 4, image.shape[0] == 1\n",
    "                    # A full image should be ~33x34 pixels (i.e. ~1km^2 at a 30m/px spatial\n",
    "                    # resolution), however we can receive smaller images if an input point\n",
    "                    # happens to be at the edge of a Landsat scene (a literal edge case). To deal\n",
    "                    # with these (edge) cases we crudely drop all images where the spatial\n",
    "                    # dimensions aren't both greater than 20 pixels.\n",
    "\n",
    "                    # if type(image) == torch.Tensor: \n",
    "                    try:\n",
    "                        if image.shape[2] >= min_image_edge and image.shape[3] >= min_image_edge:\n",
    "                            image = image.to(device)\n",
    "                            with torch.no_grad():\n",
    "                                feats = model(image).cpu().numpy()\n",
    "                            x_all[i] = feats\n",
    "                        else:\n",
    "                            # this happens if the point is close to the edge \n",
    "                            # of a scene (one or both of the spatial dimensions\n",
    "                            # of the image are very small)\n",
    "                            pass\n",
    "                    except ValueError: \n",
    "                        pass \n",
    "                else:\n",
    "                    pass  # this happens if we do not find a S2 scene for some point\n",
    "                i += 1\n",
    "                \n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            torch.cuda.empty_cache()\n",
    "                \n",
    "        features_monthly = pd.DataFrame(x_all)\n",
    "        features_monthly[[\"lon\", \"lat\"]] = points.tolist()\n",
    "        features_monthly['year'] = yr\n",
    "        features_monthly['month'] = mn\n",
    "        \n",
    "        # ft.append(features_monthly)\n",
    "\n",
    "        features_monthly.columns = features_monthly.columns.astype(str)\n",
    "        \n",
    "        # Save the features to a feather file\n",
    "        file_name = (here('data', 'random_features', f'{satellite}',\n",
    "                          f'{satellite}_bands-{bands_short}_{country_code}_{pt_len/1000:.0f}'+\n",
    "                          f'k-points_{num_features}-features_{yr}_{mn}.feather'\n",
    "                         ))\n",
    "        # file_name = (f'data/{satellite}_bands-{bands_short}_{country_code}_{pt_len/1000:.0f}'+\n",
    "        #             f'k-points_{num_features}-features_{yr}_{mn}.feather')\n",
    "        \n",
    "        print(\"Saving file as:\", file_name)\n",
    "        features_monthly.to_feather(file_name)\n",
    "        \n",
    "        # Free memory before loop iterates\n",
    "        print(\"Freeing RAM\")\n",
    "        del meta\n",
    "        del query\n",
    "        del df2\n",
    "        del df3\n",
    "        del points\n",
    "        del dataset\n",
    "        del dataloader\n",
    "        del x_all\n",
    "        del features_monthly\n",
    "        gc.collect()\n",
    "        print(f\"Done in {(time.time()-toc)/60:0.2f} minutes\")\n",
    "        print('')\n",
    "    # features = pd.concat(ft).reset_index(drop = True)\n",
    "    \n",
    "    # features.columns = features.columns.astype(str)\n",
    "    \n",
    "    # # Save the features to a feather file\n",
    "    # file_name = (f'data/{satellite}_bands-{bands_short}_{country_code}_{pt_len/1000:.0f}'+\n",
    "    #              f'k-points_{num_features}-features_{yr}.feather')\n",
    "    \n",
    "    # print(\"Saving file as:\", file_name)\n",
    "    # features.to_feather(file_name)\n",
    "    \n",
    "    # display(FileLink(file_name))\n",
    "    \n",
    "    print(\"Save finished!\")\n",
    "    # Free memory before loop iterates\n",
    "    print(\"Freeing RAM\")\n",
    "    # del features\n",
    "    # del ft\n",
    "    # gc.collect()\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acfcbd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5cfb145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# points[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0b42494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nofeature_mask = ~(x_all.sum(axis=1) == 0)\n",
    "# x_all = x_all[nofeature_mask]\n",
    "# points = points[nofeature_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b8c7fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_monthly = pd.DataFrame(x_all)\n",
    "# features_monthly[[\"lon\", \"lat\"]] = points.tolist()\n",
    "# features_monthly['year'] = yr\n",
    "# features_monthly['month'] = mn\n",
    "\n",
    "# features_monthly.columns = features_monthly.columns.astype(str)\n",
    "\n",
    "# # Save the features to a feather file\n",
    "# file_name = (here('data', 'random_features', f'{satellite}',\n",
    "#                   f'{satellite}_bands-{bands_short}_{country_code}_{pt_len/1000:.0f}'+\n",
    "#                   f'k-points_{num_features}-features_{yr}_{mn}.feather'\n",
    "#                  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff6c5442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6a9fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Saving file as:\", file_name)\n",
    "# features_monthly.to_feather(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9529b138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_feather(here('data', 'random_features', f'{satellite}', \n",
    "#                      \"landsat-c2-l2_bands-r-g-b-nir-swir16-swir22_ZMB_20k-points_1024-features_2022_4.feather\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6089735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stackstac\n",
    "# import pystac_client\n",
    "# import pyproj\n",
    "# import planetary_computer as pc\n",
    "# from pystac.extensions.eo import EOExtension as eo\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# catalog = pystac_client.Client.open('https://planetarycomputer.microsoft.com/api/stac/v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "486d0795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aoi = dict(type=\"Point\", coordinates=[24.784878, -16.664232])\n",
    "# daterange=\"2022-07-01/2022-08-01\"\n",
    "# cloud_cover=20\n",
    "# search = catalog.search(filter_lang=\"cql2-json\", filter={\n",
    "#   \"op\": \"and\",\n",
    "#   \"args\": [\n",
    "#     {\"op\": \"s_intersects\", \"args\": [{\"property\": \"geometry\"}, aoi]},\n",
    "#     {\"op\": \"anyinteracts\", \"args\": [{\"property\": \"datetime\"}, daterange]},\n",
    "#       # 'Item id=LC09_L2SP_174071_20220721_02_T1'\n",
    "#     {\"op\": \"=\", \"args\": [{\"property\": \"id\"}, \"LC09_L2SP_174071_20220721_02_T1\"]},\n",
    "#     # {\"op\": \"=\", \"args\": [{\"property\": \"collection\"}, \"landsat-c2-l2\"]},\n",
    "#     # {\"op\": \"in\", \"args\": [{\"property\": \"platform\"}, [\"landsat-9\"]]},\n",
    "#     # {\"op\": \"lt\", \"args\": [{\"property\": \"eo:cloud_cover\"}, cloud_cover]},\n",
    "#   ]\n",
    "# })\n",
    "# first_item = next(search.get_items())\n",
    "# item = pc.sign_item(first_item)\n",
    "# items = pc.sign(search); print(len(items)); items[0].properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25db2e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9eb492c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack = stackstac.stack(item, assets=[\"red\", \"green\", \"blue\",])\n",
    "# x_utm, y_utm = pyproj.Proj(stack.crs)(20, 5)\n",
    "# buffer = 500  # meters\n",
    "# aoi = stack.loc[..., y_utm+buffer:y_utm-buffer, x_utm-buffer:x_utm+buffer]\n",
    "# data = aoi.compute()\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ff91511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.plot.imshow(row=\"time\", rgb=\"band\", robust=True, size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266521e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 2,
           "op": "addrange",
           "valuelist": "9"
          },
          {
           "key": 2,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
